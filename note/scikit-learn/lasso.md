📌 使用场景

Lasso 适用于以下情况：

- 特征选择需求强：希望模型具有稀疏性，只保留重要特征。
- 特征数量远大于样本数量（高维数据）：如基因组数据、文本数据等。
- 有很多冗余或无关变量时，希望自动去除部分无关特征。
- 想要构建一个简洁、可解释性强的线性模型。

典型应用：

- 生物信息学（选出与疾病相关的基因）
- 金融建模（挑选影响因子）
- 文本建模（筛选关键词）

📌 基本原理

Lasso 是在线性回归的基础上加入 L1 正则化项，目标函数为：

$$
\min_\beta \|X\beta - y\|^2 + \alpha \|\beta\|_1
$$

- 第一项是残差平方和（和普通最小二乘一样）
- 第二项是 L1 正则化，会推动某些系数变为 0，实现特征选择
- $\alpha \geq 0$：控制正则化强度（稀疏性）

Lasso 的主要特点：

- L1 范数会导致一部分回归系数精确为零
- 模型变得更稀疏，更容易解释

📌 注意事项

1. 特征选择能力强但不稳定：

   - 在特征高度相关时，Lasso 可能会随机选择其中一部分并丢弃其他。
   - 这种不稳定性可通过 ElasticNet 改进（结合 L1 和 L2 正则化）。

2. 正则化参数 $\alpha$ 很关键：

   - $\alpha$ 越大，正则化越强，模型越稀疏。
   - 使用 `LassoCV` 可通过交叉验证自动选取最优 $\alpha$。

3. 特征必须标准化：

   - 因为 L1 正则依赖于系数绝对值的大小，不同量纲会造成偏差。
   - 建议使用 `StandardScaler` 进行特征缩放。

4. 计算成本较高：

   - 相比 Ridge，Lasso 的优化问题更复杂（非光滑），尤其当特征数非常多时。
   - 使用坐标下降（Coordinate Descent）等算法实现。

5. 不适用于所有特征都重要的情况：

   - 如果你认为所有特征都应该参与预测（而不是删掉某些特征），Lasso 可能不合适，可用 Ridge 替代。
