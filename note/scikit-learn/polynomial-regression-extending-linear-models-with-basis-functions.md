> 实质上是使用线性回归框架来拟合非线性数据，通过对原始特征进行多项式扩展来实现。

📌 使用场景

多项式回归适用于以下场景：

- 特征与目标变量之间的关系是非线性的，但可以用多项式函数近似；
- 想保留线性模型的可解释性，但提升其表达能力；
- 应用于自然科学、工程模拟、经济预测等需要拟合非线性趋势的问题；
- 快速构建非线性模型，不想使用更复杂的模型（如树模型、神经网络）；
- 想将输入特征扩展为更高维空间，从而进行非线性分类或回归（作为核方法的一种显式近似）。

📌 基本原理

- 多项式回归本质上仍是线性模型，但它是在线性模型中引入了非线性的特征映射（Basis Functions）：

  $$
  y = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_d x^d
  $$

- 特征经过变换后，再使用线性回归进行拟合；

- 在 Scikit-learn 中通常通过 `PolynomialFeatures` + `LinearRegression` 组合实现：

  1. `PolynomialFeatures(degree=d)` 将输入特征扩展为多项式形式；
  2. 将变换后的特征输入到 `LinearRegression` 训练模型；

- 实现非线性拟合的同时仍能保留线性模型的求解优势（解析解、闭式解或正则解）。

📌 注意事项

1. 容易过拟合：

   - 尤其是在使用高阶多项式（如 degree > 3）时，模型可能在训练数据上拟合过好，但在测试数据上泛化差；
   - 建议配合 正则化 使用（如 Ridge、Lasso）控制复杂度。

2. 特征维度迅速增加：

   - 多项式特征组合会显著增加维度，尤其是多维输入和高阶时，容易造成“维度灾难”；
   - 注意计算成本和内存开销。

3. 需要特征缩放：

   - 多项式特征在数值上可能跨度大（如 $x^2, x^3$），需先使用 `StandardScaler` 等方法对原始特征归一化。

4. 不适合太复杂的非线性关系：

   - 多项式回归拟合能力有限，对于复杂边界或局部模式，效果可能不如核方法、树模型、神经网络等。

5. 可以组合多个特征交互项：

   - `interaction_only=True` 时，只考虑特征间的乘积项，不含幂次项，可用于特征工程。

6. 使用交叉验证选择合适的多项式阶数（degree）：

   - 阶数太低：欠拟合；阶数太高：过拟合。
