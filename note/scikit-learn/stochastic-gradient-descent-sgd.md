📌 使用场景

- 适合 大规模、高维度数据集，当传统优化算法（如解析解、批量梯度下降）计算成本过高时。
- 需要训练线性模型（如线性回归、逻辑回归、支持向量机等），且样本量非常大。
- 希望对模型进行在线学习（增量式训练）或流式数据训练。
- 适用于多种损失函数和正则化形式，灵活性强。
- 适合需要自定义损失函数和正则项的高级用户。

📌 基本原理

- SGD 是基于 梯度下降 的一种优化算法，但每次只用**一个样本（或小批量样本**计算梯度并更新模型参数。
- 与批量梯度下降相比，更新更频繁，计算开销更小，适合大数据。
- 核心流程：

  1. 从数据中随机抽取一个样本（或小批量）。
  2. 计算该样本的梯度。
  3. 根据梯度更新模型参数。
  4. 重复上述过程直到收敛或达到最大迭代次数。

- 支持多种损失函数（如平方误差、对数损失、hinge 损失等）和正则化（L1、L2、Elastic Net）。
- Scikit-learn 的 `SGDClassifier` 和 `SGDRegressor` 都是基于此算法实现。

📌 注意事项

1. 需要调节学习率（step size）：

   - 学习率过大可能导致不收敛，过小则收敛慢；
   - 常用学习率调度策略（如逐步衰减）提高性能。

2. 随机性导致训练过程波动较大：

   - 损失函数曲线震荡明显，不如批量方法平滑；
   - 通常需要较多迭代和合适的正则化来稳定。

3. 训练结果依赖于初始化和随机种子：

   - 不同初始化可能导致不同局部最优。

4. 对特征标准化非常敏感：

   - 未标准化的特征会导致梯度更新不均匀，影响收敛。
   - 推荐使用 `StandardScaler` 等方法预处理数据。

5. 适合增量学习和在线学习场景：

   - 可以方便地进行部分拟合（partial_fit），适合流式数据。

6. 对超参数调节较敏感：

   - 包括学习率、正则化参数、迭代次数、批量大小等，需通过交叉验证调整。

7. 不能保证全局最优：

   - 尤其在非凸优化问题中，可能陷入局部极小。
