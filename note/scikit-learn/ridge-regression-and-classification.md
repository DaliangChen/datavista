📌 使用场景

Ridge 回归适用于：

- 线性回归任务，但你希望引入一定的正则化来控制模型复杂度。
- 特征高度相关或特征数量多于样本数量的情况（多重共线性问题）。
- 需要平衡模型拟合度与泛化能力，防止过拟合。
- 也适用于分类问题（通过 `RidgeClassifier` 实现），尤其当样本维度高时比逻辑回归更稳定。

典型应用：

- 财务预测（具有高度共线性的变量）
- 医疗或基因数据（高维小样本）
- 文本分类（大量稀疏特征）

📌 基本原理

Ridge 回归是对普通最小二乘（OLS）的扩展，通过添加 L2 正则项 来限制系数大小：

回归的目标函数：

$$
\min_\beta \|X\beta - y\|^2 + \alpha \|\beta\|^2
$$

- 第一项是残差平方和（与 OLS 相同）
- 第二项是正则项，惩罚模型系数过大
- $\alpha \geq 0$ 是正则化强度，越大越平滑

Ridge 分类（`RidgeClassifier`）：

- 原理类似，对线性分类器加上 L2 正则化。
- 使用平方损失（而不是对数损失）进行分类训练。

📌 注意事项

1. 正则化强度 $\alpha$ 的选择至关重要：

   - 太小 → 接近 OLS，可能过拟合；
   - 太大 → 可能欠拟合。
   - 可以通过交叉验证（如 `RidgeCV`）自动选择。

2. 不会执行特征选择：

   - Ridge 会将所有特征的系数缩小，但不会将它们完全变为零（不像 Lasso）。
   - 如果你希望进行特征选择，可考虑使用 Lasso 或 ElasticNet。

3. 对特征缩放敏感：

   - 因为正则项依赖于系数的大小，所以必须进行特征标准化（如使用 `StandardScaler`）。

4. 适用于高维场景：

   - Ridge 对多重共线性具有良好鲁棒性，适合维度大于样本数的情况。

5. 损失函数与逻辑回归不同：

   - RidgeClassifier 使用平方损失，非对数损失，因此与 `LogisticRegression` 在分类策略上略有不同。
