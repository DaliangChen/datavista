📌 使用场景

线性回归（OLS）适用于以下情况：

- 目标变量（因变量）是连续型数值。
- 假设输入变量（自变量）和目标变量之间存在线性关系。
- 需要快速、可解释的模型作为基线模型。
- 需要评估各特征对输出变量的影响（例如回归系数大小和符号）。

典型应用：

- 经济学预测（如房价、工资）
- 医疗数据分析（如药物剂量与疗效关系）
- 业务分析（如广告支出与销售额）

📌 基本原理

Ordinary Least Squares（OLS）线性回归的核心思想是：

- 拟合一条直线 $y = X\beta + \varepsilon$，使预测值 $\hat{y}$ 与真实值 $y$ 的残差平方和最小。

- 换句话说，最小化目标函数：

  $$
  \min_\beta ||X\beta - y||^2
  $$

- 其中：

  - $X$ 是特征矩阵
  - $y$ 是目标变量向量
  - $\beta$ 是需要学习的回归系数
  - $||\cdot||^2$ 表示平方和

- 解法通常使用正规方程或 QR 分解、SVD 等数值方法。

📌 注意事项

1. 线性假设： 模型假设特征与目标变量之间存在线性关系。如果实际是非线性关系，模型效果会变差。

2. 共线性问题：

   - 多个特征之间存在高度相关性时，会导致模型不稳定，系数不可靠。
   - 可通过 Ridge（L2）或 Lasso（L1）回归正则化处理。

3. 对异常值敏感：

   - OLS 对离群点非常敏感，可能导致整体预测偏离。
   - 可使用稳健回归（如 RANSAC）处理异常值较多的情况。

4. 特征缩放：

   - OLS 本身对特征缩放不敏感，但如果你后续要进行正则化或可解释性分析（如标准化系数大小），建议对特征进行标准化处理。

5. 模型评估：

   - 通常使用 $R^2$、MSE、MAE 等指标评估模型拟合优度。
   - 注意过拟合问题，尤其当特征数远多于样本数时。
