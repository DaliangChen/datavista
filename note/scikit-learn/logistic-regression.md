📌 使用场景

逻辑回归适合用于：

- 二分类和多分类问题，如信用风险评估、疾病诊断、文本分类等。
- 需要对结果进行概率预测，并且希望模型结果易于解释。
- 特征数不算特别大，数据线性可分或近似线性可分的情况。
- 希望快速建立基准模型，或者作为复杂模型的基础组件。
- 适用于大规模数据，Scikit-learn 实现支持多种优化算法，可扩展性好。

📌 基本原理

- 逻辑回归本质上是广义线性模型的一种，使用 sigmoid 函数将线性组合的输入映射为概率：

  $$
  P(y=1 | x) = \frac{1}{1 + e^{-(w^Tx + b)}}
  $$

- 模型通过最大化训练数据的对数似然函数（最大似然估计）来学习参数。

- 损失函数通常是 log-loss（对数损失）。

- 支持多种正则化方式（L1、L2、Elastic Net）以防止过拟合。

- 对多分类问题，常用策略有：

  - OvR（one-vs-rest）：训练多个二分类器。
  - Softmax 回归（多项逻辑回归）：直接处理多类。

📌 注意事项

1. 特征标准化重要：

   - 特别是带正则化时，未标准化的特征会影响收敛速度和模型性能。

2. 线性假设限制：

   - 逻辑回归假设类别之间是线性可分，若实际关系复杂可能欠拟合。

3. 正则化参数调节：

   - 参数 `C` 是正则化强度的倒数，需调优以平衡拟合和泛化。

4. 对于类别不平衡问题：

   - 需要调整样本权重或使用专门方法（如 `class_weight='balanced'`）。

5. 优化算法选择影响性能：

   - Scikit-learn 支持多种 solver，如 `liblinear`（适合小规模和 L1 正则），`saga`（适合大规模和 Elastic Net），`lbfgs`（适合多分类和 L2 正则）等。
   - 不同 solver 对正则类型和数据规模有不同适应性。

6. 多分类时注意 solver 兼容性：

   - 某些 solver 不支持 L1 正则或多分类。

7. 模型解释性强：

   - 系数反映特征对类别的影响方向和强度。
