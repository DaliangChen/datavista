📌 使用场景

分位数回归适用于以下情况：

- 不仅关注平均趋势（均值），而是关注目标变量的某个分位数（如中位数、上四分位数等）。
- 希望估计预测分布的上下边界（如 构建置信区间或预测区间）。
- 数据具有异方差性（即误差的方差随输入变量变化）；
- 数据中存在离群点，使用均值回归（如线性回归）可能失真。
- 常用于：

  - 风险控制（如 VaR：Value at Risk）
  - 医疗领域（预测不同群体间的分布差异）
  - 经济学、房价建模、交通预测等领域

📌 基本原理

- 分位数回归不是最小化均方误差（MSE），而是最小化 分位数损失函数（pinball loss / check function）。

- 给定一个分位数 $q \in (0, 1)$，目标是预测 $y$ 的第 $q$ 分位值 $\hat{y}_q$，使得：

  $$
  \min_{\beta} \sum_i \rho_q(y_i - X_i \beta)
  $$

  其中，损失函数定义为：

  $$
  \rho_q(u) =
  \begin{cases}
  q \cdot u, & \text{if } u \geq 0 \\
  (q - 1) \cdot u, & \text{if } u < 0
  \end{cases}
  $$

- 损失对正残差与负残差的惩罚不同，控制目标值朝目标分位数逼近；

- Scikit-learn 中的 `QuantileRegressor` 使用 L1 惩罚（可以稀疏化模型）并通过线性规划优化求解；

- 可设置多个 `quantile` 值分别训练，构建上下置信边界（如 0.1 与 0.9 分位数回归）。

📌 注意事项

1. 训练多个分位数模型时需单独训练：

   - 若想估计多个分位数（如 10%、50%、90%），需要分别训练多个模型。

2. 容易出现分位数交叉问题：

   - 即较低分位数的预测值高于较高分位数，可通过后处理或联合约束优化避免。

3. 训练时间可能高于普通线性回归：

   - 因为使用线性规划或坐标下降法解决非光滑损失，训练效率较低。

4. 默认使用 L1 正则化，可调节 `alpha` 控制稀疏性与拟合程度：

   - 适合高维特征选择，但也可能导致欠拟合。

5. 对异常值鲁棒：

   - 中位数回归（q = 0.5）对异常值比最小二乘回归更稳定。

6. 输入特征最好标准化：

   - 提高优化效率，避免不同量纲影响模型。

7. 适合构建预测区间：

   - 可结合多个分位数模型（例如 0.05 和 0.95）构建区间预测结果。
