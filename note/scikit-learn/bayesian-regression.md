📌 使用场景

贝叶斯回归适用于：

- 你希望不仅得到预测值，还希望获得 预测不确定性（置信区间）。
- 当数据量较小，且想用先验知识控制模型复杂度时。
- 对模型的泛化能力与不确定性量化有要求。
- 面对高维特征空间时，希望避免过拟合并保留解释性。
- 希望在模型中融入先验假设（例如参数稀疏性或正态分布等）。

典型应用：

- 医疗预测（需要置信区间）
- 风险评估和决策系统（要量化预测风险）
- 小样本机器学习场景

📌 基本原理

贝叶斯回归与普通线性回归不同的核心是 —— 将模型参数（如系数）看作随机变量，用贝叶斯方法来进行推断。

一般流程：

1. 设置先验分布（Prior）：

   - 对模型参数 $\beta$ 设定一个先验分布（如正态分布）。

2. 根据数据更新成后验分布（Posterior）：

   - 利用观测数据，计算参数的后验概率分布 $P(\beta | X, y)$。

3. 预测阶段输出分布：

   - 得到的是预测结果的分布而非单一值，可估计置信区间。

Scikit-learn 中提供两种实现：

- `BayesianRidge`: 贝叶斯岭回归，先验和后验都为高斯分布，结果平滑稳定；
- `ARDRegression`: 自动相关确定回归（Automatic Relevance Determination），在贝叶斯岭基础上加强了特征稀疏性控制能力，适合高维特征选择。

📌 注意事项

1. 训练成本高于普通线性回归：

   - 虽然计算效率还可以，但由于涉及后验估计，训练过程更复杂。

2. 对小样本效果好，对大样本可能不明显优于常规方法。

3. 需要标准化特征：

   - 贝叶斯岭回归中参数估计对量纲敏感，建议对输入做标准化。

4. 预测结果是概率分布，而不是单个点值：

   - 模型输出包括均值和标准差（用于置信区间），更适合不确定性敏感应用。

5. ARD 比 BayesianRidge 更适合特征选择：

   - ARD 会自动将无关特征的系数推向 0，得到稀疏模型；
   - 但相应的训练过程更复杂，且对数据更敏感。

6. 先验假设的合理性影响模型表现：

   - 如果先验设置不合理，可能导致欠拟合或错误的置信估计。
