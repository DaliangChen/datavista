📌 使用场景

- 数据集中存在异常值（离群点）或噪声点，传统最小二乘法对这些异常值敏感，可能导致模型偏差大。
- 需要对异常数据具有较强的鲁棒性，保证模型的稳定性和可靠性。
- 应用于金融风险评估、传感器数据分析、异常检测等对抗噪声要求高的场景。
- 适合误差分布非正态、存在极端异常的回归任务。

📌 基本原理

- 稳健回归通过减小异常点的影响来拟合模型，而非单纯最小化平方误差。
- 采用对异常值不敏感的损失函数，如 Huber 损失、Tukey 损失等。
- Scikit-learn 中主要有以下方法：

  - HuberRegressor：

    - 结合了平方损失和绝对值损失的优点，损失函数在误差较小时是二次的，误差较大时变为线性，降低异常值影响。

  - RANSACRegressor：

    - 基于随机采样一致性算法，随机选取子集拟合模型，筛选内点（符合模型的样本）迭代优化，自动剔除离群点。

  - TheilSenRegressor：

    - 基于中位数的回归方法，对异常值极其鲁棒，适合小样本数据和高维数据。

  - QuantileRegressor：

    - 用于估计条件分位数，能建模数据的不同分布位置，间接减少异常值影响。

📌 注意事项

1. 稳健回归通常比普通最小二乘法计算成本更高，尤其是 RANSAC 和 Theil-Sen 算法。
2. HuberRegressor 需要合理设置阈值参数（epsilon），影响异常点判定与权重分配。
3. RANSAC 需要调节内点阈值和最大迭代次数，过于严格或宽松都影响模型性能。
4. 稳健回归方法对噪声数据表现优异，但对所有数据异常不多时可能效果不明显。
5. 部分稳健方法对数据量有一定限制，Theil-Sen 适合小到中等规模数据。
6. 在使用前建议对数据做预处理和特征缩放，增强算法稳定性。
7. 不同稳健算法适合不同应用场景，应结合数据特性和计算资源选择。
