<link rel="stylesheet" href="../style.css">

# 1. Supervised learning

## 1.1. Linear Models

ç›®æ ‡å€¼é¢„æœŸæ˜¯ç‰¹å¾çš„çº¿æ€§ç»„åˆã€‚ç”¨æ•°å­¦ç¬¦å·è¡¨ç¤ºï¼Œå¦‚æœ $\hat{y}$
æ˜¯é¢„æµ‹å€¼ã€‚

$$\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p$$

æˆ‘ä»¬æŒ‡å®šå‘é‡ $w = (w_1,..., w_p)$
ä½œä¸º $coef_$ å’Œ $w_0$ä½œä¸º $intercept_$

### 1.1.1. Ordinary Least Squares

ğŸ“Œ ä¸»è¦æ€æƒ³

æˆ‘ä»¬å‡è®¾å› å˜é‡ $y$ ä¸è¾“å…¥å˜é‡ $x$ ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼š

$$
y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_p x_p + \varepsilon
$$

æ ¸å¿ƒç›®æ ‡ï¼šæœ€å°åŒ–æ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰

æˆ‘ä»¬å¸Œæœ›é€‰å‡ºä¸€ä¸ªæœ€ä¼˜çš„ $w$ï¼Œä½¿å¾—ï¼š

$$
\text{RSS}(w) = \sum_{i=1}^n (y_i - X_i w)^2
$$

å³ï¼šè®©æ¨¡å‹é¢„æµ‹å€¼ $Xw$ å°½å¯èƒ½æ¥è¿‘çœŸå®å€¼ $y$ã€‚

ğŸ“Œ æ³¨æ„äº‹é¡¹

| ç±»åˆ«     | æ³¨æ„ç‚¹                         | è¯´æ˜                                             |
| -------- | ------------------------------ | ------------------------------------------------ |
| æ•°æ®è¦æ±‚ | çº¿æ€§å…³ç³»å‡è®¾                   | ç›®æ ‡å˜é‡ y ä¸è¾“å…¥ X ä¹‹é—´è¦â€œè¿‘ä¼¼çº¿æ€§â€             |
| æ•°æ®é—®é¢˜ | å¼‚å¸¸å€¼å½±å“å¤§                   | å› ä¸ºç”¨çš„æ˜¯å¹³æ–¹è¯¯å·®ï¼Œå¼‚å¸¸å€¼æƒé‡å¤§ï¼Œå®¹æ˜“â€œæ‹‰æ­ªâ€æ¨¡å‹ |
| ç‰¹å¾å…³ç³» | ç‰¹å¾å…±çº¿æ€§ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰       | ç‰¹å¾ä¹‹é—´é«˜åº¦ç›¸å…³ä¼šå¯¼è‡´è§£ä¸ç¨³å®šï¼ˆæˆ–ä¸å¯é€†ï¼‰       |
| ç»´åº¦é—®é¢˜ | ç‰¹å¾ç»´åº¦é«˜äºæ ·æœ¬æ•°             | ä¼šå¯¼è‡´ $X^TX$ ä¸å¯é€†ï¼Œæ¨¡å‹æ— æ³•æ±‚è§£é—­å¼è§£         |
| è¯¯å·®å‡è®¾ | æ®‹å·®ç‹¬ç«‹ã€æ–¹å·®é½æ€§             | æ®‹å·®ï¼ˆè¯¯å·®é¡¹ï¼‰åº”æ»¡è¶³ i.i.dï¼Œè¯¯å·®æ–¹å·®ä¸€è‡´         |
| è¾“å…¥è§„æ¨¡ | ç‰¹å¾æœªæ ‡å‡†åŒ–å¯èƒ½å½±å“æ•°å€¼ç¨³å®šæ€§ | ç‰¹å¾å€¼å·®å¼‚å¤§æ—¶ï¼Œç³»æ•°éš¾ä»¥æ¯”è¾ƒï¼Œè®¡ç®—ç²¾åº¦ä¹Ÿå—å½±å“   |
| è¾“å‡ºè§£é‡Š | ç³»æ•°åªèƒ½è§£é‡Šâ€œè¾¹é™…çº¿æ€§å½±å“â€     | å¤šæ•°çœŸå®å…³ç³»æ˜¯éçº¿æ€§çš„ï¼Œä¸å®œè¿‡åº¦è§£è¯»ç³»æ•°         |

- çº¿æ€§å…³ç³»å‡è®¾
  - å¦‚æœçœŸå®å…³ç³»æ˜¯éçº¿æ€§çš„ï¼ŒOLS å°±æ— æ³•å¾ˆå¥½æ‹Ÿåˆã€‚
  - è§£å†³æ–¹æ³•ï¼šå¯ä»¥å¯¹ç‰¹å¾åšå¤šé¡¹å¼æ‰©å±•ã€log è½¬æ¢ï¼Œæˆ–è€…ç›´æ¥ç”¨éçº¿æ€§æ¨¡å‹ï¼ˆå¦‚æ ‘æ¨¡å‹ï¼‰ã€‚
- å¯¹å¼‚å¸¸å€¼ç‰¹åˆ«æ•æ„Ÿ
  - å› ä¸ºè¯¯å·®æ˜¯å¹³æ–¹çš„ï¼Œä¸€ä¸ªå¤§çš„åå·®ä¼šå¯¹æŸå¤±å‡½æ•°å½±å“å¾ˆå¤§ã€‚
  - è§£å†³æ–¹æ³•ï¼š
    - ä½¿ç”¨ é²æ£’å›å½’ï¼ˆå¦‚ Huber å›å½’ï¼‰
    - å¯¹æ•°æ®åšå¼‚å¸¸å€¼æ£€æµ‹æˆ– Winsorize
    - æ”¹ç”¨ L1 æŸå¤±ï¼ˆç»å¯¹å€¼è¯¯å·®ï¼‰æ¨¡å‹
- ç‰¹å¾å…±çº¿æ€§ï¼ˆMulticollinearityï¼‰
  - å½“ä¸¤ä¸ªæˆ–å¤šä¸ªç‰¹å¾é«˜åº¦ç›¸å…³æ—¶ï¼Œ$X^TX$ æ¥è¿‘ä¸å¯é€†ï¼ŒOLS è§£ä¼šéå¸¸ä¸ç¨³å®šï¼Œç³»æ•°å˜åŠ¨å¤§ã€‚
  - è§£å†³æ–¹æ³•ï¼š
    - ç”¨ Ridge å›å½’ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰æ¥ç¼“è§£å…±çº¿æ€§ï¼›
    - ä½¿ç”¨ PCA ç­‰é™ç»´æ–¹æ³•ï¼›
    - å»æ‰å†—ä½™ç‰¹å¾ï¼ˆå¦‚å˜é‡é€‰æ‹©ï¼‰
- ç‰¹å¾ç»´åº¦å¤§äºæ ·æœ¬æ•°
  - å¦‚æœ p > nï¼ŒOLS æ— æ³•å”¯ä¸€è§£å‡ºå‚æ•°ï¼ˆæ— è§£æˆ–å¤šè§£ï¼‰ã€‚
  - è§£å†³æ–¹æ³•ï¼šä½¿ç”¨ Ridgeï¼ˆèƒ½å¤„ç† p > n çš„é—®é¢˜ï¼‰ï¼Œæˆ–é™ç»´ã€‚
- è¯¯å·®é¡¹çš„ç»Ÿè®¡å‡è®¾
  - OLS ç»å…¸å‡è®¾åŒ…æ‹¬ï¼š
    - è¯¯å·®é¡¹ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.dï¼‰
    - è¯¯å·®æœŸæœ›ä¸º 0
    - æ–¹å·®ä¸€è‡´ï¼ˆHomoskedasticityï¼‰
  - å¦‚æœè¿™äº›å‡è®¾ä¸æˆç«‹ï¼Œä¼°è®¡é‡ä¾ç„¶æ— åï¼Œä½†ç»Ÿè®¡æ€§è´¨ï¼ˆå¦‚ç½®ä¿¡åŒºé—´ï¼‰å°±ä¸å¯é äº†ã€‚
  - è§£å†³æ–¹æ³•ï¼šç”¨ White æ ‡å‡†è¯¯ã€åŠ æƒæœ€å°äºŒä¹˜ï¼ˆWLSï¼‰ç­‰ã€‚
- ç‰¹å¾æœªæ ‡å‡†åŒ–å½±å“ç³»æ•°è§£é‡Š
  - å¦‚æœä¸€ä¸ªç‰¹å¾å€¼åœ¨ 0-1 ä¹‹é—´ï¼Œå¦ä¸€ä¸ªåœ¨ 0-10000ï¼ŒOLS çš„ç³»æ•°å°±ä¸å…·æœ‰æ¯”è¾ƒæ€§ã€‚
  - è§£å†³æ–¹æ³•ï¼šä½¿ç”¨ `StandardScaler` æ ‡å‡†åŒ–ç‰¹å¾ã€‚
- æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆå°¤å…¶æ˜¯é«˜ç»´ï¼‰
  - OLS æ²¡æœ‰æ­£åˆ™é¡¹ï¼Œå¯¹å°æ ·æœ¬/å¤šç‰¹å¾é—®é¢˜ç‰¹åˆ«å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
  - è§£å†³æ–¹æ³•ï¼š
    - ä½¿ç”¨ Ridge æˆ– Lasso å›å½’ï¼›
    - åšäº¤å‰éªŒè¯é€‰æ‹©æ¨¡å‹ï¼›
    - é™ç»´æˆ–ç‰¹å¾é€‰æ‹©ã€‚
- ç³»æ•°è§£é‡Šæœ‰é™
  - OLS çš„ç³»æ•°æ˜¯å¯¹å•ä¸€å˜é‡çš„â€œè¾¹é™…å½±å“â€è§£é‡Šï¼Œå‰ææ˜¯å…¶ä»–å˜é‡ä¸å˜ã€‚
    ä½†å¦‚æœå˜é‡ä¹‹é—´ç›¸å…³æ€§é«˜ï¼Œè¿™ç§è§£é‡Šæ˜¯ä¸å¯é çš„ã€‚

### 1.1.2. Ridge regression and classification

ğŸ“Œ ä¸»è¦æ€æƒ³

1. Ridge å›å½’çš„åŸºæœ¬å½¢å¼ï¼š

Ridge å›å½’æ˜¯åœ¨æ™®é€šæœ€å°äºŒä¹˜å›å½’ï¼ˆOLSï¼‰çš„åŸºç¡€ä¸Šæ·»åŠ äº† L2 æ­£åˆ™åŒ–é¡¹ï¼Œç›®çš„æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

å…¶ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š

$$
\min_w \|y - Xw\|_2^2 + \alpha \|w\|_2^2
$$

1. æ­£åˆ™åŒ–çš„ä½œç”¨ï¼š

- åœ¨æ•°æ®å…·æœ‰å…±çº¿æ€§ï¼ˆç‰¹å¾ä¹‹é—´é«˜åº¦ç›¸å…³ï¼‰æ—¶ï¼Œæ™®é€šæœ€å°äºŒä¹˜ä¼°è®¡ä¼šå˜å¾—ä¸ç¨³å®šã€‚
- å¼•å…¥æ­£åˆ™åŒ–é¡¹å¯ä»¥å¯¹æƒé‡ $w$ è¿›è¡Œæ”¶ç¼©ï¼Œä»è€Œå‡å°‘æ¨¡å‹å¤æ‚åº¦ã€æå‡ç¨³å®šæ€§ã€‚
- éšç€ $\alpha$ å¢å¤§ï¼Œæ¨¡å‹ç³»æ•°ä¼šè¢«è¿›ä¸€æ­¥å‹ç¼©ã€‚

5. ä½¿ç”¨å»ºè®®ï¼š

- å¯¹äºç‰¹å¾ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§çš„å›å½’é—®é¢˜ï¼ŒRidge å›å½’é€šå¸¸ä¼˜äºæ™®é€šçº¿æ€§å›å½’ã€‚
- å¦‚æœä½ æƒ³è¦åœ¨æ­£åˆ™åŒ–çš„åŒæ—¶è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆå³è®©æŸäº›ç³»æ•°ä¸ºé›¶ï¼‰ï¼Œåº”ä½¿ç”¨ Lasso å›å½’ï¼ˆL1 æ­£åˆ™ï¼‰ã€‚

ğŸ“Œ æ³¨æ„äº‹é¡¹

ä¸€ã€ç‰¹å¾éœ€è¦æ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰

- åŸå› ï¼š
  å²­å›å½’å¯¹ç‰¹å¾çš„å°ºåº¦æ•æ„Ÿï¼Œå› ä¸ºæ­£åˆ™åŒ–é¡¹ $\|w\|_2^2$ ç›´æ¥å¯¹æƒé‡æ–½åŠ çº¦æŸã€‚
- åšæ³•ï¼š
  ä½¿ç”¨ `StandardScaler` å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆå‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼‰ã€‚

äºŒã€æ­£åˆ™åŒ–ç³»æ•° $\alpha$ éœ€è¦è°ƒå‚

- å° Î±ï¼š æ­£åˆ™åŒ–å¼±ï¼Œæ¨¡å‹å¤æ‚ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ
- å¤§ Î±ï¼š æ­£åˆ™åŒ–å¼ºï¼Œæ¨¡å‹è¿‡äºç®€å•ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ
- æ¨èåšæ³•ï¼š

  - ä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆå¦‚ `GridSearchCV` æˆ– `RidgeCV`ï¼‰å¯»æ‰¾æœ€ä¼˜ Î±
  - å…¸å‹è°ƒå‚èŒƒå›´ï¼šå¦‚ $\alpha \in [10^{-3}, 10^3]$

ä¸‰ã€ä¸é€‚åˆåšç‰¹å¾é€‰æ‹©

- å²­å›å½’ä¸ä¼šå°†ç³»æ•°å‹ç¼©ä¸º 0ï¼Œå› æ­¤ä¸èƒ½è‡ªåŠ¨å‰”é™¤æ— å…³ç‰¹å¾ã€‚
- å¦‚æœç›®æ ‡æ˜¯å¯»æ‰¾æœ€é‡è¦çš„ç‰¹å¾ï¼Œåº”è€ƒè™‘ï¼š

  - Lasso å›å½’ï¼ˆL1 æ­£åˆ™ï¼‰
  - æˆ– ElasticNetï¼ˆL1 + L2 ç»“åˆï¼‰

å››ã€ç»“æœè§£é‡Šæ€§å¼±äºæ™®é€šçº¿æ€§å›å½’

- å› ä¸º Ridge ä¼šç¼©å°æ‰€æœ‰ç³»æ•°ï¼Œè§£é‡Šå˜é‡å’Œç›®æ ‡çš„ç›´æ¥çº¿æ€§å…³ç³»è¢«â€œå‹åˆ¶â€äº†ã€‚
- å¦‚æœä½ å…³å¿ƒå›å½’ç³»æ•°çš„å®é™…æ„ä¹‰ï¼ˆå¦‚åŒ»å­¦/ç¤¾ä¼šç§‘å­¦å»ºæ¨¡ï¼‰ï¼Œå¯èƒ½ä¸å¦‚çº¿æ€§å›å½’ç›´è§‚ã€‚

äº”ã€ç›®æ ‡å˜é‡ä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œä½†ç‰¹å¾å¿…é¡»

- $y$ ä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œå²­å›å½’å¯¹ $y$ æ²¡æœ‰ç‰¹åˆ«è¦æ±‚
- ä½†ç‰¹å¾ï¼ˆ$X$ï¼‰å¿…é¡»æ ‡å‡†åŒ–ï¼Œå¦åˆ™æ­£åˆ™åŒ–ä½œç”¨ä¼šå¤±è¡¡

å…­ã€é€‚ç”¨äºçº¿æ€§é—®é¢˜ï¼Œä¸é€‚åˆéçº¿æ€§å»ºæ¨¡

- å²­å›å½’åªèƒ½æ‹Ÿåˆçº¿æ€§æˆ–è¿‘ä¼¼çº¿æ€§å…³ç³»
- å¦‚æœæ•°æ®å­˜åœ¨æ˜¾è‘—çš„éçº¿æ€§å…³ç³»ï¼Œå¯ä»¥è€ƒè™‘ï¼š

  - åŠ å…¥å¤šé¡¹å¼ç‰¹å¾ï¼ˆ`PolynomialFeatures`ï¼‰
  - ä½¿ç”¨æ ¸æ–¹æ³•ï¼ˆå¦‚ `KernelRidge`ï¼‰

ä¸ƒã€ä¸è¦åœ¨ç¨€ç–æ•°æ®ä¸Šç›²ç›®ä½¿ç”¨ Ridge

- å¦‚æœä½ çš„è¾“å…¥æ•°æ®æ˜¯ç¨€ç–çŸ©é˜µï¼ˆå¦‚ TF-IDF æ–‡æœ¬æ•°æ®ï¼‰ï¼Œæ ‡å‡†åŒ–ä¼šç ´åç¨€ç–æ€§
- å»ºè®®ç”¨ä¸éœ€è¦æ ‡å‡†åŒ–çš„ç®—æ³•ï¼ˆå¦‚æœ´ç´ è´å¶æ–¯ï¼‰ï¼Œæˆ–è€…ç”¨ç‰¹åŒ–çš„æ­£åˆ™æ–¹æ³•ï¼ˆå¦‚ Lassoï¼‰

å…«ã€ä¸å²­åˆ†ç±»å™¨ï¼ˆRidgeClassifierï¼‰çš„åŒºåˆ«

- `Ridge` æ˜¯å›å½’æ¨¡å‹
- `RidgeClassifier` æ˜¯ä¸€ä¸ªå°†åˆ†ç±»è½¬åŒ–ä¸ºå›å½’æ±‚è§£çš„æ¨¡å‹ï¼Œä½†å…¶è®­ç»ƒæ–¹å¼ç•¥æœ‰ä¸åŒï¼Œä¸ä½¿ç”¨æ¦‚ç‡è¾“å‡ºï¼ˆä¸åƒ `LogisticRegression`ï¼‰

### 1.1.3. Lasso

ğŸ“Œ ä¸»è¦æ€æƒ³

Lasso å›å½’çš„ç›®æ ‡æ˜¯åœ¨æœ€å°åŒ–é¢„æµ‹è¯¯å·®çš„åŒæ—¶ï¼Œå¼•å…¥å¯¹æ¨¡å‹å‚æ•°çš„ç¨€ç–çº¦æŸï¼Œä»è€Œè¾¾åˆ°ç‰¹å¾é€‰æ‹©çš„æ•ˆæœã€‚

Lasso å›å½’é€šè¿‡åœ¨æ™®é€šæœ€å°äºŒä¹˜ï¼ˆOLSï¼‰çš„æŸå¤±å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ª L1 èŒƒæ•°ï¼ˆL1-normï¼‰ çš„æƒ©ç½šé¡¹ï¼Œçº¦æŸå›å½’ç³»æ•°çš„å¤§å°ï¼š

$$
\min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^n \left( y_i - X_i \cdot \beta \right)^2 + \alpha \sum_{j=1}^{p} |\beta_j| \right\}
$$

æ ¸å¿ƒæ€æƒ³æ€»ç»“ï¼š

| æ ¸å¿ƒç‚¹       | è¯´æ˜                                                                   |
| ------------ | ---------------------------------------------------------------------- |
| L1 æ­£åˆ™åŒ–    | é€šè¿‡ç»å¯¹å€¼æƒ©ç½šæ¥ä½¿éƒ¨åˆ†ç‰¹å¾çš„ç³»æ•°å˜ä¸º 0ï¼Œèµ·åˆ°â€œç‰¹å¾é€‰æ‹©â€çš„ä½œç”¨           |
| ç¨€ç–è§£       | ä¸ Ridge å›å½’ï¼ˆL2 æ­£åˆ™ï¼‰ä¸åŒï¼ŒLasso èƒ½äº§ç”Ÿç¨€ç–è§£ â€”â€” åªä¿ç•™æœ€æœ‰ç”¨çš„ç‰¹å¾ |
| é™ç»´èƒ½åŠ›     | è‡ªåŠ¨å»é™¤ä¸é‡è¦çš„ç‰¹å¾ï¼Œå°¤å…¶é€‚ç”¨äºé«˜ç»´æ•°æ®ï¼ˆp â‰« nï¼‰çš„æƒ…å†µ                |
| æ¨¡å‹è§£é‡Šæ€§å¼º | ä¿ç•™çš„ç‰¹å¾å°‘ï¼Œæ¨¡å‹æ›´æ˜“äºè§£é‡Š                                           |

é€‚ç”¨åœºæ™¯

- ç‰¹å¾å¾ˆå¤šï¼Œä½†æœŸæœ›åªä¿ç•™ä¸€éƒ¨åˆ†æ˜¾è‘—ç‰¹å¾ï¼›
- é«˜ç»´æ•°æ®ï¼ˆå¦‚åŸºå› æ•°æ®ã€æ–‡æœ¬åˆ†ç±»ç­‰ï¼‰ï¼›
- å¸Œæœ›æ„å»ºç®€å•ã€å¯è§£é‡Šçš„æ¨¡å‹ã€‚

ğŸ“Œ ä¸»è¦ç‰¹ç‚¹

| ç‰¹ç‚¹ç±»åˆ«       | å†…å®¹                                                             |     |     |
| -------------- | ---------------------------------------------------------------- | --- | --- |
| æ­£åˆ™ç±»å‹       | ä½¿ç”¨ L1 æ­£åˆ™åŒ–å¯¹æ¨¡å‹å‚æ•°æ–½åŠ çº¦æŸ                                 |
| ç‰¹å¾é€‰æ‹©èƒ½åŠ›   | èƒ½å°†éƒ¨åˆ†å›å½’ç³»æ•°å‹ç¼©ä¸º 0ï¼Œå³è‡ªåŠ¨å»é™¤ä¸é‡è¦çš„ç‰¹å¾ï¼ˆç¨€ç–è§£ï¼‰       |     |     |
| æ¨¡å‹å¯è§£é‡Šæ€§å¼º | å› ä¸ºåªä¿ç•™é‡è¦å˜é‡ï¼Œæ¨¡å‹æ›´ç®€å•ã€æ›´æ˜“è§£é‡Š                         |     |     |
| é˜²æ­¢è¿‡æ‹Ÿåˆ     | æ­£åˆ™é¡¹æœ‰åŠ©äºé™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œé¿å…åœ¨å°æ ·æœ¬ä¸‹è¿‡æ‹Ÿåˆ                 |     |     |
| è¶…å‚æ•°æ§åˆ¶     | é€šè¿‡æ­£åˆ™å¼ºåº¦ $\alpha$ æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ä¸å˜é‡ç¨€ç–æ€§ä¹‹é—´çš„æƒè¡¡       |     |     |
| é€‚ç”¨åœºæ™¯       | é«˜ç»´æ•°æ®ï¼ˆä¾‹å¦‚ p â‰« nï¼‰ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€åŸºå› æ•°æ®åˆ†æç­‰               |     |     |
| ä¸ Ridge åŒºåˆ«  | Lasso ç”¨ L1 æ­£åˆ™ï¼ˆå¯ç¨€ç–ï¼‰ï¼›Ridge ç”¨ L2 æ­£åˆ™ï¼ˆä¸ä¼šäº§ç”Ÿç¨€ç–ç³»æ•°ï¼‰ |     |     |
| å¤šé‡å…±çº¿æ€§å¤„ç† | å¯¹é«˜åº¦ç›¸å…³çš„å˜é‡ï¼ŒLasso å¾€å¾€åªä¿ç•™å…¶ä¸­ä¸€ä¸ªï¼Œä¸èƒ½å¾ˆå¥½åœ°å…±äº«æƒé‡   |     |     |

ğŸ“Œ æ³¨æ„äº‹é¡¹

ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆéå¸¸é‡è¦ï¼‰

- åŸå› ï¼šLasso ä¾èµ–æ­£åˆ™åŒ–æƒ©ç½šï¼ˆ$\alpha \sum |\beta_j|$ï¼‰ï¼Œè€Œç‰¹å¾çš„å°ºåº¦ä¼šå½±å“æƒ©ç½šåŠ›åº¦ã€‚
- åšæ³•ï¼šåœ¨ä½¿ç”¨ Lasso å‰å¿…é¡»å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œ æ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰æˆ–å½’ä¸€åŒ–ï¼ˆMinMaxScalerï¼‰ã€‚

æ­£åˆ™ç³»æ•° $\alpha$ çš„é€‰æ‹©

- å¤ªå°ï¼šæ¨¡å‹æ¥è¿‘æ™®é€šçº¿æ€§å›å½’ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼›
- å¤ªå¤§ï¼šæ¨¡å‹è¿‡åº¦ç¨€ç–ï¼Œé‡è¦ç‰¹å¾å¯èƒ½è¢«å‹ç¼©ä¸º 0ï¼Œå¯¼è‡´æ¬ æ‹Ÿåˆï¼›
- å»ºè®®ï¼šä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆå¦‚ `LassoCV`ï¼‰è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ $\alpha$ã€‚

å¯¹å…±çº¿æ€§ç‰¹å¾æ•æ„Ÿï¼ˆå˜é‡äº’ç›¸ç›¸å…³ï¼‰

- å¦‚æœå¤šä¸ªç‰¹å¾é«˜åº¦ç›¸å…³ï¼ŒLasso å¯èƒ½åªä¿ç•™å…¶ä¸­ä¸€ä¸ªï¼Œå…¶ä½™è®¾ä¸º 0ï¼›
- è§£å†³åŠæ³•ï¼š

  - å¯ä½¿ç”¨ ElasticNetï¼ˆç»“åˆ L1 å’Œ L2ï¼‰æ›´ç¨³å¥åœ°å¤„ç†å…±çº¿æ€§ï¼›
  - æˆ–æå‰ä½¿ç”¨é™ç»´æ–¹æ³•å¦‚ PCAï¼›

ç¨€ç–æ€§ä¸æ˜¯ä¸‡èƒ½çš„

- ç¨€ç–è§£é€‚åˆè§£é‡Šæ€§è¦æ±‚é«˜çš„ä»»åŠ¡ï¼›
- å¦‚æœç›®æ ‡æ˜¯æœ€å¤§åŒ–é¢„æµ‹ç²¾åº¦ï¼Œè€Œä¸æ˜¯è§£é‡Šå˜é‡ï¼ŒLasso å¯èƒ½ä¸æ˜¯æœ€ä½³é€‰æ‹©ï¼›
- å»ºè®®æ ¹æ®ä»»åŠ¡ç›®æ ‡é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚

å°æ ·æœ¬ + é«˜ç»´åº¦æ—¶è¦è°¨æ…

- è™½ç„¶ Lasso é€‚ç”¨äºé«˜ç»´ï¼Œä½†å¦‚æœæ ·æœ¬æ•°é‡æå°‘ï¼ˆn â‰ª pï¼‰ï¼Œç»“æœå¯èƒ½ä¸ç¨³å®šï¼›
- ç‰¹åˆ«æ˜¯åœ¨é‡è¦ç‰¹å¾è¢«é”™è¯¯å‹ä¸º 0 çš„é£é™©ä¸‹ï¼Œå»ºè®®ä½¿ç”¨æ›´é²æ£’çš„æ–¹å¼ï¼ˆå¦‚ bootstrap éªŒè¯ç‰¹å¾ç¨³å®šæ€§ï¼‰ï¼›

è¾“å‡ºç³»æ•°å®¹æ˜“éœ‡è¡ï¼ˆä¸ç¨³å®šï¼‰

- Lasso å›å½’çš„è¾“å‡ºç»“æœå¯¹æ•°æ®æ‰°åŠ¨è¾ƒæ•æ„Ÿï¼›
- å¯¹äºâ€œè¾¹ç¼˜é‡è¦â€çš„å˜é‡ï¼Œå…¶æ˜¯å¦è¢«é€‰ä¸­å¯èƒ½éšç€æ ·æœ¬ç¨å¾®å˜åŠ¨è€Œä¸åŒï¼›
- å¯ä½¿ç”¨æ¨¡å‹ç¨³å®šæ€§åˆ†æï¼ˆstability selectionï¼‰è¾…åŠ©åˆ¤æ–­ç‰¹å¾æ˜¯å¦å¯ä¿¡ã€‚

ç›®æ ‡å˜é‡ä¸è¦æ ‡å‡†åŒ–

- ç‰¹å¾éœ€è¦æ ‡å‡†åŒ–
- ç›®æ ‡å˜é‡ y é€šå¸¸ä¸è¦æ ‡å‡†åŒ–ï¼Œå¦åˆ™é¢„æµ‹å€¼è§£é‡Šä¼šå˜å¤æ‚ï¼›
- é™¤éä½ ä¸“é—¨å¯¹ y æœ‰åˆ†å¸ƒçº¦æŸæˆ–é¢„æµ‹èŒƒå›´è¦æ±‚ã€‚

### 1.1.4. Multi-task Lasso

ğŸ“Œ ä¸»è¦æ€æƒ³

åœ¨å…·æœ‰å¤šä¸ªç›¸å…³è¾“å‡ºå˜é‡çš„çº¿æ€§å›å½’é—®é¢˜ä¸­ï¼ŒåŒæ—¶è¿›è¡Œæ‰€æœ‰ä»»åŠ¡çš„ç¨€ç–ç‰¹å¾é€‰æ‹©ï¼Œä»¥åˆ©ç”¨ä»»åŠ¡é—´çš„å…±åŒç»“æ„ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ä¸æ¨¡å‹ç®€æ´æ€§ã€‚

æ•°å­¦è¡¨è¾¾ï¼ˆä¸æ™®é€š Lasso çš„åŒºåˆ«ï¼‰

å¯¹äºå¤šä¸ªè¾“å‡ºå˜é‡ï¼ˆæ¯”å¦‚ $Y \in \mathbb{R}^{n \times T}$ï¼Œæœ‰ T ä¸ªä»»åŠ¡ï¼‰ï¼ŒMultiTaskLasso ä¼˜åŒ–ä»¥ä¸‹ç›®æ ‡ï¼š

$$
\min_{W} \left\{ \frac{1}{2n} \| Y - XW \|_F^2 + \alpha \sum_{j=1}^{p} \| W_{j, :} \|_2 \right\}
$$

æ ¸å¿ƒæœºåˆ¶

| ç‰¹ç‚¹                       | è¯´æ˜                                                                            |
| -------------------------- | ------------------------------------------------------------------------------- |
| L2/L1 ç»„åˆæ­£åˆ™åŒ–           | å¯¹æ¯ä¸€åˆ—ç‰¹å¾åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„ç³»æ•°ä½¿ç”¨ L2 èŒƒæ•°ï¼Œä½†æ‰€æœ‰ç‰¹å¾åŠ å’Œä»æ˜¯ç¨€ç–çš„ï¼ˆL1 ç»“æ„ï¼‰ |
| è¡Œç¨€ç–æ€§ï¼ˆGroup sparsityï¼‰ | è‹¥æŸç‰¹å¾å¯¹æ‰€æœ‰ä»»åŠ¡éƒ½ä¸é‡è¦ï¼Œå…¶å¯¹åº”çš„æ•´è¡Œ $W_{j,:}$ ä¼šè¢«å‹ç¼©ä¸º 0                 |
| å…±äº«ç‰¹å¾é€‰æ‹©               | æ‰€æœ‰ä»»åŠ¡å…±äº«åŒä¸€ç»„è¢«é€‰æ‹©çš„ç‰¹å¾ï¼Œé€‚åˆå¤šä¸ªè¾“å‡ºé«˜åº¦ç›¸å…³çš„æƒ…å†µ                      |
| ä»»åŠ¡é—´ä¿¡æ¯å…±äº«             | ç›¸æ¯”ç‹¬ç«‹è®­ç»ƒå¤šä¸ª Lassoï¼Œæ›´èƒ½åˆ©ç”¨å¤šä¸ªè¾“å‡ºé—´çš„å…³è”ä¿¡æ¯æé«˜æ³›åŒ–èƒ½åŠ›                |

ä¸æ™®é€š Lasso çš„åŒºåˆ«

| å¯¹æ¯”é¡¹   | Lasso                        | MultiTaskLasso                     |
| -------- | ---------------------------- | ---------------------------------- |
| é€‚ç”¨ä»»åŠ¡ | å•ä»»åŠ¡ï¼ˆä¸€ä¸ªè¾“å‡ºï¼‰           | å¤šä»»åŠ¡ï¼ˆå¤šä¸ªè¾“å‡ºï¼‰                 |
| ç‰¹å¾é€‰æ‹© | æ¯ä¸ªä»»åŠ¡å•ç‹¬å†³å®šæ˜¯å¦ä½¿ç”¨ç‰¹å¾ | æ‰€æœ‰ä»»åŠ¡å…±äº«ç›¸åŒç‰¹å¾é€‰æ‹©           |
| ç¨€ç–ç»“æ„ | æ¯ä¸ªä»»åŠ¡ç¨€ç–ç‹¬ç«‹             | è¡Œç¨€ç–ï¼ˆæŸç‰¹å¾åŒæ—¶ä¸ºæ‰€æœ‰ä»»åŠ¡ç½® 0ï¼‰ |
| æ­£åˆ™åŒ–   | L1ï¼ˆå•ä¸ªå‚æ•°ï¼‰               | L2/L1 æ··åˆï¼ˆç»„ç¨€ç–ï¼‰               |

ğŸ“Œ ä¸»è¦ç‰¹ç‚¹

| ç‰¹ç‚¹ç±»åˆ«       | è¯´æ˜                                                                                |
| -------------- | ----------------------------------------------------------------------------------- |
| å¤šä»»åŠ¡å¤„ç†     | åŒæ—¶æ‹Ÿåˆå¤šä¸ªç›¸å…³çš„å›å½’ä»»åŠ¡ï¼Œè¾“å‡ºå¤šä¸ªè¿ç»­å˜é‡çš„é¢„æµ‹ç»“æœã€‚                            |
| å…±äº«ç‰¹å¾é€‰æ‹©   | é€šè¿‡ç»„ç¨€ç–æ­£åˆ™åŒ–ï¼Œæ‰€æœ‰ä»»åŠ¡å…±äº«ç›¸åŒçš„ç‰¹å¾å­é›†ï¼Œå®ç°ç»Ÿä¸€çš„ç‰¹å¾é€‰æ‹©ã€‚                  |
| ç»„ç¨€ç–ç»“æ„     | å¯¹æ¯ä¸ªç‰¹å¾å¯¹åº”çš„æ‰€æœ‰ä»»åŠ¡ç³»æ•°ä¸€èµ·æ­£åˆ™åŒ–ï¼ˆL2 èŒƒæ•°ï¼‰ï¼Œè‹¥è¯¥ç»„ç³»æ•°å…¨ä¸ºé›¶åˆ™è¯¥ç‰¹å¾è¢«ä¸¢å¼ƒã€‚ |
| æ­£åˆ™åŒ–ç±»å‹     | ä½¿ç”¨æ··åˆçš„ L1/L2 æ­£åˆ™åŒ–ï¼Œç»“åˆäº† Lasso çš„ç¨€ç–æ€§å’Œå¤šä»»åŠ¡çš„ç»“æ„ä¿¡æ¯ã€‚                  |
| æé«˜æ³›åŒ–èƒ½åŠ›   | é€šè¿‡åˆ©ç”¨ä»»åŠ¡é—´çš„ç›¸å…³æ€§ï¼Œæå‡æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„é¢„æµ‹å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚                  |
| é¿å…è¿‡æ‹Ÿåˆ     | æ­£åˆ™åŒ–å¸®åŠ©å‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼Œé¿å…åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚                          |
| é€‚ç”¨äºé«˜ç»´æ•°æ® | å¯ä»¥å¤„ç†é«˜ç»´ç‰¹å¾æ•°æ®ï¼Œè‡ªåŠ¨ç­›é€‰å‡ºé‡è¦ç‰¹å¾ï¼Œå‡å°æ¨¡å‹è§„æ¨¡ã€‚                            |
| æ¨¡å‹è§£é‡Šæ€§å¼º   | å› ä¸ºå…±äº«ç‰¹å¾é€‰æ‹©ï¼Œä¾¿äºç†è§£å¤šä¸ªä»»åŠ¡é—´çš„å…±åŒå½±å“å› ç´ ã€‚                                |

ğŸ“Œ æ³¨æ„äº‹é¡¹

ç‰¹å¾é¢„å¤„ç†å¿…ä¸å¯å°‘

- å¿…é¡»å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆå‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼‰ï¼Œå¦åˆ™æ­£åˆ™åŒ–æƒ©ç½šä¼šå› ç‰¹å¾å°ºåº¦ä¸åŒè€Œå¤±æ•ˆã€‚
- å¤šä»»åŠ¡æ•°æ®é€šå¸¸ç»´åº¦è¾ƒé«˜ï¼Œæ ‡å‡†åŒ–å¯¹æ¨¡å‹ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦éƒ½éå¸¸å…³é”®ã€‚

æ­£åˆ™åŒ–å‚æ•° $\alpha$ è°ƒèŠ‚

- $\alpha$ æ§åˆ¶æ¨¡å‹çš„ç¨€ç–æ€§å’Œæ‹Ÿåˆç¨‹åº¦ï¼š

  - $\alpha$ å¤ªå¤§ï¼Œä¼šå¯¼è‡´è¿‡åº¦ç¨€ç–ï¼Œé‡è¦ç‰¹å¾è¢«å¿½ç•¥ï¼›
  - $\alpha$ å¤ªå°ï¼Œæ¨¡å‹å¤æ‚ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆã€‚

- æ¨èä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆä¾‹å¦‚ `MultiTaskLassoCV`ï¼‰è‡ªåŠ¨è°ƒä¼˜ã€‚

ä»»åŠ¡ç›¸å…³æ€§å‡è®¾

- MultiTaskLasso å‡è®¾å¤šä¸ªä»»åŠ¡ä¹‹é—´å…±äº«ç›¸åŒé‡è¦ç‰¹å¾ï¼Œå³ä»»åŠ¡é—´å­˜åœ¨ç›¸å…³æ€§ã€‚
- è‹¥ä»»åŠ¡å·®å¼‚å¾ˆå¤§ï¼Œå¼ºåˆ¶å…±äº«ç‰¹å¾å¯èƒ½åè€Œé™ä½æ€§èƒ½ã€‚
- æ­¤æ—¶åº”è€ƒè™‘å•ç‹¬å»ºæ¨¡æˆ–ä½¿ç”¨æ›´çµæ´»çš„å¤šä»»åŠ¡æ¨¡å‹ã€‚

é«˜åº¦ç›¸å…³ç‰¹å¾çš„å¤„ç†

- è™½ç„¶ MultiTaskLasso é€šè¿‡ç»„ç¨€ç–é€‰æ‹©ç‰¹å¾ï¼Œä½†å¯¹é«˜åº¦å…±çº¿æ€§ç‰¹å¾ä»ç„¶æ•æ„Ÿã€‚
- å¯ç»“åˆ ElasticNet å¤šä»»åŠ¡ç‰ˆæˆ–å…ˆåšé™ç»´ã€ç‰¹å¾ç­›é€‰ã€‚

æ ·æœ¬æ•°é‡è¦æ±‚

- å¤šä»»åŠ¡æ¨¡å‹éœ€è¦è¶³å¤Ÿçš„æ ·æœ¬æ•°æ”¯æŒå¤šè¾“å‡ºæ‹Ÿåˆï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆæˆ–ä¸ç¨³å®šã€‚
- æ ·æœ¬æ•°è¿œå°‘äºç‰¹å¾æ•°æ—¶ï¼Œç»“æœå¯èƒ½ä¸ç¨³å®šï¼Œéœ€è¦è°¨æ…ã€‚

è§£é‡Šå’Œä½¿ç”¨åœºæ™¯

- é€‚åˆå¤šä¸ªè¾“å‡ºå˜é‡ç›¸å…³ä¸”å…±äº«å½±å“å› ç´ çš„åœºæ™¯ã€‚
- ä¸é€‚åˆä»»åŠ¡å®Œå…¨ç‹¬ç«‹æˆ–ç‰¹å¾å½±å“æœºåˆ¶å·®å¼‚æå¤§çš„æƒ…å†µã€‚

ç›®æ ‡å˜é‡æ ‡å‡†åŒ–

- ç›®æ ‡å˜é‡ä¸€èˆ¬ä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œä½†è‹¥ä¸åŒä»»åŠ¡é‡çº²ç›¸å·®å¾ˆå¤§ï¼Œé€‚å½“å½’ä¸€åŒ–æœ‰åŠ©äºå¹³è¡¡è®­ç»ƒã€‚

### 1.1.5. Elastic-Net

ğŸ“Œ ä¸»è¦æ€æƒ³

1. ç›®æ ‡å‡½æ•°

Elastic Net çš„ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å°åŒ–ä¸‹é¢çš„æŸå¤±å‡½æ•°ï¼š

$$
\min_{\beta} \left\{ \frac{1}{2n} \| y - X\beta \|_2^2 + \alpha \left( \rho \|\beta\|_1 + \frac{1 - \rho}{2} \|\beta\|_2^2 \right) \right\}
$$

2. æ ¸å¿ƒæ€æƒ³

- ç»“åˆ L1 å’Œ L2 æ­£åˆ™åŒ–çš„ä¼˜ç‚¹ï¼šL1 å¸¦æ¥æ¨¡å‹ç¨€ç–æ€§ï¼ˆå˜é‡é€‰æ‹©ï¼‰ï¼ŒL2 å¸¦æ¥æ¨¡å‹ç¨³å®šæ€§å’Œå¤„ç†å¤šé‡å…±çº¿æ€§ï¼›
- è§£å†³ Lasso åœ¨é«˜åº¦ç›¸å…³ç‰¹å¾ä¸­çš„ä¸ç¨³å®šé€‰æ‹©é—®é¢˜ï¼šElastic Net å…è®¸ç›¸å…³ç‰¹å¾å…±åŒè¿›å…¥æ¨¡å‹ï¼Œé¿å…äº† Lasso åªé€‰æ‹©å…¶ä¸­ä¸€ä¸ªçš„ç¼ºé™·ï¼›
- é€‚åˆé«˜ç»´æ•°æ®ï¼šåœ¨ $p \gg n$ çš„åœºæ™¯ä¸‹ï¼ŒElastic Net ä¾ç„¶è¡¨ç°ä¼˜å¼‚ã€‚

3. ä¼˜åŠ¿æ€»ç»“

| ä¼˜åŠ¿                 | è¯´æ˜                                       |
| -------------------- | ------------------------------------------ |
| å˜é‡é€‰æ‹©ä¸æ­£åˆ™åŒ–ç»“åˆ | æ—¢èƒ½è‡ªåŠ¨é€‰æ‹©é‡è¦å˜é‡ï¼Œåˆèƒ½æ§åˆ¶æ¨¡å‹å¤æ‚åº¦   |
| å¤„ç†å…±çº¿æ€§           | å¯¹é«˜åº¦ç›¸å…³ç‰¹å¾æ›´ç¨³å®šï¼Œä¸éšæ•°æ®æ³¢åŠ¨éšæœºé€‰æ‹© |
| é€‚åˆé«˜ç»´æ•°æ®         | é€‚åˆç‰¹å¾æ•°å¤§äºæ ·æœ¬æ•°çš„æƒ…å†µ                 |
| çµæ´»è°ƒèŠ‚å‚æ•°         | $\rho$ æ§åˆ¶ L1/L2 æ¯”ä¾‹ï¼Œæ¨¡å‹å¯å®šåˆ¶         |

ğŸ“Œ ä¸»è¦ç‰¹ç‚¹

| ç‰¹ç‚¹ç±»åˆ«           | è¯´æ˜                                                                  |
| ------------------ | --------------------------------------------------------------------- |
| ç»“åˆ L1 å’Œ L2 æ­£åˆ™ | åŒæ—¶ä½¿ç”¨ L1ï¼ˆç¨€ç–ï¼‰å’Œ L2ï¼ˆç¨³å®šï¼‰æ­£åˆ™ï¼Œèåˆä¸¤è€…ä¼˜åŠ¿ã€‚                  |
| è‡ªåŠ¨ç‰¹å¾é€‰æ‹©       | L1 æ­£åˆ™é¡¹ä½¿éƒ¨åˆ†ç³»æ•°å˜ä¸ºé›¶ï¼Œå®ç°ç‰¹å¾ç­›é€‰ã€‚                             |
| å¤„ç†å…±çº¿æ€§         | L2 æ­£åˆ™é¡¹å‡ç¼“é«˜åº¦ç›¸å…³ç‰¹å¾é—´çš„éšæœºé€‰æ‹©é—®é¢˜ï¼Œä½¿ç›¸å…³å˜é‡å¯ä»¥è¢«ä¸€èµ·ä¿ç•™ã€‚ |
| é€‚åˆé«˜ç»´æ•°æ®       | å¯¹äºç‰¹å¾æ•°è¿œå¤§äºæ ·æœ¬æ•°çš„åœºæ™¯ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚                              |
| å‚æ•°çµæ´»è°ƒèŠ‚       | é€šè¿‡å‚æ•° $\rho$ çµæ´»æ§åˆ¶ L1 å’Œ L2 æ¯”ä¾‹ï¼Œé€‚åº”ä¸åŒæ•°æ®ç‰¹æ€§ã€‚            |
| æ¨¡å‹ç¨³å®šæ€§å¼º       | ç›¸æ¯”çº¯ Lassoï¼Œæ¨¡å‹åœ¨æ•°æ®æ³¢åŠ¨æ—¶æ›´ç¨³å®šï¼Œé¿å…è¿‡åº¦ç¨€ç–ã€‚                  |
| å¹¿æ³›åº”ç”¨           | åœ¨åŸºå› è¡¨è¾¾ã€é‡‘èå»ºæ¨¡ã€æ–‡æœ¬æŒ–æ˜ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚                        |

ğŸ“Œ æ³¨æ„äº‹é¡¹

ç‰¹å¾é¢„å¤„ç†

- å¿…é¡»å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰ï¼Œå¦åˆ™æ­£åˆ™é¡¹ä¼šå› ç‰¹å¾å°ºåº¦ä¸åŒå¯¼è‡´æƒ©ç½šä¸å‡ã€‚
- æ ‡å‡†åŒ–æœ‰åŠ©äºç®—æ³•æ”¶æ•›å’Œå‚æ•°è§£é‡Šã€‚

å‚æ•°è°ƒèŠ‚

- Elastic Net æœ‰ä¸¤ä¸ªé‡è¦è¶…å‚æ•°ï¼š

  - $\alpha$ï¼šæ•´ä½“æ­£åˆ™åŒ–å¼ºåº¦ï¼›
  - $\rho$ï¼ˆæˆ– l1_ratioï¼‰ï¼šæ§åˆ¶ L1 å’Œ L2 æ­£åˆ™åŒ–çš„æƒé‡æ¯”ä¾‹ã€‚

- éœ€è¦ä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆå¦‚ `ElasticNetCV`ï¼‰æ¥è‡ªåŠ¨å¯»æ‰¾æœ€ä½³ç»„åˆï¼Œé¿å…æ¬ æ‹Ÿåˆæˆ–è¿‡æ‹Ÿåˆã€‚

å¯¹å…±çº¿æ€§çš„ä¼˜åŠ¿

- Elastic Net æ¯”çº¯ Lasso æ›´é€‚åˆé«˜åº¦ç›¸å…³ç‰¹å¾çš„åœºæ™¯ï¼Œä½†åœ¨æç«¯å¤šé‡å…±çº¿æ€§æ—¶ä»éœ€è°¨æ…ã€‚
- å¯ä»¥ç»“åˆé™ç»´æˆ–ç‰¹å¾é€‰æ‹©æ–¹æ³•æé«˜è¡¨ç°ã€‚

ç¨€ç–æ€§ä¸ç¨³å®šæ€§çš„æƒè¡¡

- $\rho$ è¶‹è¿‘äº 1ï¼Œæ¨¡å‹æ›´ç¨€ç–ï¼Œä½†ç¨³å®šæ€§å¯èƒ½ä¸‹é™ï¼›
- $\rho$ è¶‹è¿‘äº 0ï¼Œæ¨¡å‹æ›´å¹³æ»‘ç¨³å®šï¼Œä½†ç¨€ç–æ€§å¼±ï¼›
- æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ã€‚

è®¡ç®—èµ„æº

- Elastic Net è®¡ç®—ç›¸å¯¹ Ridge å’Œ Lasso ç•¥å¤æ‚ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ•°æ®æ—¶ï¼Œæ³¨æ„è®¡ç®—æˆæœ¬ã€‚

ç›®æ ‡å˜é‡å¤„ç†

- ä¸€èˆ¬ä¸éœ€è¦å¯¹ç›®æ ‡å˜é‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œé™¤éä¸åŒä»»åŠ¡é‡çº²å·®å¼‚å¤§ã€‚

è§£é‡Šæ€§

- ç”±äºéƒ¨åˆ†ç³»æ•°ä¸ä¸ºé›¶ï¼Œæ¨¡å‹æœ‰ä¸€å®šçš„å¯è§£é‡Šæ€§ï¼Œä½†æ³¨æ„ç³»æ•°å—å‚æ•°å½±å“è¾ƒå¤§ï¼Œéœ€è°¨æ…è§£è¯»ã€‚

### 1.1.6. Multi-task Elastic-Net

ğŸ“Œ ä¸»è¦æ€æƒ³

åœ¨å¤šä¸ªç›¸å…³å›å½’ä»»åŠ¡ä¸­ï¼ŒåŒæ—¶ä½¿ç”¨ L1ï¼ˆç¨€ç–ï¼‰å’Œ L2ï¼ˆç¨³å®šï¼‰æ­£åˆ™åŒ–ï¼Œå¹¶é€šè¿‡å…±äº«ç‰¹å¾é€‰æ‹©ï¼ˆè¡Œç¨€ç–ï¼‰ï¼Œæå‡æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

æ•°å­¦è¡¨è¾¾å¼

$$
\min_{W} \left\{ \frac{1}{2n} \| Y - XW \|_F^2 + \alpha \left( \rho \sum_{j=1}^p \|W_{j,:}\|_2 + \frac{1 - \rho}{2} \|W\|_F^2 \right) \right\}
$$

| ç‰¹ç‚¹             | æè¿°                                                           |
| ---------------- | -------------------------------------------------------------- |
| å¤šä»»åŠ¡å›å½’       | åŒæ—¶é¢„æµ‹å¤šä¸ªè¾“å‡ºï¼ˆå¤šä¸ª Yï¼‰ï¼Œè€Œä¸æ˜¯å¤šä¸ªç‹¬ç«‹å›å½’                 |
| ä»»åŠ¡å…±äº«ç‰¹å¾é€‰æ‹© | æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨åŒä¸€ç»„é‡è¦ç‰¹å¾ï¼ˆè¡Œç¨€ç–ç»“æ„ï¼‰                       |
| L1 + L2 æ­£åˆ™ç»“åˆ | æä¾› Lasso çš„ç¨€ç–æ€§ + Ridge çš„ç¨³å®šæ€§ï¼Œç‰¹åˆ«é€‚åˆç›¸å…³ç‰¹å¾å¤šçš„æƒ…å½¢ |
| ç¨³å®š + ç¨€ç–      | ç›¸æ¯” MultiTaskLassoï¼Œæ›´ç¨³å¥ã€æ›´æŠ—å…±çº¿æ€§                        |
| é€‚åˆé«˜ç»´å¤šä»»åŠ¡   | åœ¨æ ·æœ¬æ•°å°ã€ç‰¹å¾å¤šã€å¤šä»»åŠ¡çš„å®é™…é—®é¢˜ä¸­è¡¨ç°ä¼˜ç§€                 |

é€‚ç”¨åœºæ™¯

- å¤šè¾“å‡ºå˜é‡é«˜åº¦ç›¸å…³ï¼›
- å¸Œæœ›å¤šä¸ªä»»åŠ¡å…±äº«ç›¸åŒçš„ç‰¹å¾å­é›†ï¼ˆè§£é‡Šæ€§å¥½ï¼‰ï¼›
- ç‰¹å¾æ•°å¤šã€æ ·æœ¬å°‘ï¼ˆå¦‚åŸºå› æ•°æ®ã€ä¼ æ„Ÿå™¨ä¿¡å·ç­‰ï¼‰ï¼›
- å­˜åœ¨ç‰¹å¾å…±çº¿æ€§ï¼Œéœ€è¦æ¨¡å‹ç¨³å¥ï¼›

ğŸ“Œ ä¸»è¦ç‰¹ç‚¹

| ç±»åˆ«                   | è¯´æ˜                                                                                                         |
| ---------------------- | ------------------------------------------------------------------------------------------------------------ |
| å¤šä»»åŠ¡å»ºæ¨¡             | å¯ä»¥åŒæ—¶é¢„æµ‹å¤šä¸ªç›¸å…³è¾“å‡ºå˜é‡ï¼ˆå›å½’ä»»åŠ¡ï¼‰ï¼Œæå‡ä»»åŠ¡é—´ååŒå»ºæ¨¡èƒ½åŠ›ã€‚                                           |
| å…±äº«ç‰¹å¾é€‰æ‹©ï¼ˆè¡Œç¨€ç–ï¼‰ | å¯¹æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾å­é›†ï¼Œæå‡æ¨¡å‹ç®€æ´æ€§å’Œå¯è§£é‡Šæ€§ã€‚å³ï¼Œå¦‚æœæŸä¸ªç‰¹å¾å¯¹æ‰€æœ‰ä»»åŠ¡éƒ½ä¸é‡è¦ï¼Œå®ƒä¼šæ•´ä½“è¢«å‰”é™¤ã€‚ |
| èåˆ L1 ä¸ L2 æ­£åˆ™åŒ–   | åŒæ—¶å…·å¤‡ Lasso çš„ç‰¹å¾é€‰æ‹©èƒ½åŠ›ï¼ˆL1ï¼‰å’Œ Ridge çš„æŠ—å…±çº¿æ€§èƒ½åŠ›ï¼ˆL2ï¼‰ï¼Œé€šè¿‡ `l1_ratio` æ§åˆ¶å¹³è¡¡ã€‚                 |
| æ¨¡å‹æ›´ç¨³å®š             | ç›¸æ¯” MultiTaskLassoï¼Œå¯¹ç‰¹å¾å…±çº¿æ€§æ›´é²æ£’ï¼Œè¾“å‡ºç³»æ•°æ›´å¹³æ»‘ã€ä¸å®¹æ˜“éœ‡è¡ã€‚                                        |
| é€‚åˆé«˜ç»´å¤šè¾“å‡ºé—®é¢˜     | å°¤å…¶é€‚åˆæ ·æœ¬æ•°å°‘ã€ç‰¹å¾æ•°å¤šã€å¤šè¾“å‡ºå˜é‡åŒæ—¶é¢„æµ‹çš„å¤æ‚é—®é¢˜ã€‚                                                   |
| å¯è°ƒèŠ‚æ€§å¼º             | å¯é€šè¿‡ `alpha` å’Œ `l1_ratio` æ§åˆ¶æ•´ä½“æ­£åˆ™å¼ºåº¦å’Œ L1/L2 æƒé‡ï¼Œå®ç°æ›´çµæ´»çš„æ¨¡å‹è°ƒèŠ‚ã€‚                           |

ğŸ“Œ æ³¨æ„äº‹é¡¹

ç‰¹å¾æ ‡å‡†åŒ–æ˜¯å¿…é¡»çš„

- Elastic Net ä¾èµ– L1/L2 æ­£åˆ™é¡¹ï¼Œå¦‚æœç‰¹å¾é‡çº²ä¸ä¸€è‡´ï¼Œä¼šå¯¼è‡´æ­£åˆ™æƒ©ç½šå¤±è¡¡ã€‚
- å¿…é¡»å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆå¦‚ `StandardScaler`ï¼‰ï¼š

è¶…å‚æ•°è°ƒèŠ‚è¦è°¨æ…

- ä¸¤ä¸ªå…³é”®è¶…å‚æ•°ï¼š
  - `alpha`: æ§åˆ¶æ­£åˆ™å¼ºåº¦ï¼ˆæ•´ä½“æƒ©ç½šåŠ›åº¦ï¼‰ï¼›
  - `l1_ratio`: æ§åˆ¶ L1ï¼ˆç¨€ç–ï¼‰ä¸ L2ï¼ˆå¹³æ»‘ï¼‰ä¹‹é—´çš„æƒé‡ï¼Œå¸¸è®¾ä¸º 0.1\~0.9ã€‚
- æ¨èä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆå¦‚ `MultiTaskElasticNetCV`ï¼‰è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç»„åˆã€‚

ä»»åŠ¡åº”å…·æœ‰ä¸€å®šçš„ç›¸å…³æ€§

- MultiTaskElasticNet å‡è®¾å¤šä¸ªä»»åŠ¡ï¼ˆå¤šä¸ªè¾“å‡ºå˜é‡ï¼‰å…±äº«ç‰¹å¾å­é›†ï¼›
- å¦‚æœä»»åŠ¡ä¹‹é—´å®Œå…¨ç‹¬ç«‹ï¼Œå¼ºåˆ¶å…±äº«ç‰¹å¾å¯èƒ½åè€Œå½±å“æ€§èƒ½ï¼›
- å¯å…ˆåšç›¸å…³æ€§åˆ†æï¼ˆä¾‹å¦‚è¾“å‡ºå˜é‡ä¹‹é—´çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼‰å†å†³å®šä½¿ç”¨ä¸å¦ã€‚

æ³¨æ„è¾“å‡ºç»´åº¦åŒ¹é…

- `Y` å¿…é¡»æ˜¯äºŒç»´çŸ©é˜µï¼ˆå³ shape ä¸º `[n_samples, n_outputs]`ï¼‰ï¼›
- å¦‚æœåªé¢„æµ‹ä¸€ä¸ªè¾“å‡ºï¼Œä¸å»ºè®®ä½¿ç”¨ MultiTask ç‰ˆæœ¬ï¼Œä½¿ç”¨æ™®é€š `ElasticNet` å³å¯ã€‚

æ¨¡å‹å¯è§£é‡Šæ€§è¦ç»“åˆä¸šåŠ¡ç†è§£

- è™½ç„¶æ¨¡å‹èƒ½è‡ªåŠ¨é€‰ç‰¹å¾ï¼Œä½†æœ€ç»ˆè§£é‡Šå“ªäº›ç‰¹å¾è¢«é€‰æ‹©ï¼Œåº”ç»“åˆä»»åŠ¡å«ä¹‰ã€é¢†åŸŸçŸ¥è¯†åˆ¤æ–­ï¼›
- ç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦ã€é‡‘èç­‰å¯¹â€œå› æœè§£é‡Šâ€è¦æ±‚é«˜çš„åœºæ™¯ä¸­ã€‚

æ ·æœ¬ä¸è¶³æ—¶æ˜“ä¸ç¨³å®š

- åœ¨æ ·æœ¬æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œå¤šä»»åŠ¡æ¨¡å‹ä»å¯èƒ½å‘ç”Ÿè¿‡æ‹Ÿåˆï¼›
- å¯ä½¿ç”¨æ­£åˆ™è¾ƒå¼ºçš„åˆå§‹æ¨¡å‹ï¼ˆè¾ƒå¤§ `alpha`ï¼‰ï¼Œæˆ–å…ˆè¿›è¡Œç‰¹å¾é™ç»´ã€è¿‡æ»¤ã€‚

å¤šä»»åŠ¡æ¨¡å‹è®­ç»ƒè¾ƒæ…¢

- ç›¸æ¯”å•ä»»åŠ¡ Lasso/Ridgeï¼ŒMultiTaskElasticNet çš„è®¡ç®—å¤æ‚åº¦æ›´é«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç»´æ•°æ®ä¸‹ï¼›
- å¯æå‰ç”¨ç¨€ç–æ€§å¼ºçš„æ¨¡å‹è¿›è¡Œé¢„ç­›é€‰ç‰¹å¾ï¼Œå‡è½»è®¡ç®—è´Ÿæ‹…ã€‚

### 1.1.7. Least Angle Regression

ğŸ“Œ ä¸»è¦æ€æƒ³

LARS æ˜¯ä¸€ç§ç±»ä¼¼äºå‰å‘é€æ­¥å›å½’çš„ç®—æ³•ï¼Œåœ¨æ¯ä¸€æ­¥ä¸­åªå¼•å…¥æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œä½†å®ƒé‡‡ç”¨äº†ä¸€ç§æ›´æ¸©å’Œä¸”çº¿æ€§åœ°å‰è¿›çš„ç­–ç•¥ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°é€¼è¿‘ Lasso è·¯å¾„ã€‚

ç±»æ¯”ä¼ ç»Ÿå‰å‘å›å½’ï¼š

- ä¼ ç»Ÿå‰å‘å›å½’æ¯ä¸€æ­¥å›ºå®šé€‰æ‹©ä¸æ®‹å·®æœ€ç›¸å…³çš„å˜é‡å¹¶å®Œå…¨åŠ å…¥ï¼›
- LARS ä¸ä¼šä¸€ä¸‹å­ç”¨æ»¡è¿™ä¸ªå˜é‡ï¼Œè€Œæ˜¯é€æ¸æ²¿æœ€ç›¸å…³å˜é‡æ–¹å‘å‰è¿›ï¼›
- å½“å¦ä¸€ä¸ªå˜é‡ä¸å½“å‰æ®‹å·®å˜å¾—ä¸€æ ·ç›¸å…³æ—¶ï¼ŒLARS å¼€å§‹æ²¿ä¸¤ä¸ªå˜é‡çš„æ–¹å‘åŒæ—¶å‰è¿›ã€‚

å·¥ä½œæµç¨‹ç®€è¿°ï¼š

1. åˆå§‹åŒ–ï¼šæ‰€æœ‰ç³»æ•°ä¸º 0ï¼›
2. åœ¨æ¯ä¸€æ­¥ï¼š

   - é€‰æ‹©ä¸å½“å‰æ®‹å·®æœ€ç›¸å…³çš„å˜é‡ï¼ˆå³ç›¸å…³ç³»æ•°æœ€å¤§ï¼‰ï¼›
   - æ²¿ç€è¯¥å˜é‡çš„æ–¹å‘é€æ­¥å¢åŠ ç³»æ•°ï¼›
   - ä¸€æ—¦å¦ä¸€ä¸ªå˜é‡ä¸æ®‹å·®çš„ç›¸å…³æ€§è¿½å¹³ï¼Œå°±å°†å®ƒåŠ å…¥æ´»åŠ¨é›†åˆï¼›
   - æ²¿ç€è¿™äº›å˜é‡çš„â€œç­‰è§’æ–¹å‘â€ï¼ˆleast angleï¼‰ç»§ç»­ç§»åŠ¨ï¼›

3. é‡å¤ï¼Œç›´åˆ°æ‰€æœ‰å˜é‡éƒ½è¢«åŠ å…¥ï¼Œæˆ–æ»¡è¶³åœæ­¢æ¡ä»¶ã€‚

ç‰¹ç‚¹æ¦‚æ‹¬

| ç‰¹ç‚¹                       | æè¿°                                                        |
| -------------------------- | ----------------------------------------------------------- |
| é«˜æ•ˆé€¼è¿‘ Lasso è·¯å¾„        | LARS é€šè¿‡é€æ­¥è°ƒæ•´ç³»æ•°æ–¹å‘ï¼Œæ¨¡æ‹Ÿå‡ºä¸ Lasso ç›¸ä¼¼çš„æ­£åˆ™è·¯å¾„    |
| é€Ÿåº¦å¿«                     | ç‰¹åˆ«é€‚åˆé«˜ç»´æ•°æ®ï¼ˆp â‰« nï¼‰åœºæ™¯ä¸‹ï¼Œç›¸æ¯” Lasso æ›´å¿«            |
| ç³»æ•°è·¯å¾„å¯è§£é‡Šæ€§å¼º         | LARS è¿”å›çš„æ˜¯ä¸€ä¸ªè·¯å¾„è§£ï¼ˆæ‰€æœ‰å˜é‡åŠ å…¥çš„é¡ºåºã€ç³»æ•°å˜åŒ–è¿‡ç¨‹ï¼‰ |
| ç¨€ç–æ€§                     | æ¯ä¸€æ­¥æœ€å¤šå¢åŠ ä¸€ä¸ªå˜é‡ï¼Œå¤©ç„¶äº§ç”Ÿç¨€ç–æ¨¡å‹                    |
| ç²¾ç¡®æ§åˆ¶å˜é‡è¿›å…¥æ¨¡å‹çš„é¡ºåº | æœ‰åŠ©äºç†è§£å˜é‡çš„é‡è¦æ€§åŠæ¨¡å‹ç»“æ„                            |

é€‚ç”¨åœºæ™¯

- é«˜ç»´å°æ ·æœ¬æ•°æ®ï¼ˆå¦‚åŸºå› æ•°æ®åˆ†æï¼‰ï¼›
- æƒ³å¿«é€Ÿä¼°ç®—å®Œæ•´çš„ Lasso è§£è·¯å¾„ï¼›
- å¸Œæœ›ç†è§£å˜é‡è¿›å…¥æ¨¡å‹çš„é¡ºåºå’Œè¿‡ç¨‹ï¼›
- æ¨¡å‹è¿½æ±‚ç¨€ç–å’Œå¯è§£é‡Šæ€§ï¼Œè€Œä¸æ˜¯æè‡´é¢„æµ‹ç²¾åº¦ã€‚

ğŸ“Œ æ³¨æ„äº‹é¡¹

é€‚åˆé«˜ç»´ç¨€ç–åœºæ™¯

- LARS è®¾è®¡åˆè¡·å°±æ˜¯ç”¨äºç‰¹å¾æ•°è¿œå¤§äºæ ·æœ¬æ•°ï¼ˆp â‰« nï¼‰ä¸”åªæœ‰å°‘æ•°ç‰¹å¾æœ‰ç”¨çš„æƒ…å†µã€‚
- å¦‚æœç‰¹å¾æ•°é‡ä¸å¤šï¼Œæˆ–è€…å˜é‡å¹¶ä¸ç¨€ç–ï¼ŒLARS çš„ä¼˜åŠ¿ä¼šå‡å°‘ï¼Œæ™®é€šçº¿æ€§å›å½’æˆ– Ridge ä¼šæ›´åˆé€‚ã€‚

å¯¹ç‰¹å¾å…±çº¿æ€§æ•æ„Ÿ

- ä¸ Lasso ç±»ä¼¼ï¼ŒLARS å¯¹é«˜åº¦ç›¸å…³çš„ç‰¹å¾ä¼šåœ¨è·¯å¾„ä¸­éšæœºé€‰æ‹©å…¶ä¸€ï¼Œç»“æœå¯èƒ½ä¸ç¨³å®šã€‚
- å¯ä½¿ç”¨ LassoLarsï¼ˆLARS çš„ Lasso å˜ä½“ï¼‰æ¥è‡ªåŠ¨è¿›è¡Œæ­£åˆ™åŒ–ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚

è¾“å…¥ç‰¹å¾éœ€æ ‡å‡†åŒ–

- å› ä¸º LARS æ˜¯åŸºäºå˜é‡é—´ç›¸å…³æ€§å‰è¿›çš„ï¼Œå¦‚æœç‰¹å¾æ²¡æœ‰ç»Ÿä¸€é‡çº²ï¼Œä¼šå½±å“å˜é‡è¿›å…¥æ¨¡å‹çš„é¡ºåºã€‚
- æ¨èä½¿ç”¨ `StandardScaler` å¯¹æ‰€æœ‰è¾“å…¥ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼š

åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹

- LARS æ˜¯çº¿æ€§å›å½’æ–¹æ³•ï¼Œä¸èƒ½ç›´æ¥ç”¨äºéçº¿æ€§å…³ç³»å»ºæ¨¡ï¼›
- å¦‚æœæ•°æ®å­˜åœ¨æ˜¾è‘—éçº¿æ€§å…³ç³»ï¼Œéœ€è€ƒè™‘æ ¸æ–¹æ³•ã€éçº¿æ€§å›å½’æˆ–ç‰¹å¾å·¥ç¨‹ã€‚

è®¡ç®—ç¨³å®šæ€§å·®äºåæ ‡ä¸‹é™æ³•

- å°½ç®¡ LARS è·¯å¾„æ„å»ºé«˜æ•ˆï¼Œä½†åœ¨æµ®ç‚¹ç²¾åº¦ã€å™ªå£°æ•°æ®ä¸Šï¼Œæ•°å€¼ç¨³å®šæ€§å¯èƒ½ç•¥é€Šäº Lasso åæ ‡ä¸‹é™ï¼ˆå¦‚ `LassoCV`ï¼‰ã€‚
- å°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡ç¨ å¯†æ•°æ®ä¸Šï¼Œåæ ‡ä¸‹é™æœ‰æ›´å¼ºçš„æ”¶æ•›ä¿è¯ã€‚

ä¸ç›´æ¥æä¾›æ­£åˆ™å¼ºåº¦è°ƒèŠ‚

- ä¸ Lasso çš„ $\alpha$ ä¸åŒï¼Œæ ‡å‡† LARS æ²¡æœ‰ä¸€ä¸ªå¯æ§çš„æ­£åˆ™åŒ–å¼ºåº¦è¶…å‚æ•°ï¼›
- ä¸è¿‡å¯ä»¥é€šè¿‡â€œæ—©åœâ€æˆ–è€…é€‰æ‹©ç‰¹å®šè·¯å¾„ç‚¹æ¥é—´æ¥æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ã€‚

æ¨¡å‹é€‰æ‹©ä¾èµ–è·¯å¾„æˆªæ–­ç‚¹

- ä½¿ç”¨ LARS æ—¶ï¼Œå¾€å¾€éœ€æ ¹æ®äº¤å‰éªŒè¯æˆ– AIC/BIC é€‰æ‹©æœ€ä½³æˆªæ–­ç‚¹ï¼ˆå˜é‡æ•°é‡ï¼‰ï¼Œå¦åˆ™å®¹æ˜“è¿‡æ‹Ÿåˆï¼›
- `LarsCV` æä¾›äº¤å‰éªŒè¯æ”¯æŒï¼Œè‡ªåŠ¨é€‰æ‹©æˆªæ–­ç‚¹ã€‚

### 1.1.8. LARS Lasso

ğŸ“Œ ä¸»è¦æ€æƒ³  
ğŸ“Œ ä¸»è¦ç‰¹ç‚¹
ğŸ“Œ æ³¨æ„äº‹é¡¹

### 1.1.9. Orthogonal Matching Pursuit (OMP)

ğŸ“Œ ä¸»è¦æ€æƒ³  
ğŸ“Œ ä¸»è¦ç‰¹ç‚¹
ğŸ“Œ æ³¨æ„äº‹é¡¹

### 1.1.10. Bayesian Regression

ğŸ“Œ ä¸»è¦æ€æƒ³  
ğŸ“Œ ä¸»è¦ç‰¹ç‚¹
ğŸ“Œ æ³¨æ„äº‹é¡¹

### 1.1.11. Logistic regression

### 1.1.12. Generalized Linear Models

### 1.1.13. Stochastic Gradient Descent

### 1.1.14. Perceptron

### 1.1.15. Passive Aggressive Algorithms

### 1.1.16. Robustness regression: outliers and modeling errors

### 1.1.17. Quantile Regression

### 1.1.18. Polynomial regression: extending linear models with basis functions

## 1.2. Linear and Quadratic Discriminant Analysis

## 1.3. Kernel ridge regression

## 1.4. Support Vector Machines

### 1.4.1. Classification

### 1.4.2. Regression

### 1.4.3. Density estimation, novelty detection

### 1.4.4. Complexity

### 1.4.5. Tips on Practical Use

### 1.4.6. Kernel functions

### 1.4.7. Mathematical formulation

### 1.4.8. Implementation details

## 1.5. Stochastic Gradient Descent

### 1.5.1. Classification

### 1.5.2. Regression

### 1.5.3. Online One-Class SVM

### 1.5.4. Stochastic Gradient Descent for sparse data

### 1.5.5. Complexity

### 1.5.6. Stopping criterion

### 1.5.7. Tips on Practical Use

### 1.5.8. Mathematical formulation

### 1.5.9. Implementation details

## 1.6. Nearest Neighbors

### 1.6.1. Unsupervised Nearest Neighbors

### 1.6.2. Nearest Neighbors Classification

### 1.6.3. Nearest Neighbors Regression

### 1.6.4. Nearest Neighbor Algorithms

### 1.6.5. Nearest Centroid Classifier

### 1.6.6. Nearest Neighbors Transformer

### 1.6.7. Neighborhood Components Analysis

## 1.7. Gaussian Processes

### 1.7.1. Gaussian Process Regression (GPR)

### 1.7.2. Gaussian Process Classification (GPC)

### 1.7.3. GPC examples

### 1.7.4. Kernels for Gaussian Processes

## 1.8. Cross decomposition

### 1.8.1. PLSCanonical

### 1.8.2. PLSSVD

### 1.8.3. PLSRegression

### 1.8.4. Canonical Correlation Analysis

## 1.9. Naive Bayes

### 1.9.1. Gaussian Naive Bayes

### 1.9.2. Multinomial Naive Bayes

### 1.9.3. Complement Naive Bayes

### 1.9.4. Bernoulli Naive Bayes

### 1.9.5. Categorical Naive Bayes

### 1.9.6. Out-of-core naive Bayes model fitting

## 1.10. Decision Trees

### 1.10.1. Classification

### 1.10.2. Regression

### 1.10.3. Multi-output problems

### 1.10.4. Complexity

### 1.10.5. Tips on practical use

### 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART

### 1.10.7. Mathematical formulation

### 1.10.8. Missing Values Support

### 1.10.9. Minimal Cost-Complexity Pruning

## 1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking

### 1.11.1. Gradient-boosted trees

### 1.11.2. Random forests and other randomized tree ensembles

### 1.11.3. Bagging meta-estimator

### 1.11.4. Voting Classifier

### 1.11.5. Voting Regressor

### 1.11.6. Stacked generalization

### 1.11.7. AdaBoost

## 1.12. Multiclass and multioutput algorithms

### 1.12.1. Multiclass classification

### 1.12.2. Multilabel classification

### 1.12.3. Multiclass-multioutput classification

### 1.12.4. Multioutput regression

## 1.13. Feature selection

### 1.13.1. Removing features with low variance

### 1.13.2. Univariate feature selection

### 1.13.3. Recursive feature elimination

### 1.13.4. Feature selection using SelectFromModel

### 1.13.5. Sequential Feature Selection

### 1.13.6. Feature selection as part of a pipeline

## 1.14. Semi-supervised learning

### 1.14.1. Self Training

### 1.14.2. Label Propagation

## 1.15. Isotonic regression

## 1.16. Probability calibration

### 1.16.1. Calibration curves

### 1.16.2. Calibrating a classifier

### 1.16.3. Usage

## 1.17. Neural network models (supervised)

### 1.17.1. Multi-layer Perceptron

### 1.17.2. Classification

### 1.17.3. Regression

### 1.17.4. Regularization

### 1.17.5. Algorithms

### 1.17.6. Complexity

### 1.17.7. Tips on Practical Use

### 1.17.8. More control with warm_start

# 2. Unsupervised learning

## 2.1. Gaussian mixture models

### 2.1.1. Gaussian Mixture

### 2.1.2. Variational Bayesian Gaussian Mixture

## 2.2. Manifold learning

### 2.2.1. Introduction

### 2.2.2. Isomap

### 2.2.3. Locally Linear Embedding

### 2.2.4. Modified Locally Linear Embedding

### 2.2.5. Hessian Eigenmapping

### 2.2.6. Spectral Embedding

### 2.2.7. Local Tangent Space Alignment

### 2.2.8. Multi-dimensional Scaling (MDS)

### 2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)

### 2.2.10. Tips on practical use

## 2.3. Clustering

### 2.3.1. Overview of clustering methods

### 2.3.2. K-means

### 2.3.3. Affinity Propagation

### 2.3.4. Mean Shift

### 2.3.5. Spectral clustering

### 2.3.6. Hierarchical clustering

### 2.3.7. DBSCAN

### 2.3.8. HDBSCAN

### 2.3.9. OPTICS

### 2.3.10. BIRCH

### 2.3.11. Clustering performance evaluation

## 2.4. Biclustering

### 2.4.1. Spectral Co-Clustering

### 2.4.2. Spectral Biclustering

### 2.4.3. Biclustering evaluation

## 2.5. Decomposing signals in components (matrix factorization problems)

### 2.5.1. Principal component analysis (PCA)

### 2.5.2. Kernel Principal Component Analysis (kPCA)

### 2.5.3. Truncated singular value decomposition and latent semantic analysis

### 2.5.4. Dictionary Learning

### 2.5.5. Factor Analysis

### 2.5.6. Independent component analysis (ICA)

### 2.5.7. Non-negative matrix factorization (NMF or NNMF)

### 2.5.8. Latent Dirichlet Allocation (LDA)

## 2.6. Covariance estimation

### 2.6.1. Empirical covariance

### 2.6.2. Shrunk Covariance

### 2.6.3. Sparse inverse covariance

### 2.6.4. Robust Covariance Estimation

## 2.7. Novelty and Outlier Detection

### 2.7.1. Overview of outlier detection methods

### 2.7.2. Novelty Detection

### 2.7.3. Outlier Detection

### 2.7.4. Novelty detection with Local Outlier Factor

## 2.8. Density Estimation

### 2.8.1. Density Estimation: Histograms

### 2.8.2. Kernel Density Estimation

## 2.9. Neural network models (unsupervised)

### 2.9.1. Restricted Boltzmann machines

# 3. Model selection and evaluation

## 3.1. Cross-validation: evaluating estimator performance

### 3.1.1. Computing cross-validated metrics

### 3.1.2. Cross validation iterators

### 3.1.3. A note on shuffling

### 3.1.4. Cross validation and model selection

### 3.1.5. Permutation test score

## 3.2. Tuning the hyper-parameters of an estimator

### 3.2.1. Exhaustive Grid Search

### 3.2.2. Randomized Parameter Optimization

### 3.2.3. Searching for optimal parameters with successive halving

### 3.2.4. Tips for parameter search

### 3.2.5. Alternatives to brute force parameter search

## 3.3. Tuning the decision threshold for class prediction

### 3.3.1. Post-tuning the decision threshold

## 3.4. Metrics and scoring: quantifying the quality of predictions

### 3.4.1. Which scoring function should I use?

### 3.4.2. Scoring API overview

### 3.4.3. The `scoring` parameter: defining model evaluation rules

### 3.4.4. Classification metrics

### 3.4.5. Multilabel ranking metrics

### 3.4.6. Regression metrics

### 3.4.7. Clustering metrics

### 3.4.8. Dummy estimators

## 3.5. Validation curves: plotting scores to evaluate models

### 3.5.1. Validation curve

### 3.5.2. Learning curve

# 4. Metadata Routing

## 4.1. Usage Examples

### 4.1.1. Weighted scoring and fitting

### 4.1.2. Weighted scoring and unweighted fitting

### 4.1.3. Unweighted feature selection

### 4.1.4. Different scoring and fitting weights

## 4.2. API Interface

## 4.3. Metadata Routing Support Status

# 5. Inspection

## 5.1. Partial Dependence and Individual Conditional Expectation plots

### 5.1.1. Partial dependence plots

### 5.1.2. Individual conditional expectation (ICE) plot

### 5.1.3. Mathematical Definition

### 5.1.4. Computation methods

## 5.2. Permutation feature importance

### 5.2.1. Outline of the permutation importance algorithm

### 5.2.2. Relation to impurity-based importance in trees

### 5.2.3. Misleading values on strongly correlated features

# 6. Visualizations

## 6.1. Available Plotting Utilities

### 6.1.1. Display Objects

# 7. Dataset transformations

## 7.1. Pipelines and composite estimators

### 7.1.1. Pipeline: chaining estimators

### 7.1.2. Transforming target in regression

### 7.1.3. FeatureUnion: composite feature spaces

### 7.1.4. ColumnTransformer for heterogeneous data

### 7.1.5. Visualizing Composite Estimators

## 7.2. Feature extraction

### 7.2.1. Loading features from dicts

### 7.2.2. Feature hashing

### 7.2.3. Text feature extraction

### 7.2.4. Image feature extraction

## 7.3. Preprocessing data

### 7.3.1. Standardization, or mean removal and variance scaling

### 7.3.2. Non-linear transformation

### 7.3.3. Normalization

### 7.3.4. Encoding categorical features

### 7.3.5. Discretization

### 7.3.6. Imputation of missing values

### 7.3.7. Generating polynomial features

### 7.3.8. Custom transformers

## 7.4. Imputation of missing values

### 7.4.1. Univariate vs. Multivariate Imputation

### 7.4.2. Univariate feature imputation

### 7.4.3. Multivariate feature imputation

### 7.4.4. Nearest neighbors imputation

### 7.4.5. Keeping the number of features constant

### 7.4.6. Marking imputed values

### 7.4.7. Estimators that handle NaN values

## 7.5. Unsupervised dimensionality reduction

### 7.5.1. PCA: principal component analysis

### 7.5.2. Random projections

### 7.5.3. Feature agglomeration

## 7.6. Random Projection

### 7.6.1. The Johnson-Lindenstrauss lemma

### 7.6.2. Gaussian random projection

### 7.6.3. Sparse random projection

### 7.6.4. Inverse Transform

## 7.7. Kernel Approximation

### 7.7.1. Nystroem Method for Kernel Approximation

### 7.7.2. Radial Basis Function Kernel

### 7.7.3. Additive Chi Squared Kernel

### 7.7.4. Skewed Chi Squared Kernel

### 7.7.5. Polynomial Kernel Approximation via Tensor Sketch

### 7.7.6. Mathematical Details

## 7.8. Pairwise metrics, Affinities and Kernels

### 7.8.1. Cosine similarity

### 7.8.2. Linear kernel

### 7.8.3. Polynomial kernel

### 7.8.4. Sigmoid kernel

### 7.8.5. RBF kernel

### 7.8.6. Laplacian kernel

### 7.8.7. Chi-squared kernel

## 7.9. Transforming the prediction target (`y`)

### 7.9.1. Label binarization

### 7.9.2. Label encoding

# 8. Dataset loading utilities

## 8.1. Toy datasets

### 8.1.1. Iris plants dataset

### 8.1.2. Diabetes dataset

### 8.1.3. Optical recognition of handwritten digits dataset

### 8.1.4. Linnerrud dataset

### 8.1.5. Wine recognition dataset

### 8.1.6. Breast cancer Wisconsin (diagnostic) dataset

## 8.2. Real world datasets

### 8.2.1. The Olivetti faces dataset

### 8.2.2. The 20 newsgroups text dataset

### 8.2.3. The Labeled Faces in the Wild face recognition dataset

### 8.2.4. Forest covertypes

### 8.2.5. RCV1 dataset

### 8.2.6. Kddcup 99 dataset

### 8.2.7. California Housing dataset

### 8.2.8. Species distribution dataset

## 8.3. Generated datasets

### 8.3.1. Generators for classification and clustering

### 8.3.2. Generators for regression

### 8.3.3. Generators for manifold learning

### 8.3.4. Generators for decomposition

## 8.4. Loading other datasets

### 8.4.1. Sample images

### 8.4.2. Datasets in svmlight / libsvm format

### 8.4.3. Downloading datasets from the openml.org repository

### 8.4.4. Loading from external datasets

# 9. Computing with scikit-learn

## 9.1. Strategies to scale computationally: bigger data

### 9.1.1. Scaling with instances using out-of-core learning

## 9.2. Computational Performance

### 9.2.1. Prediction Latency

### 9.2.2. Prediction Throughput

### 9.2.3. Tips and Tricks

## 9.3. Parallelism, resource management, and configuration

### 9.3.1. Parallelism

### 9.3.2. Configuration switches

# 10. Model persistence

## 10.1. Workflow Overview

### 10.1.1. Train and Persist the Model

## 10.2. ONNX

## 10.3. `skops.io`

## 10.4. `pickle`, `joblib`, and `cloudpickle`

## 10.5. Security & Maintainability Limitations

### 10.5.1. Replicating the training environment in production

### 10.5.2. Serving the model artifact

## 10.6. Summarizing the key points

# 11. Common pitfalls and recommended practices

## 11.1. Inconsistent preprocessing

## 11.2. Data leakage

### 11.2.1. How to avoid data leakage

### 11.2.2. Data leakage during pre-processing

## 11.3. Controlling randomness

### 11.3.1. Using `None` or `RandomState` instances, and repeated calls to `fit` and `split`

### 11.3.2. Common pitfalls and subtleties

### 11.3.3. General recommendations

# 12. Dispatching

## 12.1. Array API support (experimental)

### 12.1.1. Example usage

### 12.1.2. Support for `Array API`\-compatible inputs

### 12.1.3. Input and output array type handling

### 12.1.4. Common estimator checks

# 13. Choosing the right estimator

# 14. External Resources, Videos and Talks

## 14.1. The scikit-learn MOOC

## 14.2. Videos

## 14.3. New to Scientific Python?

## 14.4. External Tutorials
