<link rel="stylesheet" href="../style.css">

# 1. Supervised learning

## 1.1. Linear Models

ç›®æ ‡å€¼é¢„æœŸæ˜¯ç‰¹å¾çš„çº¿æ€§ç»„åˆã€‚ç”¨æ•°å­¦ç¬¦å·è¡¨ç¤ºï¼Œå¦‚æœ $\hat{y}$
æ˜¯é¢„æµ‹å€¼ã€‚

$$\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p$$

æˆ‘ä»¬æŒ‡å®šå‘é‡ $w = (w_1,..., w_p)$
ä½œä¸º $coef_$ å’Œ $w_0$ä½œä¸º $intercept_$

### 1.1.1. Ordinary Least Squares

ğŸ“Œ ä¸»è¦æ€æƒ³

æˆ‘ä»¬å‡è®¾å› å˜é‡ $y$ ä¸è¾“å…¥å˜é‡ $x$ ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼š

$$
y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_p x_p + \varepsilon
$$

æ ¸å¿ƒç›®æ ‡ï¼šæœ€å°åŒ–æ®‹å·®å¹³æ–¹å’Œï¼ˆRSSï¼‰

æˆ‘ä»¬å¸Œæœ›é€‰å‡ºä¸€ä¸ªæœ€ä¼˜çš„ $w$ï¼Œä½¿å¾—ï¼š

$$
\text{RSS}(w) = \sum_{i=1}^n (y_i - X_i w)^2
$$

å³ï¼šè®©æ¨¡å‹é¢„æµ‹å€¼ $Xw$ å°½å¯èƒ½æ¥è¿‘çœŸå®å€¼ $y$ã€‚

ğŸ“Œ æ³¨æ„äº‹é¡¹

| ç±»åˆ«     | æ³¨æ„ç‚¹                         | è¯´æ˜                                             |
| -------- | ------------------------------ | ------------------------------------------------ |
| æ•°æ®è¦æ±‚ | çº¿æ€§å…³ç³»å‡è®¾                   | ç›®æ ‡å˜é‡ y ä¸è¾“å…¥ X ä¹‹é—´è¦â€œè¿‘ä¼¼çº¿æ€§â€             |
| æ•°æ®é—®é¢˜ | å¼‚å¸¸å€¼å½±å“å¤§                   | å› ä¸ºç”¨çš„æ˜¯å¹³æ–¹è¯¯å·®ï¼Œå¼‚å¸¸å€¼æƒé‡å¤§ï¼Œå®¹æ˜“â€œæ‹‰æ­ªâ€æ¨¡å‹ |
| ç‰¹å¾å…³ç³» | ç‰¹å¾å…±çº¿æ€§ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰       | ç‰¹å¾ä¹‹é—´é«˜åº¦ç›¸å…³ä¼šå¯¼è‡´è§£ä¸ç¨³å®šï¼ˆæˆ–ä¸å¯é€†ï¼‰       |
| ç»´åº¦é—®é¢˜ | ç‰¹å¾ç»´åº¦é«˜äºæ ·æœ¬æ•°             | ä¼šå¯¼è‡´ $X^TX$ ä¸å¯é€†ï¼Œæ¨¡å‹æ— æ³•æ±‚è§£é—­å¼è§£         |
| è¯¯å·®å‡è®¾ | æ®‹å·®ç‹¬ç«‹ã€æ–¹å·®é½æ€§             | æ®‹å·®ï¼ˆè¯¯å·®é¡¹ï¼‰åº”æ»¡è¶³ i.i.dï¼Œè¯¯å·®æ–¹å·®ä¸€è‡´         |
| è¾“å…¥è§„æ¨¡ | ç‰¹å¾æœªæ ‡å‡†åŒ–å¯èƒ½å½±å“æ•°å€¼ç¨³å®šæ€§ | ç‰¹å¾å€¼å·®å¼‚å¤§æ—¶ï¼Œç³»æ•°éš¾ä»¥æ¯”è¾ƒï¼Œè®¡ç®—ç²¾åº¦ä¹Ÿå—å½±å“   |
| è¾“å‡ºè§£é‡Š | ç³»æ•°åªèƒ½è§£é‡Šâ€œè¾¹é™…çº¿æ€§å½±å“â€     | å¤šæ•°çœŸå®å…³ç³»æ˜¯éçº¿æ€§çš„ï¼Œä¸å®œè¿‡åº¦è§£è¯»ç³»æ•°         |

- çº¿æ€§å…³ç³»å‡è®¾
  - å¦‚æœçœŸå®å…³ç³»æ˜¯éçº¿æ€§çš„ï¼ŒOLS å°±æ— æ³•å¾ˆå¥½æ‹Ÿåˆã€‚
  - è§£å†³æ–¹æ³•ï¼šå¯ä»¥å¯¹ç‰¹å¾åšå¤šé¡¹å¼æ‰©å±•ã€log è½¬æ¢ï¼Œæˆ–è€…ç›´æ¥ç”¨éçº¿æ€§æ¨¡å‹ï¼ˆå¦‚æ ‘æ¨¡å‹ï¼‰ã€‚
- å¯¹å¼‚å¸¸å€¼ç‰¹åˆ«æ•æ„Ÿ
  - å› ä¸ºè¯¯å·®æ˜¯å¹³æ–¹çš„ï¼Œä¸€ä¸ªå¤§çš„åå·®ä¼šå¯¹æŸå¤±å‡½æ•°å½±å“å¾ˆå¤§ã€‚
  - è§£å†³æ–¹æ³•ï¼š
    - ä½¿ç”¨ é²æ£’å›å½’ï¼ˆå¦‚ Huber å›å½’ï¼‰
    - å¯¹æ•°æ®åšå¼‚å¸¸å€¼æ£€æµ‹æˆ– Winsorize
    - æ”¹ç”¨ L1 æŸå¤±ï¼ˆç»å¯¹å€¼è¯¯å·®ï¼‰æ¨¡å‹
- ç‰¹å¾å…±çº¿æ€§ï¼ˆMulticollinearityï¼‰
  - å½“ä¸¤ä¸ªæˆ–å¤šä¸ªç‰¹å¾é«˜åº¦ç›¸å…³æ—¶ï¼Œ$X^TX$ æ¥è¿‘ä¸å¯é€†ï¼ŒOLS è§£ä¼šéå¸¸ä¸ç¨³å®šï¼Œç³»æ•°å˜åŠ¨å¤§ã€‚
  - è§£å†³æ–¹æ³•ï¼š
    - ç”¨ Ridge å›å½’ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰æ¥ç¼“è§£å…±çº¿æ€§ï¼›
    - ä½¿ç”¨ PCA ç­‰é™ç»´æ–¹æ³•ï¼›
    - å»æ‰å†—ä½™ç‰¹å¾ï¼ˆå¦‚å˜é‡é€‰æ‹©ï¼‰
- ç‰¹å¾ç»´åº¦å¤§äºæ ·æœ¬æ•°
  - å¦‚æœ p > nï¼ŒOLS æ— æ³•å”¯ä¸€è§£å‡ºå‚æ•°ï¼ˆæ— è§£æˆ–å¤šè§£ï¼‰ã€‚
  - è§£å†³æ–¹æ³•ï¼šä½¿ç”¨ Ridgeï¼ˆèƒ½å¤„ç† p > n çš„é—®é¢˜ï¼‰ï¼Œæˆ–é™ç»´ã€‚
- è¯¯å·®é¡¹çš„ç»Ÿè®¡å‡è®¾
  - OLS ç»å…¸å‡è®¾åŒ…æ‹¬ï¼š
    - è¯¯å·®é¡¹ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.dï¼‰
    - è¯¯å·®æœŸæœ›ä¸º 0
    - æ–¹å·®ä¸€è‡´ï¼ˆHomoskedasticityï¼‰
  - å¦‚æœè¿™äº›å‡è®¾ä¸æˆç«‹ï¼Œä¼°è®¡é‡ä¾ç„¶æ— åï¼Œä½†ç»Ÿè®¡æ€§è´¨ï¼ˆå¦‚ç½®ä¿¡åŒºé—´ï¼‰å°±ä¸å¯é äº†ã€‚
  - è§£å†³æ–¹æ³•ï¼šç”¨ White æ ‡å‡†è¯¯ã€åŠ æƒæœ€å°äºŒä¹˜ï¼ˆWLSï¼‰ç­‰ã€‚
- ç‰¹å¾æœªæ ‡å‡†åŒ–å½±å“ç³»æ•°è§£é‡Š
  - å¦‚æœä¸€ä¸ªç‰¹å¾å€¼åœ¨ 0-1 ä¹‹é—´ï¼Œå¦ä¸€ä¸ªåœ¨ 0-10000ï¼ŒOLS çš„ç³»æ•°å°±ä¸å…·æœ‰æ¯”è¾ƒæ€§ã€‚
  - è§£å†³æ–¹æ³•ï¼šä½¿ç”¨ `StandardScaler` æ ‡å‡†åŒ–ç‰¹å¾ã€‚
- æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆå°¤å…¶æ˜¯é«˜ç»´ï¼‰
  - OLS æ²¡æœ‰æ­£åˆ™é¡¹ï¼Œå¯¹å°æ ·æœ¬/å¤šç‰¹å¾é—®é¢˜ç‰¹åˆ«å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
  - è§£å†³æ–¹æ³•ï¼š
    - ä½¿ç”¨ Ridge æˆ– Lasso å›å½’ï¼›
    - åšäº¤å‰éªŒè¯é€‰æ‹©æ¨¡å‹ï¼›
    - é™ç»´æˆ–ç‰¹å¾é€‰æ‹©ã€‚
- ç³»æ•°è§£é‡Šæœ‰é™
  - OLS çš„ç³»æ•°æ˜¯å¯¹å•ä¸€å˜é‡çš„â€œè¾¹é™…å½±å“â€è§£é‡Šï¼Œå‰ææ˜¯å…¶ä»–å˜é‡ä¸å˜ã€‚
    ä½†å¦‚æœå˜é‡ä¹‹é—´ç›¸å…³æ€§é«˜ï¼Œè¿™ç§è§£é‡Šæ˜¯ä¸å¯é çš„ã€‚

### 1.1.2. Ridge regression and classification

ğŸ“Œ ä¸»è¦æ€æƒ³

1. Ridge å›å½’çš„åŸºæœ¬å½¢å¼ï¼š

Ridge å›å½’æ˜¯åœ¨æ™®é€šæœ€å°äºŒä¹˜å›å½’ï¼ˆOLSï¼‰çš„åŸºç¡€ä¸Šæ·»åŠ äº† L2 æ­£åˆ™åŒ–é¡¹ï¼Œç›®çš„æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

å…¶ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š

$$
\min_w \|y - Xw\|_2^2 + \alpha \|w\|_2^2
$$

1. æ­£åˆ™åŒ–çš„ä½œç”¨ï¼š

- åœ¨æ•°æ®å…·æœ‰å…±çº¿æ€§ï¼ˆç‰¹å¾ä¹‹é—´é«˜åº¦ç›¸å…³ï¼‰æ—¶ï¼Œæ™®é€šæœ€å°äºŒä¹˜ä¼°è®¡ä¼šå˜å¾—ä¸ç¨³å®šã€‚
- å¼•å…¥æ­£åˆ™åŒ–é¡¹å¯ä»¥å¯¹æƒé‡ $w$ è¿›è¡Œæ”¶ç¼©ï¼Œä»è€Œå‡å°‘æ¨¡å‹å¤æ‚åº¦ã€æå‡ç¨³å®šæ€§ã€‚
- éšç€ $\alpha$ å¢å¤§ï¼Œæ¨¡å‹ç³»æ•°ä¼šè¢«è¿›ä¸€æ­¥å‹ç¼©ã€‚

5. ä½¿ç”¨å»ºè®®ï¼š

- å¯¹äºç‰¹å¾ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§çš„å›å½’é—®é¢˜ï¼ŒRidge å›å½’é€šå¸¸ä¼˜äºæ™®é€šçº¿æ€§å›å½’ã€‚
- å¦‚æœä½ æƒ³è¦åœ¨æ­£åˆ™åŒ–çš„åŒæ—¶è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆå³è®©æŸäº›ç³»æ•°ä¸ºé›¶ï¼‰ï¼Œåº”ä½¿ç”¨ Lasso å›å½’ï¼ˆL1 æ­£åˆ™ï¼‰ã€‚

ğŸ“Œ æ³¨æ„äº‹é¡¹

ä¸€ã€ç‰¹å¾éœ€è¦æ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰

- åŸå› ï¼š
  å²­å›å½’å¯¹ç‰¹å¾çš„å°ºåº¦æ•æ„Ÿï¼Œå› ä¸ºæ­£åˆ™åŒ–é¡¹ $\|w\|_2^2$ ç›´æ¥å¯¹æƒé‡æ–½åŠ çº¦æŸã€‚
- åšæ³•ï¼š
  ä½¿ç”¨ `StandardScaler` å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆå‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼‰ã€‚

äºŒã€æ­£åˆ™åŒ–ç³»æ•° $\alpha$ éœ€è¦è°ƒå‚

- å° Î±ï¼š æ­£åˆ™åŒ–å¼±ï¼Œæ¨¡å‹å¤æ‚ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ
- å¤§ Î±ï¼š æ­£åˆ™åŒ–å¼ºï¼Œæ¨¡å‹è¿‡äºç®€å•ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ
- æ¨èåšæ³•ï¼š

  - ä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆå¦‚ `GridSearchCV` æˆ– `RidgeCV`ï¼‰å¯»æ‰¾æœ€ä¼˜ Î±
  - å…¸å‹è°ƒå‚èŒƒå›´ï¼šå¦‚ $\alpha \in [10^{-3}, 10^3]$

ä¸‰ã€ä¸é€‚åˆåšç‰¹å¾é€‰æ‹©

- å²­å›å½’ä¸ä¼šå°†ç³»æ•°å‹ç¼©ä¸º 0ï¼Œå› æ­¤ä¸èƒ½è‡ªåŠ¨å‰”é™¤æ— å…³ç‰¹å¾ã€‚
- å¦‚æœç›®æ ‡æ˜¯å¯»æ‰¾æœ€é‡è¦çš„ç‰¹å¾ï¼Œåº”è€ƒè™‘ï¼š

  - Lasso å›å½’ï¼ˆL1 æ­£åˆ™ï¼‰
  - æˆ– ElasticNetï¼ˆL1 + L2 ç»“åˆï¼‰

å››ã€ç»“æœè§£é‡Šæ€§å¼±äºæ™®é€šçº¿æ€§å›å½’

- å› ä¸º Ridge ä¼šç¼©å°æ‰€æœ‰ç³»æ•°ï¼Œè§£é‡Šå˜é‡å’Œç›®æ ‡çš„ç›´æ¥çº¿æ€§å…³ç³»è¢«â€œå‹åˆ¶â€äº†ã€‚
- å¦‚æœä½ å…³å¿ƒå›å½’ç³»æ•°çš„å®é™…æ„ä¹‰ï¼ˆå¦‚åŒ»å­¦/ç¤¾ä¼šç§‘å­¦å»ºæ¨¡ï¼‰ï¼Œå¯èƒ½ä¸å¦‚çº¿æ€§å›å½’ç›´è§‚ã€‚

äº”ã€ç›®æ ‡å˜é‡ä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œä½†ç‰¹å¾å¿…é¡»

- $y$ ä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œå²­å›å½’å¯¹ $y$ æ²¡æœ‰ç‰¹åˆ«è¦æ±‚
- ä½†ç‰¹å¾ï¼ˆ$X$ï¼‰å¿…é¡»æ ‡å‡†åŒ–ï¼Œå¦åˆ™æ­£åˆ™åŒ–ä½œç”¨ä¼šå¤±è¡¡

å…­ã€é€‚ç”¨äºçº¿æ€§é—®é¢˜ï¼Œä¸é€‚åˆéçº¿æ€§å»ºæ¨¡

- å²­å›å½’åªèƒ½æ‹Ÿåˆçº¿æ€§æˆ–è¿‘ä¼¼çº¿æ€§å…³ç³»
- å¦‚æœæ•°æ®å­˜åœ¨æ˜¾è‘—çš„éçº¿æ€§å…³ç³»ï¼Œå¯ä»¥è€ƒè™‘ï¼š

  - åŠ å…¥å¤šé¡¹å¼ç‰¹å¾ï¼ˆ`PolynomialFeatures`ï¼‰
  - ä½¿ç”¨æ ¸æ–¹æ³•ï¼ˆå¦‚ `KernelRidge`ï¼‰

ä¸ƒã€ä¸è¦åœ¨ç¨€ç–æ•°æ®ä¸Šç›²ç›®ä½¿ç”¨ Ridge

- å¦‚æœä½ çš„è¾“å…¥æ•°æ®æ˜¯ç¨€ç–çŸ©é˜µï¼ˆå¦‚ TF-IDF æ–‡æœ¬æ•°æ®ï¼‰ï¼Œæ ‡å‡†åŒ–ä¼šç ´åç¨€ç–æ€§
- å»ºè®®ç”¨ä¸éœ€è¦æ ‡å‡†åŒ–çš„ç®—æ³•ï¼ˆå¦‚æœ´ç´ è´å¶æ–¯ï¼‰ï¼Œæˆ–è€…ç”¨ç‰¹åŒ–çš„æ­£åˆ™æ–¹æ³•ï¼ˆå¦‚ Lassoï¼‰

å…«ã€ä¸å²­åˆ†ç±»å™¨ï¼ˆRidgeClassifierï¼‰çš„åŒºåˆ«

- `Ridge` æ˜¯å›å½’æ¨¡å‹
- `RidgeClassifier` æ˜¯ä¸€ä¸ªå°†åˆ†ç±»è½¬åŒ–ä¸ºå›å½’æ±‚è§£çš„æ¨¡å‹ï¼Œä½†å…¶è®­ç»ƒæ–¹å¼ç•¥æœ‰ä¸åŒï¼Œä¸ä½¿ç”¨æ¦‚ç‡è¾“å‡ºï¼ˆä¸åƒ `LogisticRegression`ï¼‰

### 1.1.3. Lasso

ğŸ“Œ ä¸»è¦æ€æƒ³

Lasso å›å½’çš„ç›®æ ‡æ˜¯åœ¨æœ€å°åŒ–é¢„æµ‹è¯¯å·®çš„åŒæ—¶ï¼Œå¼•å…¥å¯¹æ¨¡å‹å‚æ•°çš„ç¨€ç–çº¦æŸï¼Œä»è€Œè¾¾åˆ°ç‰¹å¾é€‰æ‹©çš„æ•ˆæœã€‚

Lasso å›å½’é€šè¿‡åœ¨æ™®é€šæœ€å°äºŒä¹˜ï¼ˆOLSï¼‰çš„æŸå¤±å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ª L1 èŒƒæ•°ï¼ˆL1-normï¼‰ çš„æƒ©ç½šé¡¹ï¼Œçº¦æŸå›å½’ç³»æ•°çš„å¤§å°ï¼š

$$
\min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^n \left( y_i - X_i \cdot \beta \right)^2 + \alpha \sum_{j=1}^{p} |\beta_j| \right\}
$$

æ ¸å¿ƒæ€æƒ³æ€»ç»“ï¼š

| æ ¸å¿ƒç‚¹       | è¯´æ˜                                                                   |
| ------------ | ---------------------------------------------------------------------- |
| L1 æ­£åˆ™åŒ–    | é€šè¿‡ç»å¯¹å€¼æƒ©ç½šæ¥ä½¿éƒ¨åˆ†ç‰¹å¾çš„ç³»æ•°å˜ä¸º 0ï¼Œèµ·åˆ°â€œç‰¹å¾é€‰æ‹©â€çš„ä½œç”¨           |
| ç¨€ç–è§£       | ä¸ Ridge å›å½’ï¼ˆL2 æ­£åˆ™ï¼‰ä¸åŒï¼ŒLasso èƒ½äº§ç”Ÿç¨€ç–è§£ â€”â€” åªä¿ç•™æœ€æœ‰ç”¨çš„ç‰¹å¾ |
| é™ç»´èƒ½åŠ›     | è‡ªåŠ¨å»é™¤ä¸é‡è¦çš„ç‰¹å¾ï¼Œå°¤å…¶é€‚ç”¨äºé«˜ç»´æ•°æ®ï¼ˆp â‰« nï¼‰çš„æƒ…å†µ                |
| æ¨¡å‹è§£é‡Šæ€§å¼º | ä¿ç•™çš„ç‰¹å¾å°‘ï¼Œæ¨¡å‹æ›´æ˜“äºè§£é‡Š                                           |

é€‚ç”¨åœºæ™¯

- ç‰¹å¾å¾ˆå¤šï¼Œä½†æœŸæœ›åªä¿ç•™ä¸€éƒ¨åˆ†æ˜¾è‘—ç‰¹å¾ï¼›
- é«˜ç»´æ•°æ®ï¼ˆå¦‚åŸºå› æ•°æ®ã€æ–‡æœ¬åˆ†ç±»ç­‰ï¼‰ï¼›
- å¸Œæœ›æ„å»ºç®€å•ã€å¯è§£é‡Šçš„æ¨¡å‹ã€‚

ğŸ“Œ ä¸»è¦ç‰¹ç‚¹

| ç‰¹ç‚¹ç±»åˆ«       | å†…å®¹                                                             |     |     |
| -------------- | ---------------------------------------------------------------- | --- | --- |
| æ­£åˆ™ç±»å‹       | ä½¿ç”¨ L1 æ­£åˆ™åŒ–å¯¹æ¨¡å‹å‚æ•°æ–½åŠ çº¦æŸ                                 |
| ç‰¹å¾é€‰æ‹©èƒ½åŠ›   | èƒ½å°†éƒ¨åˆ†å›å½’ç³»æ•°å‹ç¼©ä¸º 0ï¼Œå³è‡ªåŠ¨å»é™¤ä¸é‡è¦çš„ç‰¹å¾ï¼ˆç¨€ç–è§£ï¼‰       |     |     |
| æ¨¡å‹å¯è§£é‡Šæ€§å¼º | å› ä¸ºåªä¿ç•™é‡è¦å˜é‡ï¼Œæ¨¡å‹æ›´ç®€å•ã€æ›´æ˜“è§£é‡Š                         |     |     |
| é˜²æ­¢è¿‡æ‹Ÿåˆ     | æ­£åˆ™é¡¹æœ‰åŠ©äºé™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œé¿å…åœ¨å°æ ·æœ¬ä¸‹è¿‡æ‹Ÿåˆ                 |     |     |
| è¶…å‚æ•°æ§åˆ¶     | é€šè¿‡æ­£åˆ™å¼ºåº¦ $\alpha$ æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ä¸å˜é‡ç¨€ç–æ€§ä¹‹é—´çš„æƒè¡¡       |     |     |
| é€‚ç”¨åœºæ™¯       | é«˜ç»´æ•°æ®ï¼ˆä¾‹å¦‚ p â‰« nï¼‰ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€åŸºå› æ•°æ®åˆ†æç­‰               |     |     |
| ä¸ Ridge åŒºåˆ«  | Lasso ç”¨ L1 æ­£åˆ™ï¼ˆå¯ç¨€ç–ï¼‰ï¼›Ridge ç”¨ L2 æ­£åˆ™ï¼ˆä¸ä¼šäº§ç”Ÿç¨€ç–ç³»æ•°ï¼‰ |     |     |
| å¤šé‡å…±çº¿æ€§å¤„ç† | å¯¹é«˜åº¦ç›¸å…³çš„å˜é‡ï¼ŒLasso å¾€å¾€åªä¿ç•™å…¶ä¸­ä¸€ä¸ªï¼Œä¸èƒ½å¾ˆå¥½åœ°å…±äº«æƒé‡   |     |     |

ğŸ“Œ æ³¨æ„äº‹é¡¹

ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆéå¸¸é‡è¦ï¼‰

- åŸå› ï¼šLasso ä¾èµ–æ­£åˆ™åŒ–æƒ©ç½šï¼ˆ$\alpha \sum |\beta_j|$ï¼‰ï¼Œè€Œç‰¹å¾çš„å°ºåº¦ä¼šå½±å“æƒ©ç½šåŠ›åº¦ã€‚
- åšæ³•ï¼šåœ¨ä½¿ç”¨ Lasso å‰å¿…é¡»å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œ æ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰æˆ–å½’ä¸€åŒ–ï¼ˆMinMaxScalerï¼‰ã€‚

æ­£åˆ™ç³»æ•° $\alpha$ çš„é€‰æ‹©

- å¤ªå°ï¼šæ¨¡å‹æ¥è¿‘æ™®é€šçº¿æ€§å›å½’ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼›
- å¤ªå¤§ï¼šæ¨¡å‹è¿‡åº¦ç¨€ç–ï¼Œé‡è¦ç‰¹å¾å¯èƒ½è¢«å‹ç¼©ä¸º 0ï¼Œå¯¼è‡´æ¬ æ‹Ÿåˆï¼›
- å»ºè®®ï¼šä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆå¦‚ `LassoCV`ï¼‰è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ $\alpha$ã€‚

å¯¹å…±çº¿æ€§ç‰¹å¾æ•æ„Ÿï¼ˆå˜é‡äº’ç›¸ç›¸å…³ï¼‰

- å¦‚æœå¤šä¸ªç‰¹å¾é«˜åº¦ç›¸å…³ï¼ŒLasso å¯èƒ½åªä¿ç•™å…¶ä¸­ä¸€ä¸ªï¼Œå…¶ä½™è®¾ä¸º 0ï¼›
- è§£å†³åŠæ³•ï¼š

  - å¯ä½¿ç”¨ ElasticNetï¼ˆç»“åˆ L1 å’Œ L2ï¼‰æ›´ç¨³å¥åœ°å¤„ç†å…±çº¿æ€§ï¼›
  - æˆ–æå‰ä½¿ç”¨é™ç»´æ–¹æ³•å¦‚ PCAï¼›

ç¨€ç–æ€§ä¸æ˜¯ä¸‡èƒ½çš„

- ç¨€ç–è§£é€‚åˆè§£é‡Šæ€§è¦æ±‚é«˜çš„ä»»åŠ¡ï¼›
- å¦‚æœç›®æ ‡æ˜¯æœ€å¤§åŒ–é¢„æµ‹ç²¾åº¦ï¼Œè€Œä¸æ˜¯è§£é‡Šå˜é‡ï¼ŒLasso å¯èƒ½ä¸æ˜¯æœ€ä½³é€‰æ‹©ï¼›
- å»ºè®®æ ¹æ®ä»»åŠ¡ç›®æ ‡é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚

å°æ ·æœ¬ + é«˜ç»´åº¦æ—¶è¦è°¨æ…

- è™½ç„¶ Lasso é€‚ç”¨äºé«˜ç»´ï¼Œä½†å¦‚æœæ ·æœ¬æ•°é‡æå°‘ï¼ˆn â‰ª pï¼‰ï¼Œç»“æœå¯èƒ½ä¸ç¨³å®šï¼›
- ç‰¹åˆ«æ˜¯åœ¨é‡è¦ç‰¹å¾è¢«é”™è¯¯å‹ä¸º 0 çš„é£é™©ä¸‹ï¼Œå»ºè®®ä½¿ç”¨æ›´é²æ£’çš„æ–¹å¼ï¼ˆå¦‚ bootstrap éªŒè¯ç‰¹å¾ç¨³å®šæ€§ï¼‰ï¼›

è¾“å‡ºç³»æ•°å®¹æ˜“éœ‡è¡ï¼ˆä¸ç¨³å®šï¼‰

- Lasso å›å½’çš„è¾“å‡ºç»“æœå¯¹æ•°æ®æ‰°åŠ¨è¾ƒæ•æ„Ÿï¼›
- å¯¹äºâ€œè¾¹ç¼˜é‡è¦â€çš„å˜é‡ï¼Œå…¶æ˜¯å¦è¢«é€‰ä¸­å¯èƒ½éšç€æ ·æœ¬ç¨å¾®å˜åŠ¨è€Œä¸åŒï¼›
- å¯ä½¿ç”¨æ¨¡å‹ç¨³å®šæ€§åˆ†æï¼ˆstability selectionï¼‰è¾…åŠ©åˆ¤æ–­ç‰¹å¾æ˜¯å¦å¯ä¿¡ã€‚

ç›®æ ‡å˜é‡ä¸è¦æ ‡å‡†åŒ–

- ç‰¹å¾éœ€è¦æ ‡å‡†åŒ–
- ç›®æ ‡å˜é‡ y é€šå¸¸ä¸è¦æ ‡å‡†åŒ–ï¼Œå¦åˆ™é¢„æµ‹å€¼è§£é‡Šä¼šå˜å¤æ‚ï¼›
- é™¤éä½ ä¸“é—¨å¯¹ y æœ‰åˆ†å¸ƒçº¦æŸæˆ–é¢„æµ‹èŒƒå›´è¦æ±‚ã€‚

### 1.1.4. Multi-task Lasso

### 1.1.5. Elastic-Net

### 1.1.6. Multi-task Elastic-Net

Multi-task Elastic Net æ˜¯ Elastic Net çš„æ‰©å±•ï¼Œç”¨äº å¤šè¾“å‡ºå›å½’ä»»åŠ¡ï¼ˆmulti-output regressionï¼‰ï¼Œå³ä¸€æ¬¡æ€§é¢„æµ‹å¤šä¸ªç›¸å…³ç›®æ ‡å˜é‡ã€‚
å®ƒç»“åˆäº†ï¼š

- Lassoï¼ˆâ„“1ï¼‰ çš„ç‰¹å¾é€‰æ‹©èƒ½åŠ›
- Ridgeï¼ˆâ„“2ï¼‰ çš„é²æ£’æ€§
- å¹¶é€šè¿‡ è¡Œçº§ç¨€ç–æ€§ï¼ˆâ„“2,1 èŒƒæ•°ï¼‰ å®ç° è·¨å¤šä¸ªä»»åŠ¡çš„ä¸€è‡´æ€§ç‰¹å¾é€‰æ‹©

Multi-task Elastic Net æœ€å°åŒ–ä»¥ä¸‹ç›®æ ‡å‡½æ•°ï¼š

$$
\frac{1}{2n_{\text{samples}}} \|Y - XW\|^2_{\text{Fro}} + \alpha \left( \rho \|W\|_{2,1} + \frac{1 - \rho}{2} \|W\|^2_{\text{Fro}} \right)
$$

å…¶ä¸­ï¼š

- $Y \in \mathbb{R}^{n \times T}$ï¼šç›®æ ‡çŸ©é˜µï¼ˆT ä¸ªè¾“å‡ºä»»åŠ¡ï¼‰
- $X \in \mathbb{R}^{n \times p}$ï¼šç‰¹å¾çŸ©é˜µ
- $W \in \mathbb{R}^{p \times T}$ï¼šç³»æ•°çŸ©é˜µ
- $\|W\|_{2,1} = \sum_{i=1}^p \sqrt{\sum_{j=1}^T w_{ij}^2}$ï¼šå¯¹ç³»æ•°çŸ©é˜µæ¯ä¸€è¡Œçš„ â„“2 èŒƒæ•°æ±‚å’Œ
- $\|W\|^2_{\text{Fro}}$ï¼šFrobenius èŒƒæ•°ï¼Œç›¸å½“äºæ‰€æœ‰å…ƒç´ å¹³æ–¹å’Œ

ç‰¹ç‚¹

- åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå…±åŒé€‰æ‹©ä¸€ç»„ç‰¹å¾
- æ›´ç¨³å®šçš„ç‰¹å¾é€‰æ‹©æ•ˆæœï¼ˆå°¤å…¶åœ¨ä»»åŠ¡ç›¸å…³æ€§å¼ºæ—¶ï¼‰
- ç›¸æ¯”å•ä»»åŠ¡æ¨¡å‹æ›´èƒ½æå‡æ³›åŒ–èƒ½åŠ›

é€‚ç”¨äºï¼š

- å¤šä»»åŠ¡ã€å¤šè¾“å‡ºå›å½’é—®é¢˜
- é«˜ç»´ç¨€ç–ç‰¹å¾ç©ºé—´
- ä»»åŠ¡ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œå¸Œæœ›å…±ç”¨ä¸€ç»„ç‰¹å¾

æ³¨æ„äº‹é¡¹

- æ‰€æœ‰è¾“å‡ºå¿…é¡»ä¸º è¿ç»­å€¼ï¼Œä¸èƒ½ç”¨äºåˆ†ç±»
- ç‰¹å¾å»ºè®®æ ‡å‡†åŒ–ï¼ˆå¦‚ä½¿ç”¨ `StandardScaler`ï¼‰
- è¾“å‡ºå¿…é¡»ä¸ºäºŒç»´æ•°ç»„ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªä»»åŠ¡

### 1.1.7. Least Angle Regression

Least Angle Regressionï¼ˆLARSï¼‰ æ˜¯ä¸€ç§é«˜æ•ˆçš„å›å½’ç®—æ³•ï¼Œå°¤å…¶é€‚åˆç”¨äºå˜é‡é€‰æ‹©ï¼ˆfeature selectionï¼‰é—®é¢˜ï¼Œå°¤å…¶åœ¨ ç‰¹å¾æ•°è¿œå¤§äºæ ·æœ¬æ•° çš„æƒ…å†µä¸‹ï¼ˆå³ `n_features >> n_samples`ï¼‰ã€‚

å®ƒæ˜¯ä¸€ç§è¿­ä»£çš„å»ºæ¨¡æ–¹æ³•ï¼Œç‰¹å¾æ˜¯é€æ­¥çº³å…¥æ¨¡å‹çš„æ–¹å¼ç±»ä¼¼äºå‰å‘é€æ­¥å›å½’ï¼ˆForward Stepwise Regressionï¼‰ï¼Œä½†æ›´æ–°æ–¹å¼æ›´ä¸ºâ€œæ¸©å’Œâ€å’Œè®¡ç®—é«˜æ•ˆã€‚

LARS çš„ä¸»è¦æ€æƒ³

- åˆå§‹æ¨¡å‹ä¸ºç©ºã€‚
- æ¯æ¬¡æ‰¾åˆ°ä¸å½“å‰æ®‹å·®æœ€ç›¸å…³ï¼ˆæœ€â€œè§’åº¦å°â€ï¼‰çš„ç‰¹å¾ã€‚
- æ²¿ç€è¯¥æ–¹å‘å‰è¿›ï¼Œç›´åˆ°å¦ä¸€ä¸ªç‰¹å¾ä¸æ®‹å·®è¾¾åˆ°ç›¸åŒçš„ç›¸å…³åº¦ã€‚
- ç„¶åå‘è¿™ä¸¤ä¸ªæ–¹å‘åŒæ—¶å‰è¿›ã€‚
- é‡å¤ï¼Œç›´åˆ°æ‰€æœ‰ç‰¹å¾éƒ½è¢«çº³å…¥æ¨¡å‹æˆ–æ»¡è¶³æŸç§åœæ­¢æ¡ä»¶ã€‚

è¿™ç§â€œæœ€å°è§’åº¦â€ç­–ç•¥è®© LARS éå¸¸é€‚åˆé«˜ç»´é—®é¢˜ï¼Œä¸”ç”Ÿæˆçš„ç³»æ•°è·¯å¾„å¯ä»¥ç”¨ä½œå˜é‡é€‰æ‹©çš„ä¾æ®ã€‚

åº”ç”¨åœºæ™¯

- å˜é‡é€‰æ‹©ï¼ˆfeature selectionï¼‰
- é«˜ç»´å°æ ·æœ¬é—®é¢˜ï¼ˆå¦‚åŸºå› è¡¨è¾¾æ•°æ®åˆ†æï¼‰
- è§£é‡Šæ€§åˆ†æï¼šè·¯å¾„è·Ÿè¸ªæ–¹ä¾¿æŸ¥çœ‹å˜é‡æ˜¯ä½•æ—¶è¿›å…¥æ¨¡å‹çš„

æ³¨æ„äº‹é¡¹

- å¯¹äº LARS åŠå…¶å˜ç§ï¼Œè¾“å…¥ç‰¹å¾å»ºè®®ä¸­å¿ƒåŒ–ã€‚
- ä¸å»ºè®®ç”¨äºæå¤§è§„æ¨¡æ•°æ®é›†ï¼ˆåœ¨æŸäº›åœºæ™¯ä¸‹å¯èƒ½å†…å­˜æ¶ˆè€—è¾ƒå¤§ï¼‰ã€‚
- å¯¹äº Lasso è·¯å¾„ï¼Œå»ºè®®ä½¿ç”¨ `LassoLars` è€Œä¸æ˜¯æ™®é€šçš„ `Lasso`ï¼ˆåœ¨æ ·æœ¬æ•°å°‘çš„æƒ…å†µä¸‹ï¼‰ã€‚

### 1.1.8. LARS Lasso

LARS Lasso æ˜¯ä¸€ç§é«˜æ•ˆç®—æ³•ï¼Œç”¨äºè®¡ç®— Lasso å›å½’ çš„ å®Œæ•´è§£è·¯å¾„ï¼ˆregularization pathï¼‰ã€‚å®ƒåŸºäº Least Angle Regressionï¼ˆLARSï¼‰ç®—æ³•ã€‚

èƒŒæ™¯

- Lassoï¼ˆLeast Absolute Shrinkage and Selection Operatorï¼‰æ˜¯ä¸€ç§å¸¦ L1 æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶å®ç° å˜é‡é€‰æ‹© + æ­£åˆ™åŒ–ã€‚
- ä¼ ç»Ÿæ±‚è§£ Lasso ä½¿ç”¨çš„æ˜¯ åæ ‡ä¸‹é™æ³•ï¼ˆCoordinate Descentï¼‰ï¼Œä½†å½“ç‰¹å¾ç»´åº¦è¿œå¤§äºæ ·æœ¬æ•°æ—¶ï¼Œè¿™ç§æ–¹æ³•æ•ˆç‡ä½ã€‚
- LARS-Lasso åˆ©ç”¨ LARS ç®—æ³•ï¼Œå¯ä»¥ ä¸€æ­¥ä¸€æ­¥æ„é€ æ•´ä¸ª Lasso è·¯å¾„ï¼Œéå¸¸é€‚åˆå°æ ·æœ¬é«˜ç»´ç‰¹å¾æ•°æ®ã€‚

LARS Lasso çš„æ€æƒ³

- ä»é›¶ç³»æ•°å¼€å§‹ï¼Œé€æ¸å¢åŠ éé›¶ç³»æ•°çš„æ•°é‡ã€‚
- æ¯ä¸€æ­¥é€‰æ‹©ä¸æ®‹å·®æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œå¹¶å‘è¿™ä¸ªæ–¹å‘ç§»åŠ¨ã€‚
- å½“æŸä¸ªå˜é‡çš„ç³»æ•°è¶‹è¿‘äº 0 æ—¶ï¼Œå®ƒä¼šè¢«â€œè¸¢å‡ºâ€æ¨¡å‹ã€‚
- LARS-Lasso çš„è·¯å¾„ä¸ LARS ç±»ä¼¼ï¼Œä½†å› ä¸º L1 æ­£åˆ™åŒ–çš„å­˜åœ¨ï¼Œè·¯å¾„ä¼šå‡ºç°â€œæŠ˜è¿”â€ï¼ˆéå•è°ƒå¢åŠ ï¼‰ã€‚

åº”ç”¨åœºæ™¯

- åŸºå› æ•°æ®åˆ†æï¼šn_features â‰« n_samples
- ç‰¹å¾é€‰æ‹©ï¼ˆfeature selectionï¼‰
- ç¨€ç–å»ºæ¨¡
- æ­£åˆ™è·¯å¾„å¯è§†åŒ–

æ³¨æ„äº‹é¡¹

- ä¸é€‚åˆ n_samples â‰« n_features çš„æ•°æ®é›†ï¼ˆå¯èƒ½æ¯”åæ ‡ä¸‹é™æ›´æ…¢ï¼‰ã€‚
- ç‰¹å¾å¿…é¡»æ ‡å‡†åŒ–ï¼ˆæ¨èä½¿ç”¨ `StandardScaler`ï¼‰ã€‚
- å¯¹å™ªå£°è¾ƒå¤šçš„æ•°æ®ä¸å¤Ÿé²æ£’ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸ç¨³å®šçš„å˜é‡é€‰æ‹©ã€‚

### 1.1.9. Orthogonal Matching Pursuit (OMP)

Orthogonal Matching Pursuit (OMP) æ˜¯ä¸€ç§è´ªå¿ƒç®—æ³•ï¼Œç”¨äºåœ¨é«˜ç»´ç¨€ç–ä¿¡å·æ¢å¤ä¸­è¿›è¡Œ ç¨€ç–çº¿æ€§å›å½’ã€‚

OMP é€šè¿‡é€æ­¥é€‰æ‹©ä¸å½“å‰æ®‹å·®æœ€ç›¸å…³çš„ç‰¹å¾ï¼ˆå­—å…¸å…ƒç´ ï¼‰ï¼Œè¿­ä»£åœ°æ›´æ–°æ¨¡å‹ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„ç¨€ç–åº¦æˆ–è¯¯å·®é˜ˆå€¼ã€‚

OMP çš„åŸºæœ¬æ€æƒ³

- ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªç³»æ•°å‘é‡ï¼Œä½¿å¾—é¢„æµ‹å€¼å°½å¯èƒ½æ‹Ÿåˆç›®æ ‡ï¼ŒåŒæ—¶ç³»æ•°ä¿æŒç¨€ç–ï¼ˆå³å¤§éƒ¨åˆ†ç³»æ•°ä¸º 0ï¼‰ã€‚
- æ¯æ¬¡è¿­ä»£ï¼š

  1. æ‰¾åˆ°ä¸å½“å‰æ®‹å·®æœ€ç›¸å…³çš„ç‰¹å¾ï¼ˆå­—å…¸åˆ—ï¼‰ã€‚
  2. å°†è¯¥ç‰¹å¾åŠ å…¥å½“å‰æ´»è·ƒé›†ã€‚
  3. åœ¨æ´»è·ƒé›†ä¸Šåšæœ€å°äºŒä¹˜æ‹Ÿåˆï¼Œæ›´æ–°ç³»æ•°ã€‚
  4. è®¡ç®—æ–°çš„æ®‹å·®ã€‚

- é‡å¤ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆè¾¾åˆ°æœ€å¤§éé›¶ç³»æ•°ä¸ªæ•°æˆ–æ®‹å·®è¶³å¤Ÿå°ï¼‰ã€‚

æ•°å­¦å½¢å¼

ç»™å®šè¾“å…¥çŸ©é˜µ $X$ å’Œå“åº”å‘é‡ $y$ï¼Œæ±‚è§£ï¼š

$$
\min_{\beta} \|y - X\beta\|_2^2 \quad \text{subject to} \quad \|\beta\|_0 \leq k
$$

å…¶ä¸­ $\|\beta\|_0$ æ˜¯éé›¶ç³»æ•°çš„ä¸ªæ•°ï¼Œé™åˆ¶ç¨€ç–åº¦ã€‚

åº”ç”¨åœºæ™¯

- ç¨€ç–ä¿¡å·æ¢å¤ï¼Œå¦‚å‹ç¼©æ„ŸçŸ¥ï¼ˆCompressed Sensingï¼‰
- é«˜ç»´ç¨€ç–å›å½’
- ç‰¹å¾é€‰æ‹©
- å­—å…¸å­¦ä¹ 

ä¼˜ç¼ºç‚¹å¯¹æ¯”

| ä¼˜ç‚¹                   | ç¼ºç‚¹                     |
| ---------------------- | ------------------------ |
| è®¡ç®—é€Ÿåº¦å¿«ï¼Œç®€å•æ˜“å®ç° | å¯èƒ½å¯¹å™ªå£°æ•æ„Ÿ           |
| æ¨¡å‹ç³»æ•°ç¨€ç–ï¼Œä¾¿äºè§£é‡Š | è´ªå¿ƒç®—æ³•ï¼Œä¸ä¿è¯å…¨å±€æœ€ä¼˜ |
| é€æ­¥é€‰æ‹©å˜é‡ï¼Œæ˜“äºç†è§£ | è¿­ä»£æ¬¡æ•°éœ€åˆç†è®¾ç½®       |

### 1.1.10. Bayesian Regression

è´å¶æ–¯å›å½’æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯ç»Ÿè®¡ç†è®ºçš„çº¿æ€§å›å½’æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å…¥å…ˆéªŒæ¦‚ç‡å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»è€Œè§£å†³è¿‡æ‹Ÿåˆé—®é¢˜å¹¶ç»™å‡ºå‚æ•°çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚

ä¸»è¦æ€æƒ³

- å‡è®¾æ¨¡å‹å‚æ•°æœä»æŸç§å…ˆéªŒåˆ†å¸ƒï¼ˆä¸€èˆ¬ä¸ºé«˜æ–¯åˆ†å¸ƒï¼‰ã€‚
- è§‚æµ‹æ•°æ®æ ¹æ®æ¨¡å‹å’Œå™ªå£°äº§ç”Ÿï¼Œå™ªå£°ä¹Ÿæœä»é«˜æ–¯åˆ†å¸ƒã€‚
- åˆ©ç”¨è´å¶æ–¯å®šç†ç»“åˆå…ˆéªŒä¸ä¼¼ç„¶ï¼Œå¾—åˆ°å‚æ•°çš„åéªŒåˆ†å¸ƒã€‚
- æ¨¡å‹ä¸ä»…é¢„æµ‹ç›®æ ‡å€¼ï¼Œè¿˜èƒ½æä¾›é¢„æµ‹çš„ç½®ä¿¡åŒºé—´ã€‚

æ•°å­¦æ¨¡å‹

- çº¿æ€§æ¨¡å‹ï¼š

  $$
  y = Xw + \epsilon, \quad \epsilon \sim \mathscr{N}(0, \sigma^2 I)
  $$

- å‚æ•°å…ˆéªŒï¼š

  $$
  p(w | \alpha) = \mathscr{N}(0, \alpha^{-1} I)
  $$

- è¶…å‚æ•° $\alpha$ æ§åˆ¶å…ˆéªŒçš„å¼ºåº¦ï¼Œç±»ä¼¼æ­£åˆ™åŒ–é¡¹ã€‚
- é€šè¿‡è§‚æµ‹æ•°æ®ä¼°è®¡å‚æ•°åéªŒï¼š

  $$
  p(w | X, y, \alpha, \sigma^2)
  $$

ä¼˜ç¼ºç‚¹åŠåº”ç”¨åœºæ™¯

| ä¼˜ç‚¹                                 | ç¼ºç‚¹                             |
| ------------------------------------ | -------------------------------- |
| è‡ªåŠ¨è°ƒèŠ‚æ­£åˆ™åŒ–å¼ºåº¦ï¼Œå‡å°‘è°ƒå‚éº»çƒ¦     | è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼Œé€‚åˆä¸­å°å‹æ•°æ®é›† |
| ç»™å‡ºé¢„æµ‹çš„ç½®ä¿¡åŒºé—´ï¼Œæä¾›ä¸ç¡®å®šæ€§ä¼°è®¡ | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ                     |
| ARD å¯ä»¥è‡ªåŠ¨è¿›è¡Œç‰¹å¾é€‰æ‹©             | å¯¹éçº¿æ€§å…³ç³»å»ºæ¨¡æœ‰é™             |
| èƒ½å¤Ÿå¤„ç†å…±çº¿æ€§é—®é¢˜                   |                                  |

åº”ç”¨åœºæ™¯

- éœ€è¦ä¸ç¡®å®šæ€§ä¼°è®¡çš„å›å½’é—®é¢˜
- é«˜ç»´å°æ ·æœ¬é—®é¢˜
- è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
- éœ€è¦è´å¶æ–¯è§£é‡Šçš„æœºå™¨å­¦ä¹ ä»»åŠ¡

### 1.1.11. Logistic regression

é€»è¾‘å›å½’æ˜¯ä¸€ç§ç»å…¸çš„å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œä¸»è¦ç”¨äºäºŒåˆ†ç±»åŠå¤šåˆ†ç±»é—®é¢˜ã€‚å®ƒé€šè¿‡å¯¹çº¿æ€§ç»„åˆè¾“å…¥å˜é‡è¿›è¡Œ sigmoidï¼ˆæˆ– softmaxï¼‰å˜æ¢ï¼Œå°†è¾“å‡ºæ˜ å°„ä¸ºæ¦‚ç‡å€¼ã€‚

ä¸»è¦æ€æƒ³

- é¢„æµ‹ç›®æ ‡ $y \in \{0,1\}$ï¼ˆæˆ–å¤šç±»åˆ«ï¼‰å¯¹åº”æŸä¸€ç±»åˆ«çš„æ¦‚ç‡ã€‚
- é€šè¿‡é€»è¾‘å‡½æ•°ï¼ˆlogistic functionï¼‰æ‹Ÿåˆç±»åˆ«æ¦‚ç‡ï¼š

  $$
  P(y=1|X) = \frac{1}{1 + e^{-Xw}}
  $$

- ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°ï¼Œæˆ–ç­‰ä»·åœ°æœ€å°åŒ–å¯¹æ•°æŸå¤±ï¼ˆlog lossï¼‰ã€‚

æ•°å­¦æ¨¡å‹

- äºŒåˆ†ç±»ï¼š

  $$
  p(y=1|x) = \sigma(w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}
  $$

- å¤šåˆ†ç±»ï¼ˆå¤šé¡¹å¼é€»è¾‘å›å½’ï¼‰ï¼š

  $$
  p(y=k|x) = \frac{e^{w_k^T x + b_k}}{\sum_{j} e^{w_j^T x + b_j}}
  $$

åº”ç”¨åœºæ™¯

- äºŒåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚åƒåœ¾é‚®ä»¶åˆ†ç±»ã€ç–¾ç—…é¢„æµ‹ï¼‰
- å¤šåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æ•°å­—è¯†åˆ«ã€æ–‡æœ¬åˆ†ç±»ï¼‰
- è§£é‡Šæ€§å¼ºçš„æ¨¡å‹éœ€æ±‚
- çº¿æ€§å¯åˆ†æˆ–è¿‘ä¼¼çº¿æ€§å¯åˆ†çš„æ•°æ®é›†

æ³¨æ„äº‹é¡¹

- ç‰¹å¾æœ€å¥½åšæ ‡å‡†åŒ–ï¼ˆå¦‚ `StandardScaler`ï¼‰ï¼Œå°¤å…¶ä½¿ç”¨ `lbfgs`ã€`saga` ç­‰ solverã€‚
- å¯¹äºé«˜åº¦ä¸å¹³è¡¡æ•°æ®ï¼Œå¯ä½¿ç”¨ `class_weight='balanced'`ã€‚
- `l1` æ­£åˆ™åŒ–æœ‰åŠ©äºç‰¹å¾é€‰æ‹©ï¼Œä½†éœ€è¦ç‰¹å®š solverã€‚
- å¤šåˆ†ç±»æ—¶ï¼Œé»˜è®¤é‡‡ç”¨ One-vs-Rest ç­–ç•¥ï¼Œè‹¥éœ€æ›´ç²¾ç¡®å¯é€‰ `multinomial`ã€‚

### 1.1.12. Generalized Linear Models

å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰æ˜¯çº¿æ€§æ¨¡å‹çš„æ¨å¹¿ï¼Œèƒ½å¤Ÿå¤„ç†éæ­£æ€åˆ†å¸ƒçš„å“åº”å˜é‡ï¼ˆå¦‚äºŒé¡¹åˆ†å¸ƒã€æ³Šæ¾åˆ†å¸ƒç­‰ï¼‰ï¼Œé€šè¿‡é“¾æ¥å‡½æ•°ï¼ˆlink functionï¼‰å°†çº¿æ€§é¢„æµ‹å€¼æ˜ å°„åˆ°å“åº”å˜é‡çš„æœŸæœ›å€¼ã€‚

ä¸»è¦æ€æƒ³

- å“åº”å˜é‡ $y$ çš„æ¡ä»¶åˆ†å¸ƒå±äºæŒ‡æ•°æ—ï¼ˆExponential familyï¼‰ï¼Œå¦‚æ­£æ€åˆ†å¸ƒã€äºŒé¡¹åˆ†å¸ƒã€æ³Šæ¾åˆ†å¸ƒç­‰ã€‚
- é¢„æµ‹å˜é‡ $X$ çš„çº¿æ€§ç»„åˆ $\eta = Xw$ é€šè¿‡é“¾æ¥å‡½æ•° $g(\cdot)$ ä¸å“åº”å˜é‡çš„æœŸæœ›ç›¸å…³è”ï¼š

  $$
  g(\mathbb{E}[y|X]) = \eta = Xw
  $$

- é“¾æ¥å‡½æ•°æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©ï¼š

  - çº¿æ€§å›å½’ï¼šæ’ç­‰å‡½æ•° $g(\mu) = \mu$
  - é€»è¾‘å›å½’ï¼šlogit å‡½æ•° $g(\mu) = \log \frac{\mu}{1-\mu}$
  - æ³Šæ¾å›å½’ï¼šlog å‡½æ•° $g(\mu) = \log \mu$

åº”ç”¨åœºæ™¯

- è®¡æ•°æ•°æ®å»ºæ¨¡ï¼ˆå¦‚äº‹æ•…æ¬¡æ•°ã€äº‹ä»¶å‘ç”Ÿé¢‘ç‡ï¼‰
- ä¿é™©ç²¾ç®—ã€åŒ»ç–—äº‹ä»¶åˆ†æ
- éè´Ÿå“åº”å˜é‡çš„å›å½’é—®é¢˜
- å¹¿ä¹‰çº¿æ€§å›å½’ï¼Œæ¶‰åŠéæ­£æ€åˆ†å¸ƒ

æ³¨æ„äº‹é¡¹

- ç›®æ ‡å˜é‡å¿…é¡»ç¬¦åˆå¯¹åº”çš„åˆ†å¸ƒå‡è®¾ï¼Œå¦‚è®¡æ•°æ•°æ®é€‚åˆæ³Šæ¾å›å½’ã€‚
- ç‰¹å¾é€šå¸¸éœ€è¦è¿›è¡Œé€‚å½“é¢„å¤„ç†ã€‚
- TweedieRegressor çš„ `power` å‚æ•°å†³å®šäº†åˆ†å¸ƒç±»å‹ï¼Œéœ€è¦æ ¹æ®ä»»åŠ¡åˆç†è®¾ç½®ã€‚
- è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„ä¼˜åŒ–ç®—æ³•æ˜¯åæ ‡ä¸‹é™æˆ–å…¶ä»–å‡¸ä¼˜åŒ–æ–¹æ³•ã€‚

### 1.1.13. Stochastic Gradient Descent

éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„ä¼˜åŒ–ç®—æ³•ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å’Œé«˜ç»´åº¦çš„çº¿æ€§æ¨¡å‹è®­ç»ƒã€‚å®ƒé€šè¿‡å¯¹è®­ç»ƒæ ·æœ¬é€ä¸ªï¼ˆæˆ–å°æ‰¹é‡ï¼‰è®¡ç®—æ¢¯åº¦ï¼Œè¿­ä»£æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

SGD å¯ç”¨äºå¤šç§çº¿æ€§æ¨¡å‹çš„è®­ç»ƒï¼ŒåŒ…æ‹¬çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ç­‰ã€‚

ä¸»è¦æ€æƒ³

- ç›¸æ¯”ä¼ ç»Ÿæ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆè®¡ç®—æ‰€æœ‰æ ·æœ¬æ¢¯åº¦ï¼‰ï¼ŒSGD æ¯æ¬¡åªç”¨ä¸€ä¸ªæ ·æœ¬æ›´æ–°å‚æ•°ï¼Œè®¡ç®—æ›´å¿«ã€‚
- ç”±äºå¼•å…¥éšæœºæ€§ï¼Œæ”¶æ•›è·¯å¾„ä¸ç¨³å®šï¼Œä½†é€šå¸¸èƒ½æ›´å¿«é€¼è¿‘æœ€ä¼˜ã€‚
- æ”¯æŒå¤šç§æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–æ–¹å¼ï¼Œçµæ´»æ„å»ºä¸åŒçº¿æ€§æ¨¡å‹ã€‚

ä¼˜ç¼ºç‚¹åŠåº”ç”¨åœºæ™¯

| ä¼˜ç‚¹                       | ç¼ºç‚¹                           |
| -------------------------- | ------------------------------ |
| é€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼Œè®¡ç®—æ•ˆç‡é«˜ | éœ€è¦è°ƒèŠ‚å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–ç­‰è¶…å‚æ•° |
| æ”¯æŒå¤šç§æŸå¤±å‡½æ•°ï¼Œçµæ´»æ€§å¼º | æ”¶æ•›ä¸ç¨³å®šï¼Œå¯èƒ½éœ‡è¡           |
| å¯ä»¥åœ¨çº¿å­¦ä¹ ï¼ˆå¢é‡è®­ç»ƒï¼‰   | ä¾èµ–åˆå§‹å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°       |
| æ”¯æŒç¨€ç–æ•°æ®å’Œç¨€ç–æ¨¡å‹     | å¯¹å™ªå£°è¾ƒæ•æ„Ÿ                   |

æ³¨æ„äº‹é¡¹

- éœ€å¯¹è¾“å…¥ç‰¹å¾åšå½’ä¸€åŒ–æˆ–æ ‡å‡†åŒ–å¤„ç†ï¼Œå¦åˆ™å½±å“æ”¶æ•›é€Ÿåº¦ã€‚
- å­¦ä¹ ç‡è°ƒèŠ‚ç­–ç•¥ï¼ˆ`learning_rate`ï¼‰å¯¹è®­ç»ƒæ•ˆæœå½±å“å¤§ï¼Œæ¨èä» `'optimal'` æˆ– `'adaptive'` è¯•èµ·ã€‚
- å¯¹äºå¤§è§„æ¨¡å’Œç¨€ç–æ•°æ®éå¸¸é€‚åˆã€‚
- å¯ä»¥ç»“åˆæ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

### 1.1.14. Perceptron

- `Perceptron` æ˜¯ä¸€ç§ çº¿æ€§åˆ†ç±»å™¨ï¼Œå±äºæœ€æ—©çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¹‹ä¸€ã€‚
- ç”¨äº äºŒåˆ†ç±»ä»»åŠ¡ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ å¸¦æœ‰åœ¨çº¿å­¦ä¹ ï¼ˆonline learningï¼‰èƒ½åŠ›çš„çº¿æ€§ SVM çš„è¿‘ä¼¼ã€‚
- é€‚åˆç”¨äº å¤§è§„æ¨¡å­¦ä¹ ä»»åŠ¡ï¼Œå°¤å…¶é€‚ç”¨äº åœ¨çº¿æˆ–å¢é‡å­¦ä¹ ï¼ˆpartial_fitï¼‰ã€‚

æ•°å­¦æ¨¡å‹

æ„ŸçŸ¥æœºæ¨¡å‹ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼š

$$
f(x) = \text{sign}(w \cdot x + b)
$$

- å¦‚æœé¢„æµ‹æ­£ç¡®ï¼Œä¸æ›´æ–°æƒé‡ã€‚
- å¦‚æœé¢„æµ‹é”™è¯¯ï¼Œæ ¹æ®å¦‚ä¸‹è§„åˆ™æ›´æ–°æƒé‡ï¼š

$$
w \leftarrow w + \eta(y - \hat{y})x
$$

- $\eta$ï¼šå­¦ä¹ ç‡ï¼ˆScikit-learn é»˜è®¤æ˜¯ 1.0ï¼‰
- $y$ï¼šçœŸå®æ ‡ç­¾ï¼ˆ+1 æˆ– -1ï¼‰
- $\hat{y}$ï¼šé¢„æµ‹ç»“æœ

ç‰¹ç‚¹æ€»ç»“ï¼š

- åªå¯¹ çº¿æ€§å¯åˆ†æ•°æ® èƒ½ä¿è¯æ”¶æ•›ã€‚
- ä¸æ”¯æŒæ¦‚ç‡é¢„æµ‹ï¼ˆå¦‚ `predict_proba` ä¸å¯ç”¨ï¼‰ã€‚
- é€‚åˆç”¨äºéœ€è¦ å¢é‡å­¦ä¹  çš„åœºæ™¯ï¼ˆä½¿ç”¨ `partial_fit`ï¼‰ã€‚

### 1.1.15. Passive Aggressive Algorithms

Passive-Aggressiveï¼ˆPAï¼‰ç®—æ³•æ˜¯ä¸€ç±»åœ¨çº¿å­¦ä¹ ç®—æ³•ï¼Œç”¨äºå¤§è§„æ¨¡å­¦ä¹ ä»»åŠ¡ï¼Œç‰¹åˆ«é€‚ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼ˆå¦‚ spam æ£€æµ‹ï¼‰æˆ–æƒ…æ„Ÿåˆ†æè¿™ç±»ç¨€ç–ç‰¹å¾çš„æ•°æ®ã€‚

å®ƒä»¬çš„ç‰¹ç‚¹æ˜¯ï¼š

- Passiveï¼ˆè¢«åŠ¨ï¼‰ï¼šå¦‚æœå½“å‰æ¨¡å‹å·²ç»æ­£ç¡®åˆ†ç±»ä¸€ä¸ªæ ·æœ¬ï¼Œåˆ™ä¿æŒä¸å˜ã€‚
- Aggressiveï¼ˆæ”»å‡»æ€§ï¼‰ï¼šå¦‚æœåˆ†ç±»é”™è¯¯æˆ–é—´éš”å¤ªå°ï¼Œå°±æ›´æ–°æ¨¡å‹ä»¥æ›´å¼ºçƒˆåœ°æ‹Ÿåˆè¯¥æ ·æœ¬ã€‚

é€‚åˆï¼š

- æ–‡æœ¬åˆ†ç±»
- åœ¨çº¿å­¦ä¹ ï¼ˆæ•°æ®é€æ­¥åˆ°è¾¾æ—¶ï¼‰
- é«˜ç»´ç¨€ç–ç‰¹å¾ï¼ˆå¦‚ TF-IDFï¼‰

ç®—æ³•æ ¸å¿ƒæ€æƒ³

æ¯ä¸ªè®­ç»ƒæ ·æœ¬è¢«å¤„ç†ä¸€æ¬¡ï¼š

- å¦‚æœæ¨¡å‹åœ¨è¯¥æ ·æœ¬ä¸Šçš„é¢„æµ‹æ˜¯æ­£ç¡®çš„å¹¶ä¸”é—´éš”æ»¡è¶³æ¡ä»¶ï¼Œåˆ™ä¸åšæ›´æ–°ã€‚
- å¦åˆ™æ‰§è¡Œæ›´æ–°ï¼Œä½¿å½“å‰æ ·æœ¬è¢«æ­£ç¡®åˆ†ç±»ï¼ŒåŒæ—¶æ›´æ–°é‡æœ€å°ã€‚

æ•°å­¦å½¢å¼

Passive-Aggressive çš„ä¼˜åŒ–ç›®æ ‡ï¼ˆä»¥åˆ†ç±»ä¸ºä¾‹ï¼‰ï¼š

$$
w_{t+1} = \arg\min_w \frac{1}{2} \|w - w_t\|^2 \quad \text{s.t. } \ell(w; (x_t, y_t)) = 0
$$

å…¶ä¸­ï¼š

- $w_t$ï¼šå½“å‰æ¨¡å‹å‚æ•°
- $\ell$ï¼šæŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚ hinge lossï¼‰

ä¼˜ç‚¹

- èƒ½å¤„ç†å¤§é‡æ•°æ®ï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰
- å†…å­˜æ•ˆç‡é«˜
- æ”¯æŒç¨€ç–è¾“å…¥ï¼ˆå¦‚ TF-IDFï¼‰

æ³¨æ„äº‹é¡¹

- å¯¹æ•°æ®é¡ºåºæ•æ„Ÿï¼ˆå› ä¸ºæ˜¯åœ¨çº¿å­¦ä¹ ï¼‰
- éœ€è¦é€‰æ‹©åˆé€‚çš„ `C` å€¼ä»¥æ§åˆ¶æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### 1.1.16. Robustness regression: outliers and modeling errors

åœ¨å›å½’ä»»åŠ¡ä¸­ï¼Œæé«˜æ¨¡å‹å¯¹å¼‚å¸¸å€¼ï¼ˆoutliersï¼‰å’Œå»ºæ¨¡è¯¯å·®çš„é²æ£’æ€§ã€‚ä¼ ç»Ÿçš„æœ€å°äºŒä¹˜æ³•ï¼ˆå¦‚ LinearRegressionï¼‰å¯¹å¼‚å¸¸å€¼éå¸¸æ•æ„Ÿï¼Œå› æ­¤éœ€è¦ä½¿ç”¨æ›´ç¨³å¥çš„æ–¹æ³•ã€‚

ç¨³å¥å›å½’æ–¹æ³•ï¼ˆRobust Regression Modelsï¼‰

1. `RANSACRegressor`ï¼ˆRANdom SAmple Consensusï¼‰

- æ€æƒ³ï¼šä»æ•°æ®ä¸­éšæœºé‡‡æ ·å­é›†ï¼Œæ‹Ÿåˆæ¨¡å‹ï¼›å¯¹æ¯ä¸ªæ¨¡å‹ï¼Œè®¡ç®—æœ‰å¤šå°‘ç‚¹æ˜¯â€œå†…ç‚¹â€ï¼ˆæ‹Ÿåˆå¾—å¥½ï¼‰ï¼›æœ€ç»ˆé€‰æ‹©ä½¿â€œå†…ç‚¹â€æœ€å¤šçš„æ¨¡å‹ã€‚
- ä¼˜åŠ¿ï¼šèƒ½å¿½ç•¥ç¦»ç¾¤ç‚¹ï¼Œé€‚ç”¨äºæ•°æ®ä¸­æœ‰å¤§é‡å™ªå£°æˆ–é”™è¯¯çš„æƒ…å†µã€‚
- ç¼ºç‚¹ï¼šéç¡®å®šæ€§ï¼›å¯èƒ½æ”¶æ•›æ…¢ï¼ˆä¾èµ–é‡‡æ ·æ¬¡æ•°ï¼‰ã€‚

2. `HuberRegressor`

- æ€æƒ³ï¼šç»“åˆäº† L2 å’Œ L1 æŸå¤±ï¼š

  - å¯¹å°æ®‹å·®ä½¿ç”¨å¹³æ–¹æŸå¤±ï¼ˆåƒçº¿æ€§å›å½’ï¼‰
  - å¯¹å¤§æ®‹å·®ä½¿ç”¨çº¿æ€§æŸå¤±ï¼ˆåƒç»å¯¹å€¼å›å½’ï¼‰

- ä¼˜ç‚¹ï¼šå¯¹å¼‚å¸¸å€¼æœ‰é²æ£’æ€§ï¼ŒåŒæ—¶å¯¹å†…ç‚¹ä»ä¿æŒè¾ƒå¥½æ‹Ÿåˆæ•ˆæœ
- ä¸ RANSAC åŒºåˆ«ï¼šHuber æ˜¯ç¡®å®šæ€§ã€ä¼˜åŒ–å¼æ–¹æ³•ï¼›é€‚åˆè½»å¾®å¼‚å¸¸ç‚¹

1. `TheilSenRegressor`

- æ€æƒ³ï¼šä¸€ç§éå‚æ•°ã€ç¨³å¥çš„çº¿æ€§å›å½’æ–¹æ³•ï¼›é€šè¿‡å¤šæ¬¡å­æ ·æœ¬æ‹Ÿåˆå†å¯¹æ–œç‡å’Œæˆªè·æ±‚ä¸­ä½æ•°ã€‚
- ä¼˜ç‚¹ï¼š

  - å¯¹ç¦»ç¾¤ç‚¹éå¸¸ç¨³å¥
  - åœ¨é«˜å™ªå£°åœºæ™¯ä¸‹æ•ˆæœä¼˜äº OLS

- ç¼ºç‚¹ï¼š

  - è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯é«˜ç»´æ•°æ®

å°ç»“ï¼šé€‰æ‹©å»ºè®®

| æ–¹æ³•              | å¯¹å¼‚å¸¸å€¼çš„é²æ£’æ€§ | è®¡ç®—é€Ÿåº¦ | é€‚ç”¨æƒ…å†µ       |
| ----------------- | ---------------- | -------- | -------------- |
| LinearRegression  | å¼±               | å¿«       | æ•°æ®å¹²å‡€       |
| HuberRegressor    | ä¸­ç­‰             | å¿«       | å°‘é‡å¼‚å¸¸ç‚¹     |
| RANSACRegressor   | å¼º               | æ…¢       | æ˜æ˜¾ç¦»ç¾¤å€¼     |
| TheilSenRegressor | å¼º               | æ…¢       | é«˜å™ªå£°ã€éé«˜ç»´ |

### 1.1.17. Quantile Regression

Quantile Regression ä¸æ˜¯åªé¢„æµ‹å¹³å‡å€¼ï¼ˆå¦‚æ™®é€šçº¿æ€§å›å½’ï¼‰ï¼Œè€Œæ˜¯é¢„æµ‹ä¸åŒåˆ†ä½ç‚¹ï¼ˆquantilesï¼‰ä¸Šçš„æ¡ä»¶ä¸­ä½æ•°æˆ–å…¶ä»–åˆ†ä½æ•°ã€‚ä¾‹å¦‚ï¼š

- 0.5 åˆ†ä½æ•° = ä¸­ä½æ•°å›å½’
- 0.1 åˆ†ä½æ•° = é¢„æµ‹â€œä¸‹é™â€
- 0.9 åˆ†ä½æ•° = é¢„æµ‹â€œä¸Šé™â€

è¿™æ ·å¯ä»¥ç”¨æ¥ï¼š

- æ„å»ºé¢„æµ‹åŒºé—´
- åˆ†æç›®æ ‡å€¼åˆ†å¸ƒçš„åæ€æ€§
- åšå‡ºæ›´ç¨³å¥çš„å†³ç­–ï¼ˆå¦‚é£é™©ç®¡ç†ï¼‰

ä½¿ç”¨åœºæ™¯

- é¢„æµ‹åŒºé—´æ„é€ ï¼šä¾‹å¦‚ [0.1, 0.9] åˆ†ä½é¢„æµ‹åŒºé—´ï¼Œåæ˜ æ¨¡å‹ä¸ç¡®å®šæ€§
- å¯¹åæ€è¯¯å·®é²æ£’
- é‡‘èï¼šå¦‚ä»·å€¼-at-é£é™©ï¼ˆVaRï¼‰
- åŒ»ç–—/æ°”å€™ç­‰é«˜ä¸ç¡®å®šæ€§ä»»åŠ¡

æ•°å­¦åŸç†

åˆ†ä½æ•°å›å½’é€šè¿‡æœ€å°åŒ–ä¸å¯¹ç§°æŸå¤±å‡½æ•°ï¼ˆcheck functionï¼‰æ¥å­¦ä¹ ï¼š

$$
\text{Quantile Loss (pinball loss)} = \sum_i \max(q(y_i - \hat{y}_i), (q - 1)(y_i - \hat{y}_i))
$$

å…¶ä¸­ï¼š

- $q \in (0, 1)$ï¼šç›®æ ‡åˆ†ä½æ•°
- $y_i$ï¼šçœŸå®å€¼
- $\hat{y}_i$ï¼šé¢„æµ‹å€¼

ä¸åŒäºæ™®é€šå›å½’ï¼ˆå¹³æ–¹è¯¯å·®ï¼‰ï¼Œè¿™ç§æŸå¤±å¯¹ä¸åŒæ–¹å‘çš„è¯¯å·®åŠ æƒä¸åŒã€‚

ç‰¹ç‚¹ï¼š

- ä½¿ç”¨çº¿æ€§æ¨¡å‹ï¼ˆL1 æŸå¤± + L2 æ­£åˆ™ï¼‰
- å¯ä»¥ç”¨ä¸åŒåˆ†ä½æ•°æ¨¡å‹æ„å»ºé¢„æµ‹åŒºé—´

ä¼˜ç‚¹

- å¯ä¼°è®¡å®Œæ•´çš„å“åº”åˆ†å¸ƒï¼ˆè€Œéä»…å¹³å‡å€¼ï¼‰
- å¯¹å¼‚å¸¸å€¼é²æ£’
- æ˜“äºè§£é‡Šï¼ˆæ¯ä¸ªæ¨¡å‹è¡¨ç¤ºä¸€ä¸ªåˆ†ä½ç‚¹ï¼‰

æ³¨æ„äº‹é¡¹

- æ¯ä¸ªåˆ†ä½æ•°éƒ½éœ€è¦å•ç‹¬è®­ç»ƒä¸€ä¸ªæ¨¡å‹
- æ¨¡å‹å¯èƒ½å‡ºç°é¢„æµ‹äº¤å‰ï¼ˆä¾‹å¦‚ 90 åˆ†ä½é¢„æµ‹å€¼ä½äº 10 åˆ†ä½ï¼‰â†’ éœ€è¦åå¤„ç†
- è®¡ç®—æˆæœ¬ç•¥é«˜äºæ™®é€šçº¿æ€§å›å½’ï¼ˆä½†æ¯”éå‚æ•°æ–¹æ³•å¿«å¾ˆå¤šï¼‰

### 1.1.18. Polynomial regression: extending linear models with basis functions

å¤šé¡¹å¼å›å½’ æ˜¯é€šè¿‡å¼•å…¥å¤šé¡¹å¼åŸºå‡½æ•°æ¥æ‰©å±•çº¿æ€§å›å½’ï¼Œä»è€Œå®ç°å¯¹éçº¿æ€§æ•°æ®çš„å»ºæ¨¡ã€‚å…¶æœ¬è´¨æ˜¯é€šè¿‡éçº¿æ€§æ˜ å°„ï¼Œå°†æ•°æ®ä»åŸå§‹ç©ºé—´æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚

æ•°å­¦åŸç†

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼š

$$
y = w_0 + w_1 x + w_2 x^2 + ... + w_n x^n
$$

è¿™ä¸ªæ¨¡å‹å®é™…å°±æ˜¯å¤šé¡¹å¼å½¢å¼ã€‚ä¸ºäº†ä½¿ç”¨çº¿æ€§å›å½’è¿›è¡Œå¤šé¡¹å¼å»ºæ¨¡ï¼Œæˆ‘ä»¬é€šè¿‡å°†è¾“å…¥ç‰¹å¾è¿›è¡Œå¤šé¡¹å¼æ‰©å±•ï¼ˆä½¿ç”¨åŸºå‡½æ•°ï¼‰ï¼Œå¾—åˆ°ä¸€ä¸ªé«˜ç»´ç‰¹å¾ç©ºé—´ã€‚

- åŸºå‡½æ•°ï¼šé€šè¿‡å¯¹åŸå§‹ç‰¹å¾åº”ç”¨å¤šé¡¹å¼å˜æ¢ç”Ÿæˆæ–°çš„ç‰¹å¾ï¼Œä¾‹å¦‚ $x$ åˆ° $x^2, x^3$ï¼Œç›´åˆ°ä½ éœ€è¦çš„é˜¶æ•°ã€‚

å¤šé¡¹å¼å›å½’çš„æ­¥éª¤

1. å¤šé¡¹å¼æ‰©å±•ï¼šé€šè¿‡ `PolynomialFeatures` å°†åŸå§‹ç‰¹å¾è½¬åŒ–ä¸ºå¤šé¡¹å¼ç‰¹å¾ï¼ˆä¾‹å¦‚äºŒæ¬¡ã€ä¸‰æ¬¡ç­‰ï¼‰ã€‚
2. çº¿æ€§å›å½’ï¼šåœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­ï¼Œä½¿ç”¨æ ‡å‡†çš„çº¿æ€§å›å½’ç®—æ³•æ¥æ‹Ÿåˆè¿™äº›æ–°ç‰¹å¾ã€‚

ä¼˜ç‚¹

- çµæ´»æ€§ï¼šå¤šé¡¹å¼å›å½’èƒ½å¤Ÿæ‹Ÿåˆéçº¿æ€§å…³ç³»ã€‚
- æ˜“äºå®ç°ï¼šåœ¨ Scikit-learn ä¸­ï¼Œåªéœ€è¦ä½¿ç”¨ `PolynomialFeatures` å’Œ `LinearRegression` å³å¯ã€‚

ç¼ºç‚¹

- è¿‡æ‹Ÿåˆé£é™©ï¼šé«˜é˜¶å¤šé¡¹å¼ï¼ˆå¦‚ 10 æ¬¡ä»¥ä¸Šï¼‰å®¹æ˜“å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯å½“æ•°æ®é‡è¾ƒå°‘æ—¶ã€‚
- è®¡ç®—å¤æ‚åº¦ï¼šå¤šé¡¹å¼é˜¶æ•°é«˜æ—¶ï¼Œç‰¹å¾ç»´åº¦æ€¥å‰§å¢åŠ ï¼Œå¯èƒ½å¯¼è‡´è®¡ç®—å˜æ…¢ã€‚
- ä¸é€‚ç”¨äºé«˜ç»´åº¦æ•°æ®ï¼šå½“ç‰¹å¾ç»´åº¦å¾ˆå¤šæ—¶ï¼Œå¤šé¡¹å¼å›å½’çš„æ‰©å±•å¯èƒ½å¯¼è‡´ç»´åº¦ç¾éš¾ã€‚

é€‰æ‹©ç­–ç•¥

- é¿å…é«˜é˜¶å¤šé¡¹å¼ï¼šå¤šé¡¹å¼å›å½’é€‚ç”¨äºä¸€äº›ç®€å•çš„éçº¿æ€§æ¨¡å‹ï¼Œé˜¶æ•°ä¸è¦å¤ªé«˜ï¼Œå¦åˆ™å®¹æ˜“è¿‡æ‹Ÿåˆã€‚å¯ä»¥ä½¿ç”¨äº¤å‰éªŒè¯æ¥é€‰æ‹©åˆé€‚çš„é˜¶æ•°ã€‚
- æ­£åˆ™åŒ–ï¼šä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œå¯ä»¥ä½¿ç”¨æ­£åˆ™åŒ–çš„å¤šé¡¹å¼å›å½’ï¼Œå¦‚ å²­å›å½’ï¼ˆRidge Regressionï¼‰æˆ– å¥—ç´¢å›å½’ï¼ˆLasso Regressionï¼‰ã€‚

æ€»ç»“

å¤šé¡¹å¼å›å½’æ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥å°†çº¿æ€§å›å½’æ‰©å±•åˆ°éçº¿æ€§é—®é¢˜ä¸­ã€‚é€šè¿‡åˆç†é€‰æ‹©å¤šé¡¹å¼çš„é˜¶æ•°å’Œæ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ‹Ÿåˆå¤æ‚çš„å…³ç³»ï¼Œä½†ä¹Ÿéœ€è¦å°å¿ƒè¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚

## 1.2. Linear and Quadratic Discriminant Analysis

çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰ æ˜¯ä¸€ç§ç”¨äºé™ç»´çš„æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡æœ€å¤§åŒ–ç±»åˆ«ä¹‹é—´çš„åˆ†ç¦»æ¥é™ä½æ•°æ®çš„ç»´åº¦ã€‚å®ƒä¸ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¸åŒï¼Œå› ä¸º LDA æ˜¯ç›‘ç£å­¦ä¹ ï¼Œåˆ©ç”¨æ ‡ç­¾ä¿¡æ¯æ¥åšé™ç»´ï¼Œè€Œ PCA æ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œä¸ä¾èµ–æ ‡ç­¾ã€‚

LDA çš„æ ¸å¿ƒæ€æƒ³

LDA é€šè¿‡æ„é€ ä¸€ç»„æ–°çš„ç‰¹å¾ï¼ˆçº¿æ€§ç»„åˆï¼‰ï¼Œä½¿å¾—åŒä¸€ç±»æ ·æœ¬çš„æ•£å¸ƒå°½å¯èƒ½å°ï¼Œè€Œä¸åŒç±»ä¹‹é—´çš„æ•£å¸ƒå°½å¯èƒ½å¤§ã€‚è¿™é€šå¸¸é€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¥å®ç°ï¼š

1. è®¡ç®—ç±»å†…æ•£å¸ƒçŸ©é˜µï¼ˆWithin-class scatter matrixï¼‰ï¼šåº¦é‡æ¯ä¸ªç±»åˆ«å†…éƒ¨æ•°æ®çš„æ•£å¸ƒã€‚
2. è®¡ç®—ç±»é—´æ•£å¸ƒçŸ©é˜µï¼ˆBetween-class scatter matrixï¼‰ï¼šåº¦é‡ä¸åŒç±»åˆ«ä¹‹é—´çš„æ•£å¸ƒã€‚
3. æœ€å¤§åŒ–ç±»é—´æ•£å¸ƒå’Œç±»å†…æ•£å¸ƒçš„æ¯”å€¼ï¼Œä»è€Œè·å¾—æœ€ä¼˜çš„é™ç»´å˜æ¢ã€‚

æ•°å­¦åŸç†

- ç±»å†…æ•£å¸ƒçŸ©é˜µ $S_W$ï¼šåæ˜ åŒä¸€ç±»åˆ«æ ·æœ¬ä¹‹é—´çš„æ•£å¸ƒç¨‹åº¦ã€‚
- ç±»é—´æ•£å¸ƒçŸ©é˜µ $S_B$ï¼šåæ˜ ä¸åŒç±»åˆ«æ ·æœ¬ä¹‹é—´çš„æ•£å¸ƒç¨‹åº¦ã€‚

LDA é€šè¿‡æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°ï¼š

$$
\frac{|S_B|}{|S_W|}
$$

æ¥å¾—åˆ°æœ€ä¼˜çš„æŠ•å½±æ–¹å‘ã€‚è¿™é‡Œ $|S_B|$ å’Œ $|S_W|$ æ˜¯çŸ©é˜µçš„è¡Œåˆ—å¼ï¼Œä»£è¡¨äº†ç±»é—´å’Œç±»å†…çš„â€œæ•£å¸ƒâ€å¤§å°ã€‚

é™ç»´è¿‡ç¨‹ï¼š

- LDA è¯•å›¾æ‰¾åˆ°å°†åŸå§‹æ•°æ®æŠ•å½±åˆ°ä½ç»´ç©ºé—´çš„æœ€ä½³çº¿æ€§å˜æ¢ï¼Œä½¿å¾—ç±»åˆ«ä¹‹é—´çš„åˆ†ç¦»æœ€å¤§åŒ–ã€‚

ä¼˜ç‚¹ï¼š

- ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼šLDA è€ƒè™‘äº†æ ‡ç­¾ä¿¡æ¯ï¼Œä½¿å¾—é™ç»´åçš„æ•°æ®æ›´å…·åŒºåˆ†æ€§ã€‚
- é™ç»´ååˆ†ç±»ï¼šé™ç»´åçš„æ•°æ®é€šå¸¸æ›´å®¹æ˜“è¿›è¡Œåˆ†ç±»ã€‚
- å¤„ç†å¤šç±»åˆ«é—®é¢˜ï¼šLDA å¯ä»¥å¤„ç†å¤šç±»åˆ«åˆ†ç±»é—®é¢˜ã€‚

ç¼ºç‚¹ï¼š

- å‡è®¾æ•°æ®åˆ†å¸ƒï¼šLDA å‡è®¾æ¯ä¸ªç±»åˆ«çš„æ•°æ®éµå¾ªæ­£æ€åˆ†å¸ƒï¼Œä¸”å„ç±»åˆ«çš„åæ–¹å·®ç›¸åŒï¼Œè¿™åœ¨æŸäº›å®é™…æ•°æ®é›†ä¸­å¯èƒ½ä¸æˆç«‹ã€‚
- å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼šLDA å®¹æ˜“å—åˆ°å¼‚å¸¸å€¼å½±å“ï¼Œå¯èƒ½å¯¼è‡´é™ç»´æ•ˆæœä¸ä½³ã€‚

ä½¿ç”¨æ³¨æ„äº‹é¡¹

- æ•°æ®æ ‡å‡†åŒ–ï¼šLDA è¦æ±‚ç‰¹å¾æ•°æ®çš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼Œå› æ­¤å»ºè®®å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚
- ç±»åˆ«åˆ†å¸ƒå‡è®¾ï¼šLDA å‡è®¾å„ç±»åˆ«ä¹‹é—´çš„åæ–¹å·®ç›¸åŒã€‚å¦‚æœç±»åˆ«ä¹‹é—´çš„åæ–¹å·®å·®å¼‚è¾ƒå¤§ï¼Œå¯ä»¥è€ƒè™‘å…¶ä»–æ–¹æ³•ï¼ˆå¦‚ QDAï¼‰ã€‚

å°ç»“

LDA æ˜¯ä¸€ç§ç›‘ç£å¼é™ç»´æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–ç±»é—´å’Œç±»å†…çš„æ•£å¸ƒæ¯”ç‡æ¥å®ç°é™ç»´ã€‚å®ƒä¸ä»…èƒ½é™ä½æ•°æ®çš„ç»´åº¦ï¼Œè¿˜èƒ½æœ‰æ•ˆåœ°æé«˜åˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚é€‚ç”¨äºç±»åˆ«åˆ†å¸ƒç¬¦åˆæ­£æ€åˆ†å¸ƒå¹¶ä¸”ç±»åˆ«é—´åæ–¹å·®ç›¸ä¼¼çš„åœºæ™¯ã€‚

å¦‚ä½ æœ‰å…·ä½“çš„æ•°æ®é›†æˆ–æ¨¡å‹éœ€æ±‚ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥å¸®åŠ©ä½ ç”¨ LDA å®ç°é™ç»´å’Œåˆ†ç±»ï¼

## 1.3. Kernel ridge regression

æ ¸å²­å›å½’ï¼ˆKernel Ridge Regression, KRRï¼‰æ˜¯ä¸€ç§ç»“åˆäº†å²­å›å½’å’Œæ ¸æ–¹æ³•çš„å›å½’æŠ€æœ¯ã€‚å®ƒé€šè¿‡ä½¿ç”¨æ ¸æŠ€å·§æ‰©å±•äº†ä¼ ç»Ÿçš„å²­å›å½’ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œå­¦ä¹ ï¼Œä»è€Œæ•æ‰éçº¿æ€§å…³ç³»ã€‚

- å²­å›å½’ï¼ˆRidge Regressionï¼‰æ˜¯é€šè¿‡å¯¹æ¨¡å‹çš„å¤æ‚åº¦è¿›è¡Œ L2 æ­£åˆ™åŒ–æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€‚ç”¨äºçº¿æ€§å›å½’æ¨¡å‹ã€‚
- æ ¸æ–¹æ³•ï¼ˆKernel Methodsï¼‰é€šè¿‡å°†æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªæ›´é«˜ç»´çš„ç‰¹å¾ç©ºé—´ï¼Œåœ¨é«˜ç»´ç©ºé—´ä¸­è¿›è¡Œçº¿æ€§å›å½’ï¼Œæ¥è§£å†³éçº¿æ€§å›å½’é—®é¢˜ã€‚

æ¨¡å‹åŸç†

1. å²­å›å½’ï¼šé€šè¿‡æœ€å°åŒ–æ­£åˆ™åŒ–çš„æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨äºŒæ¬¡æŸå¤±æ¥æ‹Ÿåˆçº¿æ€§æ¨¡å‹ã€‚å…¶ç›®æ ‡å‡½æ•°ä¸ºï¼š

   $$
   \min_{\mathbf{w}} \left( \| \mathbf{y} - \mathbf{X} \mathbf{w} \|^2 + \lambda \| \mathbf{w} \|^2 \right)
   $$

   å…¶ä¸­ï¼š

   - $\mathbf{y}$ æ˜¯ç›®æ ‡å˜é‡
   - $\mathbf{X}$ æ˜¯ç‰¹å¾çŸ©é˜µ
   - $\mathbf{w}$ æ˜¯å›å½’ç³»æ•°
   - $\lambda$ æ˜¯æ­£åˆ™åŒ–å‚æ•°

2. æ ¸æŠ€å·§ï¼šä¸ºäº†å¤„ç†éçº¿æ€§å…³ç³»ï¼ŒKRR ä½¿ç”¨æ ¸å‡½æ•° $k(x, x')$ ä»£æ›¿è¾“å…¥æ•°æ®çŸ©é˜µ $X$ï¼Œé€šè¿‡æ˜ å°„å°†è¾“å…¥æ•°æ®ä»åŸå§‹ç©ºé—´æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ã€‚å¸¸ç”¨çš„æ ¸å‡½æ•°åŒ…æ‹¬ï¼š

   - çº¿æ€§æ ¸ï¼ˆLinear Kernelï¼‰
   - å¤šé¡¹å¼æ ¸ï¼ˆPolynomial Kernelï¼‰
   - é«˜æ–¯å¾„å‘åŸºæ ¸ï¼ˆRBF Kernelï¼‰

3. æ ¸å²­å›å½’çš„ç›®æ ‡å‡½æ•°ï¼š

   æ ¸å²­å›å½’çš„ç›®æ ‡æ˜¯æœ€å°åŒ–ä»¥ä¸‹æŸå¤±å‡½æ•°ï¼š

   $$
   \min_{\mathbf{w}} \left( \| \mathbf{y} - K \mathbf{w} \|^2 + \lambda \| \mathbf{w} \|^2 \right)
   $$

   å…¶ä¸­ï¼Œ$K$ æ˜¯æ ·æœ¬é—´çš„æ ¸çŸ©é˜µã€‚

   é€šè¿‡ä½¿ç”¨æ ¸çŸ©é˜µï¼Œæ ¸å²­å›å½’çš„è§£å¯ä»¥ç”¨å¦‚ä¸‹æ–¹å¼è¡¨ç¤ºï¼š

   $$
   \hat{\mathbf{w}} = (K + \lambda I)^{-1} \mathbf{y}
   $$

   è¿™é‡Œçš„ $K$ æ˜¯é€šè¿‡æ ¸å‡½æ•°è®¡ç®—çš„å†…ç§¯çŸ©é˜µï¼Œ$\lambda$ æ˜¯æ­£åˆ™åŒ–å‚æ•°ã€‚

æ ¸å‡½æ•°ç¤ºä¾‹

1. çº¿æ€§æ ¸ï¼ˆLinear Kernelï¼‰ï¼š

   - å¯¹åº”æ™®é€šçš„å²­å›å½’ï¼Œé€‚ç”¨äºçº¿æ€§å›å½’é—®é¢˜ã€‚

2. å¤šé¡¹å¼æ ¸ï¼ˆPolynomial Kernelï¼‰ï¼š

   - é€‚ç”¨äºæ•æ‰è¾“å…¥ç‰¹å¾ä¹‹é—´çš„éçº¿æ€§å…³ç³»ã€‚
   - æ ¸å‡½æ•°å½¢å¼ä¸ºï¼š$k(x, x') = (x^T x' + c)^d$

3. RBF æ ¸ï¼ˆGaussian RBF Kernelï¼‰ï¼š

   - éå¸¸å¸¸ç”¨ï¼Œé€‚ç”¨äºå¤§å¤šæ•°éçº¿æ€§å›å½’é—®é¢˜ï¼Œå°¤å…¶æ˜¯æ•°æ®æ— æ³•é€šè¿‡ç®€å•çš„çº¿æ€§æ¨¡å‹æ‹Ÿåˆæ—¶ã€‚
   - æ ¸å‡½æ•°å½¢å¼ä¸ºï¼š$k(x, x') = \exp\left( -\frac{\|x - x'\|^2}{2\sigma^2} \right)$

ä¼˜ç‚¹ï¼š

- æ•æ‰éçº¿æ€§å…³ç³»ï¼šé€šè¿‡æ ¸å‡½æ•°ï¼Œæ ¸å²­å›å½’èƒ½å¤Ÿå¤„ç†éçº¿æ€§å…³ç³»ï¼Œé€‚ç”¨äºå„ç§å¤æ‚çš„å›å½’é—®é¢˜ã€‚
- ç®€å•ä¸”å¼ºå¤§ï¼šæ¨¡å‹ç»“æ„ç®€å•ï¼Œæ˜“äºå®ç°å¹¶ä¸”èƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†é«˜ç»´æ•°æ®ã€‚
- ä¸éœ€è¦æ˜¾å¼è®¡ç®—ç‰¹å¾ç©ºé—´ï¼šåˆ©ç”¨æ ¸æŠ€å·§ï¼Œæ— éœ€æ˜¾å¼åœ°å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œå› æ­¤è®¡ç®—æ•ˆç‡è¾ƒé«˜ã€‚

ç¼ºç‚¹ï¼š

- è®¡ç®—å¼€é”€å¤§ï¼šæ ¸å²­å›å½’éœ€è¦è®¡ç®—å’Œå­˜å‚¨æ ·æœ¬é—´çš„æ ¸çŸ©é˜µ $K$ï¼Œå¯¹äºå¤§æ•°æ®é›†ï¼Œè®¡ç®—é‡å’Œå­˜å‚¨è¦æ±‚è¾ƒé«˜ã€‚
- å¯¹æ ¸å‡½æ•°çš„é€‰æ‹©æ•æ„Ÿï¼šé€‰æ‹©ä¸åŒçš„æ ¸å‡½æ•°ä¼šå½±å“æ¨¡å‹çš„æ•ˆæœï¼Œéœ€è¦é€šè¿‡äº¤å‰éªŒè¯æ¥é€‰æ‹©æœ€åˆé€‚çš„æ ¸å‡½æ•°ã€‚
- å¯¹å‚æ•°çš„é€‰æ‹©æ•æ„Ÿï¼šæ­£åˆ™åŒ–å‚æ•° $\alpha$ å’Œæ ¸å‡½æ•°çš„å‚æ•°ï¼ˆå¦‚ RBF æ ¸çš„ $\gamma$ï¼‰éœ€è¦ç²¾å¿ƒè°ƒèŠ‚ï¼Œè¿‡å°çš„ $\alpha$ å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œè¿‡å¤§çš„ $\alpha$ å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆã€‚

åº”ç”¨åœºæ™¯ï¼š

- é€‚ç”¨äºå°å‹åˆ°ä¸­å‹æ•°æ®é›†ï¼Œå…¶ä¸­å­˜åœ¨å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚
- å¸¸è§åº”ç”¨åŒ…æ‹¬éçº¿æ€§å›å½’é—®é¢˜ã€æ—¶é—´åºåˆ—é¢„æµ‹ã€ä¿¡å·å¤„ç†å’Œå›¾åƒåˆ†æç­‰é¢†åŸŸã€‚

æ€»ç»“

æ ¸å²­å›å½’ç»“åˆäº†å²­å›å½’å’Œæ ¸æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆåœ°è§£å†³éçº¿æ€§å›å½’é—®é¢˜ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„æ ¸å‡½æ•°å’Œæ­£åˆ™åŒ–å‚æ•°ï¼ŒKRR å¯ä»¥åœ¨å¤šç§å®é™…é—®é¢˜ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§æ•°æ®é›†ï¼Œå®ƒçš„è®¡ç®—å¼€é”€è¾ƒå¤§ï¼Œå› æ­¤éœ€è¦åœ¨æ•°æ®é‡å’Œè®¡ç®—èµ„æºä¹‹é—´åšæƒè¡¡ã€‚

## 1.4. Support Vector Machines

### 1.4.1. Classification

### 1.4.2. Regression

### 1.4.3. Density estimation, novelty detection

### 1.4.4. Complexity

### 1.4.5. Tips on Practical Use

### 1.4.6. Kernel functions

### 1.4.7. Mathematical formulation

### 1.4.8. Implementation details

## 1.5. Stochastic Gradient Descent

### 1.5.1. Classification

### 1.5.2. Regression

### 1.5.3. Online One-Class SVM

### 1.5.4. Stochastic Gradient Descent for sparse data

### 1.5.5. Complexity

### 1.5.6. Stopping criterion

### 1.5.7. Tips on Practical Use

### 1.5.8. Mathematical formulation

### 1.5.9. Implementation details

## 1.6. Nearest Neighbors

### 1.6.1. Unsupervised Nearest Neighbors

### 1.6.2. Nearest Neighbors Classification

### 1.6.3. Nearest Neighbors Regression

### 1.6.4. Nearest Neighbor Algorithms

### 1.6.5. Nearest Centroid Classifier

### 1.6.6. Nearest Neighbors Transformer

### 1.6.7. Neighborhood Components Analysis

## 1.7. Gaussian Processes

### 1.7.1. Gaussian Process Regression (GPR)

### 1.7.2. Gaussian Process Classification (GPC)

### 1.7.3. GPC examples

### 1.7.4. Kernels for Gaussian Processes

## 1.8. Cross decomposition

### 1.8.1. PLSCanonical

### 1.8.2. PLSSVD

### 1.8.3. PLSRegression

### 1.8.4. Canonical Correlation Analysis

## 1.9. Naive Bayes

### 1.9.1. Gaussian Naive Bayes

### 1.9.2. Multinomial Naive Bayes

### 1.9.3. Complement Naive Bayes

### 1.9.4. Bernoulli Naive Bayes

### 1.9.5. Categorical Naive Bayes

### 1.9.6. Out-of-core naive Bayes model fitting

## 1.10. Decision Trees

### 1.10.1. Classification

### 1.10.2. Regression

### 1.10.3. Multi-output problems

### 1.10.4. Complexity

### 1.10.5. Tips on practical use

### 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART

### 1.10.7. Mathematical formulation

### 1.10.8. Missing Values Support

### 1.10.9. Minimal Cost-Complexity Pruning

## 1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking

### 1.11.1. Gradient-boosted trees

### 1.11.2. Random forests and other randomized tree ensembles

### 1.11.3. Bagging meta-estimator

### 1.11.4. Voting Classifier

### 1.11.5. Voting Regressor

### 1.11.6. Stacked generalization

### 1.11.7. AdaBoost

## 1.12. Multiclass and multioutput algorithms

### 1.12.1. Multiclass classification

### 1.12.2. Multilabel classification

### 1.12.3. Multiclass-multioutput classification

### 1.12.4. Multioutput regression

## 1.13. Feature selection

### 1.13.1. Removing features with low variance

### 1.13.2. Univariate feature selection

### 1.13.3. Recursive feature elimination

### 1.13.4. Feature selection using SelectFromModel

### 1.13.5. Sequential Feature Selection

### 1.13.6. Feature selection as part of a pipeline

## 1.14. Semi-supervised learning

### 1.14.1. Self Training

### 1.14.2. Label Propagation

## 1.15. Isotonic regression

å‡è®¾ä½ çŸ¥é“æŸä¸ªå˜é‡ï¼ˆæ¯”å¦‚æ¸©åº¦ï¼‰è¶Šé«˜ï¼Œå¦ä¸€ä¸ªå˜é‡ï¼ˆæ¯”å¦‚è®¾å¤‡æ•…éšœç‡ï¼‰ä¸€å®šä¸ä¼šé™ä½ï¼Œé‚£ä¹ˆä½ å¯ä»¥ç”¨ç­‰å€¼å›å½’æ¥å»ºæ¨¡è¿™ç§å•è°ƒéå‡çš„å…³ç³»ï¼Œé¿å…æ¨¡å‹å‡ºç°â€œæ¸©åº¦å‡é«˜åè€Œé¢„æµ‹å‡ºæ›´å°‘æ•…éšœâ€çš„ä¸åˆç†æƒ…å†µã€‚

## 1.16. Probability calibration

æ¦‚ç‡æ ¡å‡†æ˜¯åœ¨å·²æœ‰æ¨¡å‹çš„åŸºç¡€ä¸Šå·¥ä½œçš„åå¤„ç†æ–¹æ³•ã€‚
å®ƒå¹¶ä¸æ˜¯ä¸€ä¸ªå•ç‹¬çš„åˆ†ç±»å™¨ï¼Œè€Œæ˜¯ä¸€ä¸ªâ€œåŒ…è£¹å™¨â€æ¨¡å‹ï¼ˆwrapperï¼‰ï¼Œç”¨äºä¿®æ­£åŸæ¨¡å‹è¾“å‡ºçš„â€œæ¦‚ç‡â€ã€‚

å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š

1. ä½ å…ˆè®­ç»ƒä¸€ä¸ªä»»æ„çš„åˆ†ç±»æ¨¡å‹ï¼ˆæ¯”å¦‚ SVMã€éšæœºæ£®æ—ã€GBDTã€ç¥ç»ç½‘ç»œç­‰ï¼‰ï¼›
2. ç„¶åç”¨å®ƒçš„é¢„æµ‹ç»“æœï¼ˆå¾—åˆ†æˆ–æ¦‚ç‡ï¼‰ï¼Œå†é€šè¿‡â€œæ ¡å‡†æ–¹æ³•â€ï¼ˆå¦‚ Platt scaling æˆ– Isotonic Regressionï¼‰è¿›è¡Œåå¤„ç†ï¼›
3. è¾“å‡ºçš„æ˜¯æ›´è´´è¿‘çœŸå®çš„æ¦‚ç‡å€¼ï¼Œè€Œä¸æ˜¯åŸå§‹æ¨¡å‹çš„æœªç»ä¿®æ­£è¾“å‡ºã€‚

### 1.16.1. Calibration curves

### 1.16.2. Calibrating a classifier

### 1.16.3. Usage

## 1.17. Neural network models (supervised)

### 1.17.1. Multi-layer Perceptron

### 1.17.2. Classification

### 1.17.3. Regression

### 1.17.4. Regularization

### 1.17.5. Algorithms

### 1.17.6. Complexity

### 1.17.7. Tips on Practical Use

### 1.17.8. More control with warm_start

# 2\. Unsupervised learning

## 2.1. Gaussian mixture models

### 2.1.1. Gaussian Mixture

### 2.1.2. Variational Bayesian Gaussian Mixture

## 2.2. Manifold learning

### 2.2.1. Introduction

### 2.2.2. Isomap

### 2.2.3. Locally Linear Embedding

### 2.2.4. Modified Locally Linear Embedding

### 2.2.5. Hessian Eigenmapping

### 2.2.6. Spectral Embedding

### 2.2.7. Local Tangent Space Alignment

### 2.2.8. Multi-dimensional Scaling (MDS)

### 2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)

### 2.2.10. Tips on practical use

## 2.3. Clustering

### 2.3.1. Overview of clustering methods

### 2.3.2. K-means

### 2.3.3. Affinity Propagation

### 2.3.4. Mean Shift

### 2.3.5. Spectral clustering

### 2.3.6. Hierarchical clustering

### 2.3.7. DBSCAN

### 2.3.8. HDBSCAN

### 2.3.9. OPTICS

### 2.3.10. BIRCH

### 2.3.11. Clustering performance evaluation

## 2.4. Biclustering

### 2.4.1. Spectral Co-Clustering

### 2.4.2. Spectral Biclustering

### 2.4.3. Biclustering evaluation

## 2.5. Decomposing signals in components (matrix factorization problems)

### 2.5.1. Principal component analysis (PCA)

### 2.5.2. Kernel Principal Component Analysis (kPCA)

### 2.5.3. Truncated singular value decomposition and latent semantic analysis

### 2.5.4. Dictionary Learning

### 2.5.5. Factor Analysis

### 2.5.6. Independent component analysis (ICA)

### 2.5.7. Non-negative matrix factorization (NMF or NNMF)

### 2.5.8. Latent Dirichlet Allocation (LDA)

## 2.6. Covariance estimation

### 2.6.1. Empirical covariance

### 2.6.2. Shrunk Covariance

### 2.6.3. Sparse inverse covariance

### 2.6.4. Robust Covariance Estimation

## 2.7. Novelty and Outlier Detection

### 2.7.1. Overview of outlier detection methods

### 2.7.2. Novelty Detection

### 2.7.3. Outlier Detection

### 2.7.4. Novelty detection with Local Outlier Factor

## 2.8. Density Estimation

### 2.8.1. Density Estimation: Histograms

### 2.8.2. Kernel Density Estimation

## 2.9. Neural network models (unsupervised)

### 2.9.1. Restricted Boltzmann machines

# 3\. Model selection and evaluation

## 3.1. Cross-validation: evaluating estimator performance

### 3.1.1. Computing cross-validated metrics

### 3.1.2. Cross validation iterators

### 3.1.3. A note on shuffling

### 3.1.4. Cross validation and model selection

### 3.1.5. Permutation test score

## 3.2. Tuning the hyper-parameters of an estimator

### 3.2.1. Exhaustive Grid Search

### 3.2.2. Randomized Parameter Optimization

### 3.2.3. Searching for optimal parameters with successive halving

### 3.2.4. Tips for parameter search

### 3.2.5. Alternatives to brute force parameter search

## 3.3. Tuning the decision threshold for class prediction

### 3.3.1. Post-tuning the decision threshold

## 3.4. Metrics and scoring: quantifying the quality of predictions

### 3.4.1. Which scoring function should I use?

### 3.4.2. Scoring API overview

### 3.4.3. The `scoring` parameter: defining model evaluation rules

### 3.4.4. Classification metrics

### 3.4.5. Multilabel ranking metrics

### 3.4.6. Regression metrics

### 3.4.7. Clustering metrics

### 3.4.8. Dummy estimators

## 3.5. Validation curves: plotting scores to evaluate models

### 3.5.1. Validation curve

### 3.5.2. Learning curve

# 4\. Metadata Routing

## 4.1. Usage Examples

### 4.1.1. Weighted scoring and fitting

### 4.1.2. Weighted scoring and unweighted fitting

### 4.1.3. Unweighted feature selection

### 4.1.4. Different scoring and fitting weights

## 4.2. API Interface

## 4.3. Metadata Routing Support Status

# 5\. Inspection

## 5.1. Partial Dependence and Individual Conditional Expectation plots

### 5.1.1. Partial dependence plots

### 5.1.2. Individual conditional expectation (ICE) plot

### 5.1.3. Mathematical Definition

### 5.1.4. Computation methods

## 5.2. Permutation feature importance

### 5.2.1. Outline of the permutation importance algorithm

### 5.2.2. Relation to impurity-based importance in trees

### 5.2.3. Misleading values on strongly correlated features

# 6\. Visualizations

## 6.1. Available Plotting Utilities

### 6.1.1. Display Objects

# 7\. Dataset transformations

## 7.1. Pipelines and composite estimators

### 7.1.1. Pipeline: chaining estimators

### 7.1.2. Transforming target in regression

### 7.1.3. FeatureUnion: composite feature spaces

### 7.1.4. ColumnTransformer for heterogeneous data

### 7.1.5. Visualizing Composite Estimators

## 7.2. Feature extraction

### 7.2.1. Loading features from dicts

### 7.2.2. Feature hashing

### 7.2.3. Text feature extraction

### 7.2.4. Image feature extraction

## 7.3. Preprocessing data

### 7.3.1. Standardization, or mean removal and variance scaling

### 7.3.2. Non-linear transformation

### 7.3.3. Normalization

### 7.3.4. Encoding categorical features

### 7.3.5. Discretization

### 7.3.6. Imputation of missing values

### 7.3.7. Generating polynomial features

### 7.3.8. Custom transformers

## 7.4. Imputation of missing values

### 7.4.1. Univariate vs. Multivariate Imputation

### 7.4.2. Univariate feature imputation

### 7.4.3. Multivariate feature imputation

### 7.4.4. Nearest neighbors imputation

### 7.4.5. Keeping the number of features constant

### 7.4.6. Marking imputed values

### 7.4.7. Estimators that handle NaN values

## 7.5. Unsupervised dimensionality reduction

### 7.5.1. PCA: principal component analysis

### 7.5.2. Random projections

### 7.5.3. Feature agglomeration

## 7.6. Random Projection

### 7.6.1. The Johnson-Lindenstrauss lemma

### 7.6.2. Gaussian random projection

### 7.6.3. Sparse random projection

### 7.6.4. Inverse Transform

## 7.7. Kernel Approximation

### 7.7.1. Nystroem Method for Kernel Approximation

### 7.7.2. Radial Basis Function Kernel

### 7.7.3. Additive Chi Squared Kernel

### 7.7.4. Skewed Chi Squared Kernel

### 7.7.5. Polynomial Kernel Approximation via Tensor Sketch

### 7.7.6. Mathematical Details

## 7.8. Pairwise metrics, Affinities and Kernels

### 7.8.1. Cosine similarity

### 7.8.2. Linear kernel

### 7.8.3. Polynomial kernel

### 7.8.4. Sigmoid kernel

### 7.8.5. RBF kernel

### 7.8.6. Laplacian kernel

### 7.8.7. Chi-squared kernel

## 7.9. Transforming the prediction target (`y`)

### 7.9.1. Label binarization

### 7.9.2. Label encoding

# 8\. Dataset loading utilities

## 8.1. Toy datasets

### 8.1.1. Iris plants dataset

### 8.1.2. Diabetes dataset

### 8.1.3. Optical recognition of handwritten digits dataset

### 8.1.4. Linnerrud dataset

### 8.1.5. Wine recognition dataset

### 8.1.6. Breast cancer Wisconsin (diagnostic) dataset

## 8.2. Real world datasets

### 8.2.1. The Olivetti faces dataset

### 8.2.2. The 20 newsgroups text dataset

### 8.2.3. The Labeled Faces in the Wild face recognition dataset

### 8.2.4. Forest covertypes

### 8.2.5. RCV1 dataset

### 8.2.6. Kddcup 99 dataset

### 8.2.7. California Housing dataset

### 8.2.8. Species distribution dataset

## 8.3. Generated datasets

### 8.3.1. Generators for classification and clustering

### 8.3.2. Generators for regression

### 8.3.3. Generators for manifold learning

### 8.3.4. Generators for decomposition

## 8.4. Loading other datasets

### 8.4.1. Sample images

### 8.4.2. Datasets in svmlight / libsvm format

### 8.4.3. Downloading datasets from the openml.org repository

### 8.4.4. Loading from external datasets

# 9\. Computing with scikit-learn

## 9.1. Strategies to scale computationally: bigger data

### 9.1.1. Scaling with instances using out-of-core learning

## 9.2. Computational Performance

### 9.2.1. Prediction Latency

### 9.2.2. Prediction Throughput

### 9.2.3. Tips and Tricks

## 9.3. Parallelism, resource management, and configuration

### 9.3.1. Parallelism

### 9.3.2. Configuration switches

# 10\. Model persistence

## 10.1. Workflow Overview

### 10.1.1. Train and Persist the Model

## 10.2. ONNX

## 10.3. `skops.io`

## 10.4. `pickle`, `joblib`, and `cloudpickle`

## 10.5. Security & Maintainability Limitations

### 10.5.1. Replicating the training environment in production

### 10.5.2. Serving the model artifact

## 10.6. Summarizing the key points

# 11\. Common pitfalls and recommended practices

## 11.1. Inconsistent preprocessing

## 11.2. Data leakage

### 11.2.1. How to avoid data leakage

### 11.2.2. Data leakage during pre-processing

## 11.3. Controlling randomness

### 11.3.1. Using `None` or `RandomState` instances, and repeated calls to `fit` and `split`

### 11.3.2. Common pitfalls and subtleties

### 11.3.3. General recommendations

# 12\. Dispatching

## 12.1. Array API support (experimental)

### 12.1.1. Example usage

### 12.1.2. Support for `Array API`\-compatible inputs

### 12.1.3. Input and output array type handling

### 12.1.4. Common estimator checks

# 13\. Choosing the right estimator

# 14\. External Resources, Videos and Talks

## 14.1. The scikit-learn MOOC

## 14.2. Videos

## 14.3. New to Scientific Python?

## 14.4. External Tutorials
